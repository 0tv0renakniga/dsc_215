{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 215: Probability and Statistics for Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1 Summary: Introduction to Data\n",
    "\n",
    "## 1. Introduction to Statistics\n",
    "\n",
    "### What is Statistics?\n",
    "- **Definition**: A discipline that focuses on the collection, analysis, and interpretation of data\n",
    "- **Applications**: Used in various fields including:\n",
    "  - \"Hard\" Sciences (physics, chemistry, biology)\n",
    "  - Social Sciences (politics, economics, education)\n",
    "  - Medicine\n",
    "  - Business and industry\n",
    "\n",
    "### Types of Data\n",
    "- **Numerical Data**: Quantitative measurements\n",
    "  - **Discrete**: Countable values (e.g., number of students in a class)\n",
    "  - **Continuous**: Measurements on a continuous scale (e.g., height, weight)\n",
    "- **Categorical Data**: Qualitative classifications\n",
    "  - **Nominal**: Categories with no natural ordering (e.g., dog breeds, political party)\n",
    "  - **Ordinal**: Categories with a natural ordering (e.g., education level, satisfaction ratings)\n",
    "\n",
    "## 2. Data Organization\n",
    "\n",
    "### Data Matrix Structure\n",
    "- **Observations/Cases**: Individual entities being measured (rows in a data matrix)\n",
    "- **Variables**: Characteristics being measured for each observation (columns)\n",
    "\n",
    "**Example**: Dog Breed Study\n",
    "```\n",
    "| Dog ID | Breed     | Weight (kg) | Height (cm) | Fur Length (cm) |\n",
    "|--------|-----------|-------------|-------------|-----------------|\n",
    "| 1      | Husky     | 25          | 58          | 5               |\n",
    "| 2      | Bulldog   | 22          | 40          | 2               |\n",
    "| 3      | Chihuahua | 2.5         | 20          | 3               |\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- Each row represents one dog (an observation/case)\n",
    "- The variables are Breed (categorical), Weight (numerical-continuous), Height (numerical-continuous), and Fur Length (numerical-continuous)\n",
    "- There are 3 categorical levels for the Breed variable: Husky, Bulldog, and Chihuahua\n",
    "\n",
    "## 3. Relationships Between Variables\n",
    "\n",
    "### Types of Associations\n",
    "- **Positive Association**: Variables increase or decrease together\n",
    "  - Example: House size and price tend to increase together\n",
    "- **Negative Association**: One variable increases as the other decreases\n",
    "  - Example: Homeownership rate decreases as percentage of multi-unit structures increases\n",
    "- **No Association**: No discernible pattern between variables\n",
    "\n",
    "### Variable Roles\n",
    "- **Explanatory Variable**: Variable that might affect or explain changes in another variable\n",
    "  - Also called independent variable or predictor\n",
    "- **Response Variable**: Variable that may be affected by the explanatory variable\n",
    "  - Also called dependent variable or outcome\n",
    "\n",
    "**Important Note**: Association does not imply causation\n",
    "\n",
    "### Visualizing Relationships\n",
    "- **Scatterplots**: Used to visualize relationships between two numerical variables\n",
    "  - Each point represents a single observation\n",
    "  - Pattern of points indicates type and strength of relationship\n",
    "\n",
    "**Example**: Homeownership and Multi-Unit Structures\n",
    "```\n",
    "The scatterplot shows a negative association between homeownership rate and \n",
    "percentage of multi-unit structures in counties. As the percentage of multi-unit \n",
    "structures increases, the homeownership rate tends to decrease.\n",
    "```\n",
    "\n",
    "## 4. Data Collection Methods\n",
    "\n",
    "### Observational Studies\n",
    "- **Definition**: Data collected without interference in how variables arise\n",
    "- **Characteristics**:\n",
    "  - Good for identifying natural associations\n",
    "  - Cannot establish causation\n",
    "  - Subject to confounding variables\n",
    "- **Applications**: Surveys, existing records, cohort studies\n",
    "\n",
    "### Experiments\n",
    "- **Definition**: Designed investigations where researchers control variables\n",
    "- **Characteristics**:\n",
    "  - Can establish causal connections\n",
    "  - Involves treatment and control groups\n",
    "  - Uses randomization to control for confounding variables\n",
    "- **Key Components**:\n",
    "  - **Treatment Group**: Receives the intervention being studied\n",
    "  - **Control Group**: Provides a baseline for comparison\n",
    "\n",
    "## 5. Sampling Principles\n",
    "\n",
    "### Population vs. Sample\n",
    "- **Population**: The entire set of cases about which we want to draw conclusions\n",
    "- **Sample**: A subset of the population from which we collect data\n",
    "- **Sampling Frame**: List of cases from which the sample is drawn\n",
    "\n",
    "### Sampling Methods\n",
    "- **Simple Random Sampling**: Each case has an equal probability of selection\n",
    "  - Formula for probability of selection: $P(\\text{selection}) = \\frac{n}{N}$\n",
    "  - Where n = sample size, N = population size\n",
    "- **Stratified Sampling**: Population divided into groups, then random sampling within groups\n",
    "- **Cluster Sampling**: Population divided into clusters, then entire clusters selected\n",
    "\n",
    "### Sampling Bias\n",
    "- **Non-response Bias**: When certain types of subjects are less likely to respond\n",
    "  - Example: Only 30% of people respond to a survey, potentially skewing results\n",
    "- **Convenience Sampling**: Using easily accessible subjects (often leads to bias)\n",
    "  - Example: Surveying only people walking in a particular neighborhood\n",
    "- **Voluntary Response Bias**: When sample consists of self-selected volunteers\n",
    "  - Example: Online product reviews typically come from very satisfied or very dissatisfied customers\n",
    "\n",
    "## 6. Experimental Design\n",
    "\n",
    "### Key Principles\n",
    "- **Control**: Managing differences between treatment and control groups\n",
    "- **Randomization**: Random assignment to account for uncontrollable variables\n",
    "- **Replication**: Using more cases for better estimation\n",
    "- **Blocking**: Subdividing based on variables that may affect response\n",
    "\n",
    "### Reducing Bias in Human Experiments\n",
    "- **Blind Studies**: Participants unaware of their treatment status\n",
    "- **Double-Blind Studies**: Both participants and researchers unaware of treatment status\n",
    "- **Placebos**: Fake treatments given to control groups\n",
    "  - Helps account for the placebo effect (improvement due to expectation)\n",
    "\n",
    "### Example of Experimental Design\n",
    "```\n",
    "Study: Effect of a sleeping pill on people with trouble sleeping\n",
    "\n",
    "Design elements:\n",
    "- 80 participants with trouble sleeping\n",
    "- Blocking variable: Age (40 people >50 years old, 40 people <50 years old)\n",
    "- Random assignment within blocks to treatment or control\n",
    "- Treatment: One sleeping pill per week\n",
    "- Control: Placebo pill\n",
    "- Blinding: Participants don't know which pill they received (single-blind)\n",
    "- Duration: 10 weeks\n",
    "- Response variable: Quality of sleep\n",
    "```\n",
    "\n",
    "## 7. Drawing Valid Conclusions\n",
    "\n",
    "### Correlation vs. Causation\n",
    "- Association between variables does not imply causation\n",
    "- Causation can only be established through well-designed experiments\n",
    "- Observational studies can suggest but not prove causal relationships\n",
    "\n",
    "### Generalizability\n",
    "- Results from a sample can only be generalized to the population it represents\n",
    "- Random sampling improves generalizability\n",
    "- External validity refers to how well results apply to other situations\n",
    "\n",
    "### Example: Valid and Invalid Conclusions\n",
    "```\n",
    "Study: Survey of 50 students in a statistics class about voluntary work participation\n",
    "\n",
    "Valid conclusion: \"X% of students in this specific statistics class participate in \n",
    "voluntary work.\"\n",
    "\n",
    "Invalid conclusion: \"Studying statistics causes students to participate in voluntary \n",
    "work.\" (Correlation doesn't imply causation)\n",
    "\n",
    "Invalid conclusion: \"X% of all university students participate in voluntary work.\" \n",
    "(Cannot generalize beyond the specific class)\n",
    "```\n",
    "\n",
    "## 8. Key Formulas and Concepts\n",
    "\n",
    "### Simple Random Sampling\n",
    "- Each case has equal probability of selection: $P(\\text{selection}) = \\frac{n}{N}$\n",
    "- Where n = sample size, N = population size\n",
    "\n",
    "### Randomization in Experiments\n",
    "- Random assignment helps ensure treatment and control groups are comparable\n",
    "- Helps control for confounding variables\n",
    "- Can be done using random number generators, coin flips, etc.\n",
    "\n",
    "## 9. Common Misconceptions\n",
    "\n",
    "1. **Correlation implies causation**: Just because two variables are associated doesn't mean one causes the other.\n",
    "\n",
    "2. **Larger samples are always better**: While larger samples generally provide more precision, a large biased sample is worse than a small unbiased sample.\n",
    "\n",
    "3. **Statistical significance equals practical importance**: A statistically significant result may not be practically meaningful.\n",
    "\n",
    "4. **Anecdotal evidence is reliable**: Individual stories or experiences are not statistically valid evidence.\n",
    "\n",
    "5. **All studies are equally valid**: The design and methodology of a study greatly affect the validity of its conclusions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2 Summary: Summarizing Data\n",
    "\n",
    "## 1. Visualizing Numerical Data\n",
    "\n",
    "### Scatterplots\n",
    "- **Purpose**: Visualize relationships between two numerical variables\n",
    "- **Features**:\n",
    "  - Each point represents a single case with coordinates (x, y)\n",
    "  - Help identify associations (positive, negative, or none)\n",
    "  - Reveal whether relationships are simple (linear) or complex (non-linear)\n",
    "- **Example**: A scatterplot of total income versus loan amount shows that borrowers with higher incomes tend to take larger loans, though the relationship isn't perfectly linear.\n",
    "\n",
    "### Histograms\n",
    "- **Purpose**: Visualize the distribution of a single numerical variable\n",
    "- **Features**:\n",
    "  - Data values are grouped into bins (intervals)\n",
    "  - Height of bars represents frequency or density\n",
    "  - Higher bars indicate where data are more common\n",
    "  - Provide a view of data density across the range of values\n",
    "- **Example**: A histogram of interest rates for loans might show that most loans have rates between 5-10%, with fewer loans having rates above 15%.\n",
    "\n",
    "## 2. Shape of Distributions\n",
    "\n",
    "### Skewness\n",
    "- **Right-skewed (Positively Skewed)**:\n",
    "  - Data trail off to the right with a longer right tail\n",
    "  - Mean is typically greater than median\n",
    "  - Example: Income distributions, home prices\n",
    "  \n",
    "- **Left-skewed (Negatively Skewed)**:\n",
    "  - Data trail off to the left with a longer left tail\n",
    "  - Mean is typically less than median\n",
    "  - Example: Age-at-death distributions, exam scores with ceiling effects\n",
    "  \n",
    "- **Symmetric**:\n",
    "  - Data show roughly equal trailing off in both directions\n",
    "  - Mean and median are approximately equal\n",
    "  - Example: Height distributions in adult populations\n",
    "\n",
    "### Modality\n",
    "- **Unimodal**: One prominent peak (most common)\n",
    "- **Bimodal**: Two prominent peaks (suggests two subpopulations)\n",
    "- **Multimodal**: More than two prominent peaks\n",
    "- **Uniform**: No peaks, approximately equal frequency across all values\n",
    "\n",
    "## 3. Measures of Center\n",
    "\n",
    "### Mean (Average)\n",
    "- **Definition**: Sum of all values divided by number of observations\n",
    "- **Formula**: \n",
    "  $$\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{x_1 + x_2 + \\ldots + x_n}{n}$$\n",
    "- **Properties**:\n",
    "  - Useful for comparisons between groups\n",
    "  - Affected by all values in the dataset\n",
    "  - Not resistant to outliers (non-robust)\n",
    "  - Appropriate for symmetric distributions\n",
    "- **Example**: For interest rates of 10.9%, 9.92%, ..., 6.08% across 50 loans, the mean is 11.57%.\n",
    "\n",
    "### Median\n",
    "- **Definition**: Middle value when data are ordered\n",
    "- **Calculation**:\n",
    "  - If n is odd: middle value\n",
    "  - If n is even: average of two middle values\n",
    "- **Properties**:\n",
    "  - Robust statistic (resistant to outliers)\n",
    "  - Better measure of center for skewed distributions\n",
    "  - Not affected by extreme values\n",
    "- **Example**: For the ordered data {1, 5, 6, 7, 10}, the median is 6. For {1, 5, 6, 7}, the median is (5+6)/2 = 5.5.\n",
    "\n",
    "### Mode\n",
    "- **Definition**: Value with a prominent peak in the distribution\n",
    "- **Properties**:\n",
    "  - Can have multiple modes\n",
    "  - Useful for categorical data\n",
    "  - Less commonly used for numerical data\n",
    "- **Example**: In the dataset {2, 3, 3, 4, 5, 5, 5, 6, 7}, the mode is 5.\n",
    "\n",
    "## 4. Measures of Spread\n",
    "\n",
    "### Variance\n",
    "- **Definition**: Average squared deviation from the mean\n",
    "- **Formula**: \n",
    "  $$s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2$$\n",
    "- **Properties**:\n",
    "  - Measures how much data deviates from the mean\n",
    "  - Units are squared (making interpretation difficult)\n",
    "  - Not resistant to outliers\n",
    "- **Example**: For the data {2, 4, 6, 8, 10}, the mean is 6, and the variance is:\n",
    "  $$s^2 = \\frac{(2-6)^2 + (4-6)^2 + (6-6)^2 + (8-6)^2 + (10-6)^2}{5-1} = \\frac{16 + 4 + 0 + 4 + 16}{4} = \\frac{40}{4} = 10$$\n",
    "\n",
    "### Standard Deviation\n",
    "- **Definition**: Square root of variance\n",
    "- **Formula**: \n",
    "  $$s = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}$$\n",
    "- **Properties**:\n",
    "  - Same units as original data\n",
    "  - Useful for considering how far data are distributed from the mean\n",
    "  - Approximately 68% of data falls within 1 standard deviation of mean in normal distributions\n",
    "  - Not resistant to outliers\n",
    "- **Example**: For the data {2, 4, 6, 8, 10}, the standard deviation is $\\sqrt{10} = 3.16$.\n",
    "\n",
    "### Range\n",
    "- **Definition**: Difference between maximum and minimum values\n",
    "- **Formula**: Range = max(x) - min(x)\n",
    "- **Properties**:\n",
    "  - Simple to calculate\n",
    "  - Highly sensitive to outliers\n",
    "  - Provides limited information about distribution\n",
    "- **Example**: For the data {2, 4, 6, 8, 10}, the range is 10 - 2 = 8.\n",
    "\n",
    "### Quartiles and IQR\n",
    "- **First quartile (Q‚ÇÅ)**: 25% of data falls below this value\n",
    "- **Second quartile (Q‚ÇÇ)**: Median (50% of data falls below)\n",
    "- **Third quartile (Q‚ÇÉ)**: 75% of data falls below this value\n",
    "- **Interquartile Range (IQR)**: Q‚ÇÉ - Q‚ÇÅ (range of middle 50% of data)\n",
    "- **Properties**:\n",
    "  - Robust statistics (resistant to outliers)\n",
    "  - Useful for describing skewed distributions\n",
    "  - Used to identify potential outliers\n",
    "- **Example**: For the data {2, 4, 6, 8, 10, 12, 14}, Q‚ÇÅ = 4, Q‚ÇÇ = 8, Q‚ÇÉ = 12, and IQR = 12 - 4 = 8.\n",
    "\n",
    "## 5. Box Plots and Outliers\n",
    "\n",
    "### Box Plots\n",
    "- **Components**:\n",
    "  - Box: Represents IQR (middle 50% of data)\n",
    "  - Line inside box: Median\n",
    "  - Whiskers: Extend to smallest/largest data points within 1.5√óIQR of Q‚ÇÅ/Q‚ÇÉ\n",
    "  - Individual points: Potential outliers beyond whiskers\n",
    "- **Uses**:\n",
    "  - Comparing distributions across groups\n",
    "  - Identifying skewness and outliers\n",
    "  - Summarizing key statistics visually\n",
    "\n",
    "### Outliers\n",
    "- **Definition**: Observations that appear extreme relative to the rest of the data\n",
    "- **Common identification method**: Values beyond Q‚ÇÅ-1.5√óIQR or Q‚ÇÉ+1.5√óIQR\n",
    "- **Importance**:\n",
    "  - May indicate data collection or recording errors\n",
    "  - Could represent important rare events\n",
    "  - Can significantly affect non-robust statistics\n",
    "  - Should be investigated, not automatically removed\n",
    "\n",
    "## 6. Summarizing Categorical Data\n",
    "\n",
    "### Tables for Categorical Data\n",
    "- **Frequency Tables**: Count occurrences of each category\n",
    "- **Relative Frequency Tables**: Show proportion or percentage in each category\n",
    "- **Contingency Tables**: Summarize data for two categorical variables\n",
    "  - Each cell represents the number of times a particular combination occurred\n",
    "  - Can be modified to show proportions (row, column, or overall)\n",
    "\n",
    "### Example of a Contingency Table\n",
    "```\n",
    "                 homeownership\n",
    "                 rent  mortgage  own   Total\n",
    "app_type individual 3496   3839    1170  8505\n",
    "         joint      362    950     183   1495\n",
    "         Total     3858   4789    1353  10000\n",
    "```\n",
    "\n",
    "### Row and Column Proportions\n",
    "- **Row proportions**: Each count divided by its row total\n",
    "  - Example: 3496/8505 = 0.411 (41.1% of individual applicants rent)\n",
    "- **Column proportions**: Each count divided by its column total\n",
    "  - Example: 3496/3858 = 0.906 (90.6% of renters applied as individuals)\n",
    "\n",
    "### Visualizing Categorical Data\n",
    "- **Bar Plots**:\n",
    "  - Display counts or proportions for categories\n",
    "  - Bars should be separated (unlike histograms)\n",
    "  - Height represents frequency or proportion\n",
    "- **Variations**:\n",
    "  - Stacked bar plots: Show composition within categories\n",
    "  - Side-by-side bar plots: Compare groups directly\n",
    "  - Standardized stacked bar plots: Show proportions within each category\n",
    "\n",
    "## 7. Comparing Distributions\n",
    "\n",
    "### Comparing Numerical Data Across Groups\n",
    "- **Side-by-side Box Plots**:\n",
    "  - Traditional tool for comparing distributions across groups\n",
    "  - Allow comparison of center, spread, and outliers\n",
    "- **Hollow Histograms**:\n",
    "  - Outlines of histograms for each group on the same plot\n",
    "  - Useful for comparing shapes of distributions\n",
    "\n",
    "### Comparing Categorical Data Across Groups\n",
    "- **Side-by-side Bar Plots**: Compare frequencies across groups\n",
    "- **Stacked Bar Plots**: Compare composition within categories\n",
    "- **Mosaic Plots**: Area represents frequency in contingency tables\n",
    "\n",
    "## 8. Statistical Transformations\n",
    "\n",
    "### Linear Transformations\n",
    "- **Adding a constant (x + c)**:\n",
    "  - Changes center but not spread\n",
    "  - Mean increases by the constant: $\\bar{x}_{new} = \\bar{x} + c$\n",
    "  - Median increases by the constant: $\\text{median}_{new} = \\text{median} + c$\n",
    "  - Range and standard deviation remain unchanged\n",
    "  - Example: Converting Celsius to Fahrenheit (F = C + 32)\n",
    "\n",
    "- **Multiplying by a constant (c √ó x)**:\n",
    "  - Changes both center and spread\n",
    "  - Mean is multiplied by the constant: $\\bar{x}_{new} = c \\times \\bar{x}$\n",
    "  - Median is multiplied by the constant: $\\text{median}_{new} = c \\times \\text{median}$\n",
    "  - Range and standard deviation are multiplied by |c|\n",
    "  - Example: Converting inches to centimeters (cm = 2.54 √ó inches)\n",
    "\n",
    "### Example of Transformations\n",
    "For the data {10, 20, 30, 40, 50}:\n",
    "- Original: mean = 30, median = 30, range = 40, standard deviation ‚âà 15.81\n",
    "- After adding 5: {15, 25, 35, 45, 55}\n",
    "  - New mean = 35, new median = 35, range = 40, standard deviation ‚âà 15.81\n",
    "- After multiplying by 2: {20, 40, 60, 80, 100}\n",
    "  - New mean = 60, new median = 60, range = 80, standard deviation ‚âà 31.62\n",
    "\n",
    "## 9. Practical Examples\n",
    "\n",
    "### Example 1: Analyzing Loan Interest Rates\n",
    "A dataset contains interest rates for 50 loans with the following statistics:\n",
    "- Mean: 11.57%\n",
    "- Median: 9.93%\n",
    "- Standard Deviation: 5.05%\n",
    "- Range: 21.9% (from 5% to 26.9%)\n",
    "- Q‚ÇÅ: 7.5%\n",
    "- Q‚ÇÉ: 15%\n",
    "- IQR: 7.5%\n",
    "\n",
    "The histogram shows a right-skewed distribution, indicating that most loans have rates under 15%, with a few loans having rates above 20%. Since the distribution is skewed, the median (9.93%) is a better measure of central tendency than the mean (11.57%).\n",
    "\n",
    "### Example 2: Comparing Student Quiz Scores\n",
    "A class of students took a quiz, and the 5-number summaries are given for 18 freshmen and 15 sophomores:\n",
    "\n",
    "```\n",
    "Summary   Min  Q1   Median  Q3   Max\n",
    "Freshmen   3   4.5   6.5    8.5  9.5\n",
    "Sophomores 4   6     7.5    9    10\n",
    "```\n",
    "\n",
    "Observations:\n",
    "- Sophomores have the highest score (10 > 9.5)\n",
    "- Freshmen have a greater range (6.5 > 6)\n",
    "- Freshmen have a greater IQR (4 > 3)\n",
    "- If the mean of freshmen scores is 6.5 and sophomores is 7, the overall mean is:\n",
    "  $$\\text{Overall Mean} = \\frac{6.5 \\times 18 + 7 \\times 15}{33} = \\frac{117 + 105}{33} = \\frac{222}{33} = 6.73$$\n",
    "\n",
    "### Example 3: Effect of Salary Changes on Statistics\n",
    "If an employee with the lowest salary in a company of three employees becomes part-time and has a salary reduction:\n",
    "\n",
    "- **Effect on measures of center**:\n",
    "  - Mean will decrease\n",
    "  - Median will not change (since the lowest value remains the lowest)\n",
    "  \n",
    "- **Effect on measures of spread**:\n",
    "  - Range will increase\n",
    "  - Standard deviation will increase (the data points become more spread out from the mean)\n",
    "  - IQR may not change (depends on whether the lowest salary is below Q‚ÇÅ)\n",
    "\n",
    "## 10. Key Takeaways\n",
    "\n",
    "1. Different statistics are appropriate for different types of data and distributions:\n",
    "   - For skewed distributions, median is often more representative than mean\n",
    "   - For symmetric distributions, mean and median are similar\n",
    "\n",
    "2. Robust statistics (median, IQR) are less affected by outliers than non-robust statistics (mean, standard deviation)\n",
    "\n",
    "3. Visualizations help identify patterns and outliers in data:\n",
    "   - Histograms show the shape of distributions\n",
    "   - Box plots summarize key statistics and identify outliers\n",
    "   - Scatterplots show relationships between variables\n",
    "\n",
    "4. Categorical data requires different analysis approaches than numerical data:\n",
    "   - Contingency tables and bar plots for categorical data\n",
    "   - Proportions often more informative than raw counts\n",
    "\n",
    "5. When comparing groups, consider both measures of center and spread:\n",
    "   - Side-by-side box plots or hollow histograms for numerical comparisons\n",
    "   - Stacked or side-by-side bar plots for categorical comparisons\n",
    "\n",
    "6. Transformations affect statistics in predictable ways:\n",
    "   - Adding a constant shifts the center but doesn't change the spread\n",
    "   - Multiplying by a constant changes both center and spread\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3 Summary: Introduction to Probability\n",
    "\n",
    "## 1. Foundations of Probability\n",
    "\n",
    "### Why Study Probability?\n",
    "- Probability is the foundation upon which statistics is built\n",
    "- Essential for machine learning, artificial intelligence, game theory, and information theory\n",
    "- Provides a formal framework for understanding uncertainty and randomness\n",
    "- Enables deeper understanding of statistical tools and techniques\n",
    "\n",
    "### Sample Space\n",
    "- **Definition**: The set of all possible outcomes of an experiment, denoted by Œ©\n",
    "- **Examples**:\n",
    "  - Rolling a die: Œ© = {1, 2, 3, 4, 5, 6}\n",
    "  - Flipping a coin: Œ© = {Heads, Tails}\n",
    "  - Tossing a coin three times: Œ© = {HHH, HHT, HTH, HTT, THH, THT, TTH, TTT}\n",
    "\n",
    "### Event Space\n",
    "- **Definition**: A set whose elements are themselves sets (subsets of Œ©)\n",
    "- Every event A in the event space ‚Ñ± is a subset of Œ© (A ‚äÇ Œ©)\n",
    "- The event space must satisfy certain properties:\n",
    "  - The empty set ‚àÖ must be in ‚Ñ±\n",
    "  - If A is in ‚Ñ±, then its complement A<sup>c</sup> must also be in ‚Ñ±\n",
    "  - If A‚ÇÅ, A‚ÇÇ, ... are all in ‚Ñ±, then their union must also be in ‚Ñ±\n",
    "\n",
    "### Probability Measure\n",
    "- **Definition**: A function ‚Ñô: ‚Ñ± ‚Üí [0,1] that assigns probabilities to events\n",
    "- Must satisfy the axioms of probability:\n",
    "  - ‚Ñô(A) ‚â• 0 for all A ‚àà ‚Ñ± (non-negativity)\n",
    "  - ‚Ñô(Œ©) = 1 (normalization)\n",
    "  - If A‚ÇÅ, A‚ÇÇ, ... are disjoint events (A<sub>i</sub> ‚à© A<sub>j</sub> = ‚àÖ for i ‚â† j), then ‚Ñô(A‚ÇÅ ‚à™ A‚ÇÇ ‚à™ ...) = Œ£·µ¢ ‚Ñô(A<sub>i</sub>)\n",
    "\n",
    "### Properties of Probability Measures\n",
    "- If A ‚äÇ B, then ‚Ñô(A) ‚â§ ‚Ñô(B)\n",
    "- ‚Ñô(A ‚à© B) ‚â§ min(‚Ñô(A), ‚Ñô(B))\n",
    "- ‚Ñô(A ‚à™ B) ‚â§ ‚Ñô(A) + ‚Ñô(B) (union bound)\n",
    "- ‚Ñô(A<sup>c</sup>) = 1 - ‚Ñô(A)\n",
    "\n",
    "### Example: Die Rolling\n",
    "For a fair six-sided die:\n",
    "- Sample space: Œ© = {1, 2, 3, 4, 5, 6}\n",
    "- Event space: ‚Ñ± = P(Œ©) (the power set of Œ©)\n",
    "- Probability measure: If a set A ‚àà ‚Ñ± has i elements, then ‚Ñô(A) = i/6\n",
    "- Example: ‚Ñô({1, 4, 6}) = 3/6 = 0.5\n",
    "\n",
    "## 2. Conditional Probability and Independence\n",
    "\n",
    "### Conditional Probability\n",
    "- **Definition**: The probability of event A occurring given that event B has occurred\n",
    "- Formula: \n",
    "  $$\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}$$\n",
    "  where ‚Ñô(B) ‚â† 0\n",
    "\n",
    "### Independence\n",
    "- **Definition**: Two events A and B are independent if and only if:\n",
    "  $$\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\times \\mathbb{P}(B)$$\n",
    "- Equivalently, A and B are independent if:\n",
    "  $$\\mathbb{P}(A|B) = \\mathbb{P}(A)$$\n",
    "\n",
    "### Example: Left-Handed People\n",
    "If 9% of people are left-handed and 2 people are selected at random from a large population:\n",
    "- Probability both are left-handed (assuming independence):\n",
    "  $$\\mathbb{P}(\\text{both left-handed}) = \\mathbb{P}(\\text{first left-handed}) \\times \\mathbb{P}(\\text{second left-handed}) = 0.09^2 = 0.0081$$\n",
    "\n",
    "### Example: Smallpox in Boston, 1721\n",
    "A dataset of 6,224 individuals exposed to smallpox in Boston:\n",
    "\n",
    "| | Inoculated | Not Inoculated | Total |\n",
    "|---|---|---|---|\n",
    "| Lived | 238 | 5,136 | 5,374 |\n",
    "| Died | 6 | 844 | 850 |\n",
    "| Total | 244 | 5,980 | 6,224 |\n",
    "\n",
    "- Probability a non-inoculated person died from smallpox:\n",
    "  $$\\mathbb{P}(\\text{died}|\\text{not inoculated}) = \\frac{\\mathbb{P}(\\text{died} \\cap \\text{not inoculated})}{\\mathbb{P}(\\text{not inoculated})} = \\frac{0.1356}{0.9608} \\approx 0.1411$$\n",
    "\n",
    "- Probability an inoculated person died from smallpox:\n",
    "  $$\\mathbb{P}(\\text{died}|\\text{inoculated}) = \\frac{\\mathbb{P}(\\text{died} \\cap \\text{inoculated})}{\\mathbb{P}(\\text{inoculated})} = \\frac{0.0010}{0.0392} \\approx 0.0246$$\n",
    "\n",
    "### Bayes' Theorem\n",
    "- **Formula**:\n",
    "  $$\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(B|A)\\mathbb{P}(A)}{\\mathbb{P}(B)}$$\n",
    "\n",
    "- When ‚Ñô(B) is not directly accessible:\n",
    "  $$\\mathbb{P}(B) = \\sum_i \\mathbb{P}(B|A_i)\\mathbb{P}(A_i)$$\n",
    "  where A<sub>i</sub> ‚à© A<sub>j</sub> = ‚àÖ for i ‚â† j and ‚à™<sub>i</sub> A<sub>i</sub> = Œ©\n",
    "\n",
    "### Example: Bayes' Theorem with Smallpox Data\n",
    "Using the smallpox data to find the probability that a person who died was not inoculated:\n",
    "$$\\mathbb{P}(\\text{not inoculated}|\\text{died}) = \\frac{\\mathbb{P}(\\text{died}|\\text{not inoculated})\\mathbb{P}(\\text{not inoculated})}{\\mathbb{P}(\\text{died})} = \\frac{0.1411 \\times 0.9608}{0.1366} \\approx 0.993$$\n",
    "\n",
    "## 3. Random Variables\n",
    "\n",
    "### Definition and Types\n",
    "- **Random Variable**: A function from the sample space Œ© to another set (typically ‚Ñù or ‚Ñï)\n",
    "- **Discrete Random Variable**: Takes values from a countable set\n",
    "- **Continuous Random Variable**: Takes values from an uncountable set (typically an interval of real numbers)\n",
    "\n",
    "### Example: Coin Tossing\n",
    "For a sequence of 3 coin tosses:\n",
    "- Sample space: Œ© = {(H,H,H), (H,H,T), (H,T,H), (H,T,T), (T,H,H), (T,H,T), (T,T,H), (T,T,T)}\n",
    "- Random variable X = number of heads in the sequence\n",
    "- For œâ = (H,H,T), X(œâ) = 2\n",
    "\n",
    "## 4. Probability Distributions for Discrete Random Variables\n",
    "\n",
    "### Probability Mass Function (PMF)\n",
    "- **Definition**: A function p<sub>X</sub>: ‚Ñù ‚Üí [0,1] that gives the probability that a discrete random variable equals a specific value\n",
    "- **Formula**: p<sub>X</sub>(a) = ‚Ñô(X = a)\n",
    "- **Properties**:\n",
    "  - p<sub>X</sub>(x) ‚â• 0 for all x\n",
    "  - Œ£<sub>x</sub> p<sub>X</sub>(x) = 1\n",
    "\n",
    "### Example: Fair Coin Toss\n",
    "For a random variable X associated with a fair coin toss (X = 1 for heads, X = 0 for tails):\n",
    "$$p_X(x) = \\begin{cases}\n",
    "1/2 & \\text{if } x = 0 \\\\\n",
    "1/2 & \\text{if } x = 1 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "## 5. Probability Distributions for Continuous Random Variables\n",
    "\n",
    "### Cumulative Distribution Function (CDF)\n",
    "- **Definition**: A function F<sub>X</sub>: ‚Ñù ‚Üí [0,1] that gives the probability that a random variable is less than or equal to a specific value\n",
    "- **Formula**: F<sub>X</sub>(x) = ‚Ñô(X ‚â§ x)\n",
    "- **Properties**:\n",
    "  - 0 ‚â§ F<sub>X</sub>(x) ‚â§ 1\n",
    "  - lim<sub>x‚Üí-‚àû</sub> F<sub>X</sub>(x) = 0\n",
    "  - lim<sub>x‚Üí+‚àû</sub> F<sub>X</sub>(x) = 1\n",
    "  - x ‚â§ y ‚üπ F<sub>X</sub>(x) ‚â§ F<sub>X</sub>(y) (non-decreasing)\n",
    "  - ‚Ñô(a ‚â§ X ‚â§ b) = F<sub>X</sub>(b) - F<sub>X</sub>(a)\n",
    "\n",
    "### Probability Density Function (PDF)\n",
    "- **Definition**: For a continuous random variable with a differentiable CDF, the PDF is the derivative of the CDF\n",
    "- **Formula**: f<sub>X</sub>(x) = d/dx F<sub>X</sub>(x)\n",
    "- **Properties**:\n",
    "  - f<sub>X</sub>(x) ‚â• 0\n",
    "  - ‚à´<sub>-‚àû</sub><sup>‚àû</sup> f<sub>X</sub>(x)dx = 1\n",
    "  - For a set A, ‚à´<sub>A</sub> f<sub>X</sub>(x)dx = ‚Ñô(X ‚àà A)\n",
    "  - ‚Ñô(a ‚â§ X ‚â§ b) = ‚à´<sub>a</sub><sup>b</sup> f<sub>X</sub>(x)dx\n",
    "  - F<sub>X</sub>(x) = ‚à´<sub>-‚àû</sub><sup>x</sup> f<sub>X</sub>(z)dz\n",
    "  - Important: In general, f<sub>X</sub>(x) ‚â† ‚Ñô(X = x)\n",
    "\n",
    "### Example: Uniform Random Variable\n",
    "For a uniform random variable on [0,1]:\n",
    "$$f_X(x) = \\begin{cases}\n",
    "1 & \\text{if } x \\in [0,1] \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "The CDF is:\n",
    "$$F_X(x) = \\begin{cases}\n",
    "0 & \\text{if } x < 0 \\\\\n",
    "x & \\text{if } 0 \\leq x \\leq 1 \\\\\n",
    "1 & \\text{if } x > 1\n",
    "\\end{cases}$$\n",
    "\n",
    "## 6. Expectation, Variance, and Standard Deviation\n",
    "\n",
    "### Expectation (Expected Value)\n",
    "- **For Discrete Random Variables**:\n",
    "  $$\\mathbb{E}(X) = \\sum_{x \\in S} x \\cdot p_X(x)$$\n",
    "\n",
    "- **For Continuous Random Variables**:\n",
    "  $$\\mathbb{E}(X) = \\int_{-\\infty}^{\\infty} x \\cdot f_X(x) dx$$\n",
    "\n",
    "- **For Functions of Random Variables**:\n",
    "  - Discrete: ùîº(g(X)) = Œ£<sub>x‚ààS</sub> g(x) ¬∑ p<sub>X</sub>(x)\n",
    "  - Continuous: ùîº(g(X)) = ‚à´<sub>-‚àû</sub><sup>‚àû</sup> g(x) ¬∑ f<sub>X</sub>(x) dx\n",
    "\n",
    "### Example: Expected Value of a Die Roll\n",
    "For a fair six-sided die:\n",
    "$$\\mathbb{E}(X) = \\sum_{i=1}^{6} i \\times \\frac{1}{6} = \\frac{1}{6} + \\frac{2}{6} + \\frac{3}{6} + \\frac{4}{6} + \\frac{5}{6} + \\frac{6}{6} = \\frac{21}{6} = 3.5$$\n",
    "\n",
    "### Example: Expected Value of a Uniform Random Variable\n",
    "For a uniform random variable on [0,1]:\n",
    "$$\\mathbb{E}(X) = \\int_{-\\infty}^{\\infty} x \\cdot f_X(x) dx = \\int_{0}^{1} x \\cdot 1 \\, dx = \\left[ \\frac{x^2}{2} \\right]_{0}^{1} = \\frac{1}{2}$$\n",
    "\n",
    "### Linearity of Expectation\n",
    "- If X and Y are random variables, and a and b are constants:\n",
    "  $$\\mathbb{E}(a \\cdot g(X) + b \\cdot h(Y)) = a \\cdot \\mathbb{E}(g(X)) + b \\cdot \\mathbb{E}(h(Y))$$\n",
    "\n",
    "### Example: Linearity of Expectation\n",
    "If the expected sales price of an apple is $1 and an orange is $2, then the expected sales price of 2 apples and 3 oranges is:\n",
    "$$\\mathbb{E}(2 \\cdot \\text{apple price} + 3 \\cdot \\text{orange price}) = 2 \\cdot \\mathbb{E}(\\text{apple price}) + 3 \\cdot \\mathbb{E}(\\text{orange price}) = 2 \\times \\$1 + 3 \\times \\$2 = \\$8$$\n",
    "\n",
    "### Variance and Standard Deviation\n",
    "- **Variance**: Measures the spread or dispersion of a random variable around its mean\n",
    "  $$\\text{Var}(X) = \\mathbb{E}((X - \\mu)^2) = \\mathbb{E}(X^2) - \\mu^2$$\n",
    "  where Œº = ùîº(X)\n",
    "\n",
    "- **Standard Deviation**: Square root of the variance\n",
    "  $$\\sigma = \\sqrt{\\text{Var}(X)}$$\n",
    "\n",
    "### Example: Variance of a Die Roll\n",
    "For a fair six-sided die with Œº = 3.5:\n",
    "$$\\text{Var}(X) = \\mathbb{E}((X - \\mu)^2) = \\sum_{i=1}^{6} (i - 3.5)^2 \\times \\frac{1}{6} = \\frac{105}{36} \\approx 2.92$$\n",
    "$$\\sigma = \\sqrt{\\frac{105}{36}} \\approx 1.71$$\n",
    "\n",
    "### Example: Variance of a Uniform Random Variable\n",
    "For a uniform random variable on [0,1] with Œº = 1/2:\n",
    "$$\\text{Var}(X) = \\mathbb{E}((X - \\mu)^2) = \\int_{0}^{1} (x - \\frac{1}{2})^2 dx = \\frac{1}{12}$$\n",
    "$$\\sigma = \\sqrt{\\frac{1}{12}} \\approx 0.289$$\n",
    "\n",
    "### Properties of Variance\n",
    "- For independent random variables X and Y, and constants a and b:\n",
    "  $$\\text{Var}(a \\cdot g(X) + b \\cdot h(Y)) = a^2 \\cdot \\text{Var}(g(X)) + b^2 \\cdot \\text{Var}(h(Y))$$\n",
    "\n",
    "## 7. Practical Examples\n",
    "\n",
    "### Example 1: Card Drawing\n",
    "For a standard deck of 52 cards:\n",
    "- The sample space for drawing two cards and recording their sum (Ace = 1, Jack/Queen/King = 10) is:\n",
    "  {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}\n",
    "- The sample space for the number of spades when drawing five cards is:\n",
    "  {0, 1, 2, 3, 4, 5}\n",
    "\n",
    "### Example 2: Die Rolling and Event Independence\n",
    "For a fair 6-sided die with events:\n",
    "- A: The number rolled is odd = {1, 3, 5}\n",
    "- B: The number rolled is greater than or equal to 4 = {4, 5, 6}\n",
    "- C: The number rolled doesn't start with the letters \"f\" or \"t\" = {1, 6}\n",
    "\n",
    "To determine if A and C are independent:\n",
    "- ‚Ñô(A) = 3/6 = 1/2\n",
    "- ‚Ñô(C) = 2/6 = 1/3\n",
    "- ‚Ñô(A ‚à© C) = |{1}|/6 = 1/6\n",
    "- Since ‚Ñô(A) √ó ‚Ñô(C) = (1/2) √ó (1/3) = 1/6 = ‚Ñô(A ‚à© C), events A and C are independent\n",
    "\n",
    "### Example 3: Blood Type Probabilities\n",
    "Given that 44% of Americans have type O blood, 42% have type A, 10% have type B, and the rest have type AB:\n",
    "- Probability of not having type A blood: 1 - 0.42 = 0.58\n",
    "- Probability of having type A or AB blood: 0.42 + (1 - 0.44 - 0.42 - 0.10) = 0.42 + 0.04 = 0.46\n",
    "\n",
    "### Example 4: Betting Game Expected Value\n",
    "In a betting game where you roll a die and:\n",
    "- Win $200 if you get a 5 on the first roll\n",
    "- Win $100 if you don't get a 5 on the first roll but get a 5 on the second roll\n",
    "- Win $0 otherwise\n",
    "\n",
    "The expected winnings are:\n",
    "- ‚Ñô(X = 200) = 1/6\n",
    "- ‚Ñô(X = 100) = (5/6) √ó (1/6) = 5/36\n",
    "- ‚Ñô(X = 0) = 1 - 1/6 - 5/36 = 25/36\n",
    "- ùîº(X) = 200 √ó (1/6) + 100 √ó (5/36) + 0 √ó (25/36) = 33.33 + 13.89 = $47.22\n",
    "\n",
    "## 8. Key Takeaways\n",
    "\n",
    "1. Probability provides a formal framework for quantifying uncertainty and randomness\n",
    "2. The three components of a probability space are:\n",
    "   - Sample space (Œ©): All possible outcomes\n",
    "   - Event space (‚Ñ±): Collection of subsets of Œ©\n",
    "   - Probability measure (‚Ñô): Function assigning probabilities to events\n",
    "\n",
    "3. Conditional probability allows us to update probabilities based on new information\n",
    "4. Independence of events means the occurrence of one event doesn't affect the probability of another\n",
    "5. Random variables map outcomes to numbers, allowing mathematical analysis\n",
    "6. Probability distributions describe the likelihood of different values of a random variable:\n",
    "   - PMF for discrete random variables\n",
    "   - PDF and CDF for continuous random variables\n",
    "\n",
    "7. Expected value represents the long-run average of a random variable\n",
    "8. Variance and standard deviation measure the spread or dispersion around the expected value\n",
    "9. Linearity of expectation and properties of variance simplify calculations for combinations of random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4 Summary: Distributions of Random Variables\n",
    "\n",
    "## 1. Introduction to Probability Distributions\n",
    "\n",
    "### Common Distributions\n",
    "- **Discrete Random Variables**:\n",
    "  - Bernoulli distribution\n",
    "  - Binomial distribution\n",
    "  - Geometric distribution\n",
    "  - Negative binomial distribution\n",
    "  - Poisson distribution\n",
    "\n",
    "- **Continuous Random Variables**:\n",
    "  - Normal distribution\n",
    "  - Chi-squared distribution\n",
    "  - t-distribution\n",
    "  - F-distribution\n",
    "  - Logistic distribution\n",
    "\n",
    "## 2. Bernoulli Distribution\n",
    "\n",
    "### Definition\n",
    "- Models processes with only two outcomes: \"success\" (1) and \"failure\" (0)\n",
    "- A random variable X with a Bernoulli distribution takes:\n",
    "  - Value 1 with probability p\n",
    "  - Value 0 with probability 1-p\n",
    "\n",
    "### Probability Mass Function (PMF)\n",
    "$$p_X(x) = \\begin{cases}\n",
    "p & \\text{if } x = 1 \\\\\n",
    "1-p & \\text{if } x = 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "### Expected Value and Variance\n",
    "- Expected value: $\\mu = \\mathbb{E}(X) = p$\n",
    "- Variance: $\\sigma^2 = p(1-p)$\n",
    "\n",
    "### Applications\n",
    "- Coin flips\n",
    "- Voting preference in a two-party system\n",
    "- Whether a product is defective\n",
    "\n",
    "## 3. Binomial Distribution\n",
    "\n",
    "### Definition\n",
    "- Describes the probability of having exactly k successes in n independent Bernoulli trials\n",
    "- Each trial has the same probability p of success\n",
    "- Notation: $X \\sim B(n,p)$ means X follows a binomial distribution with parameters n and p\n",
    "\n",
    "### Probability Mass Function (PMF)\n",
    "$$\\mathbb{P}(X = k) = \\binom{n}{k}p^k(1-p)^{n-k} \\text{ for } k = 0,1,...,n$$\n",
    "\n",
    "Where $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$ is the binomial coefficient.\n",
    "\n",
    "### Expected Value and Variance\n",
    "- Expected value: $\\mu = np$\n",
    "- Variance: $\\sigma^2 = np(1-p)$\n",
    "\n",
    "### Example: Peanut Allergies\n",
    "If the probability of a child having a peanut allergy is 2%, and a classroom has 30 children:\n",
    "- Probability that none of them has a peanut allergy:\n",
    "  $\\mathbb{P}(X = 0) = \\binom{30}{0}0.02^0 \\times 0.98^{30} \\approx 0.5455$\n",
    "- Probability that 3 of them have a peanut allergy:\n",
    "  $\\mathbb{P}(X = 3) = \\binom{30}{3}0.02^3 \\times 0.98^{27} \\approx 0.0188$\n",
    "\n",
    "### Example: International Students\n",
    "If 23.2% of UCSD students are international students, and there are 50 students on a dorm floor:\n",
    "- Expected number of international students: $E[X] = np = 50 \\times 0.232 = 11.6$\n",
    "- Standard deviation: $SD(X) = \\sqrt{np(1-p)} = \\sqrt{50 \\times 0.232 \\times 0.768} \\approx 2.99$\n",
    "\n",
    "## 4. Normal Distribution\n",
    "\n",
    "### Definition\n",
    "- Symmetric, unimodal, bell-shaped curve\n",
    "- Parametrized by mean Œº and standard deviation œÉ\n",
    "- Notation: $X \\sim \\mathcal{N}(\\mu, \\sigma)$\n",
    "\n",
    "### Probability Density Function (PDF)\n",
    "$$f_X(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}$$\n",
    "\n",
    "### Standard Normal Distribution\n",
    "- When $\\mu = 0$ and $\\sigma = 1$, we have the standard normal distribution: $\\mathcal{N}(0,1)$\n",
    "- Often denoted as Z\n",
    "\n",
    "### Z-scores\n",
    "- The Z-score of an observation x is the number of standard deviations it falls above or below the mean:\n",
    "  $$Z = \\frac{x - \\mu}{\\sigma}$$\n",
    "- Z-scores allow us to standardize data for comparison\n",
    "- Z-scores follow the standard normal distribution\n",
    "\n",
    "### Finding Tail Areas\n",
    "- Methods to calculate probabilities:\n",
    "  - Statistical software (R, Python, MATLAB)\n",
    "  - Probability tables\n",
    "  - Graphing calculators\n",
    "\n",
    "### Example: SAT Scores\n",
    "If SAT scores follow $\\mathcal{N}(1100, 200)$ and Ann scores 1300:\n",
    "- Z-score: $Z = \\frac{1300-1100}{200} = 1$\n",
    "- Probability of scoring below 1300: $\\mathbb{P}(X \\leq 1300) \\approx 0.8413$\n",
    "\n",
    "### Example: Test Scores\n",
    "For a test with scores normally distributed with mean 100 and standard deviation 15:\n",
    "- The interquartile range (IQR) can be calculated using Z-scores:\n",
    "  - Q1 corresponds to Z = -0.6745\n",
    "  - Q3 corresponds to Z = 0.6745\n",
    "  - Q1 = 100 + (-0.6745 √ó 15) = 89.88\n",
    "  - Q3 = 100 + (0.6745 √ó 15) = 110.12\n",
    "  - IQR = Q3 - Q1 = 20.24\n",
    "\n",
    "## 5. Approximating Binomial with Normal Distribution\n",
    "\n",
    "### Conditions for Approximation\n",
    "- The binomial distribution $B(n,p)$ is approximately normal when:\n",
    "  - $np \\geq 10$\n",
    "  - $n(1-p) \\geq 10$\n",
    "\n",
    "### Parameters for Approximation\n",
    "- Use $\\mathcal{N}(\\mu, \\sigma)$ where:\n",
    "  - $\\mu = np$\n",
    "  - $\\sigma = \\sqrt{np(1-p)}$\n",
    "\n",
    "### Example: Defective Light Bulbs\n",
    "If 2% of light bulbs are defective, what is the probability of getting 30 or fewer defective bulbs in a batch of 1000?\n",
    "- Check conditions: $np = 1000 \\times 0.02 = 20 \\geq 10$ and $n(1-p) = 980 \\geq 10$\n",
    "- Use normal approximation with $\\mu = 20$ and $\\sigma = \\sqrt{1000 \\times 0.02 \\times 0.98} \\approx 4.43$\n",
    "- Calculate Z-score: $Z = \\frac{30-20}{4.43} \\approx 2.26$\n",
    "- Probability: $\\mathbb{P}(X \\leq 30) = \\mathbb{P}(Z \\leq 2.26) \\approx 0.988$\n",
    "\n",
    "## 6. Theoretical Foundations\n",
    "\n",
    "### Law of Large Numbers (LLN)\n",
    "- If $X_1, X_2, ...$ is a sequence of independent and identically distributed random variables with expected value $\\mu$, then the sample average $\\bar{X}_n = \\frac{X_1 + ... + X_n}{n}$ satisfies:\n",
    "  $$\\mathbb{P}(\\lim_{n\\to\\infty} \\bar{X}_n = \\mu) = 1$$\n",
    "- Intuitively: As we increase the number of trials, the sample mean converges to the expected value\n",
    "\n",
    "### Central Limit Theorem (CLT)\n",
    "- If $X_1, X_2, ...$ is a sequence of independent and identically distributed random variables with expected value $\\mu$ and variance $\\sigma^2 < \\infty$, then:\n",
    "  $$\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2)$$\n",
    "- Intuitively: The distribution of the sample mean approaches a normal distribution as sample size increases\n",
    "- This is why the normal distribution is so prevalent in statistics\n",
    "\n",
    "## 7. Other Important Distributions\n",
    "\n",
    "### Chi-Squared Distribution\n",
    "- If $Z_1, Z_2, ..., Z_k$ are independent standard normal random variables, then $Q = \\sum_{i=1}^{k} Z_i^2$ follows a chi-squared distribution with k degrees of freedom\n",
    "- Notation: $Q \\sim \\chi^2_k$\n",
    "- Used in goodness-of-fit tests and confidence intervals for variance\n",
    "- PDF becomes less skewed as degrees of freedom increase\n",
    "\n",
    "### t-Distribution\n",
    "- Used when the population variance is unknown and estimated from the data\n",
    "- If the sample variance $s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X})^2$ is used instead of $\\sigma^2$, then $\\frac{\\bar{X} - \\mu}{s/\\sqrt{n}}$ follows a t-distribution with n-1 degrees of freedom\n",
    "- Has heavier tails than the normal distribution\n",
    "- Approaches the standard normal distribution as degrees of freedom increase\n",
    "\n",
    "### F-Distribution\n",
    "- Ratio of two chi-squared distributions, each divided by their degrees of freedom:\n",
    "  $$F = \\frac{U/d_1}{V/d_2}$$\n",
    "- Where $U \\sim \\chi^2_{d_1}$ and $V \\sim \\chi^2_{d_2}$\n",
    "- Used in ANOVA and testing equality of variances\n",
    "- Parametrized by two degrees of freedom parameters: $d_1$ and $d_2$\n",
    "\n",
    "### Example: F-Distribution\n",
    "If $W = \\frac{\\chi^2_3/3}{\\chi^2_2/2}$, then W follows an F-distribution with degrees of freedom 3 and 2, denoted as $F_{3,2}$.\n",
    "\n",
    "## 8. Practical Applications in Statistical Inference\n",
    "\n",
    "### Hypothesis Testing\n",
    "- Test statistics often follow specific distributions under the null hypothesis\n",
    "- The distribution of the test statistic is used to calculate p-values\n",
    "- Example: For testing if a coin is fair after 100 flips with 60 heads:\n",
    "  - Test statistic: $Z = \\frac{60-50}{5} = 2$\n",
    "  - Under the null hypothesis (p = 0.5), Z follows approximately $\\mathcal{N}(0,1)$\n",
    "  - P-value: $\\mathbb{P}(Z \\geq 2) \\approx 0.0228$\n",
    "\n",
    "### Confidence Intervals\n",
    "- Different distributions are used depending on what parameter is being estimated\n",
    "- Normal distribution: Used when population variance is known\n",
    "- t-distribution: Used when population variance is unknown\n",
    "- Chi-squared distribution: Used for variance estimation\n",
    "\n",
    "## 9. Key Properties of Random Variables\n",
    "\n",
    "### Discrete vs. Continuous Random Variables\n",
    "- **Discrete Random Variables**:\n",
    "  - Take values from a countable set\n",
    "  - Have a probability mass function (PMF)\n",
    "  - Example: Number of points scored in a basketball game\n",
    "  - For discrete RVs, $\\mathbb{P}(X = x)$ can be positive\n",
    "\n",
    "- **Continuous Random Variables**:\n",
    "  - Take values from an uncountable set (typically an interval)\n",
    "  - Have a probability density function (PDF)\n",
    "  - Example: Distance walked in a day\n",
    "  - For continuous RVs, $\\mathbb{P}(X = x) = 0$ for any specific value x\n",
    "\n",
    "### Properties of Symmetric Distributions\n",
    "- If Z has a symmetric distribution around 0 and $\\mathbb{P}(Z < a) = 0.25$, then:\n",
    "  - $\\mathbb{P}(Z > -a) = 1 - \\mathbb{P}(Z \\leq -a) = 1 - \\mathbb{P}(Z < -a) = 1 - 0.25 = 0.75$\n",
    "  - This is because for symmetric distributions, $\\mathbb{P}(Z < -a) = \\mathbb{P}(Z > a)$\n",
    "\n",
    "## 10. Key Takeaways\n",
    "\n",
    "1. Different distributions model different types of random phenomena\n",
    "2. The normal distribution is central to statistical inference due to the Central Limit Theorem\n",
    "3. Z-scores standardize observations for comparison across different distributions\n",
    "4. The binomial distribution can be approximated by the normal distribution under certain conditions\n",
    "5. The t-distribution, chi-squared distribution, and F-distribution are crucial for statistical inference when population parameters are unknown\n",
    "6. Understanding the properties of these distributions is essential for hypothesis testing and confidence interval construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 215: Probability and Statistics for Data Science\n",
    "## Midterm Exam Analysis and Solution Guide\n",
    "\n",
    "## Part A: Exam Summary and Solution Guide\n",
    "\n",
    "### Overview\n",
    "This midterm exam covers material from Modules 1-4, testing students' understanding of:\n",
    "- Research design and variable identification\n",
    "- Descriptive statistics and data distribution\n",
    "- Probability and random variables\n",
    "- Normal distribution and standardization\n",
    "\n",
    "The exam consists of 5 questions with multiple parts, totaling approximately 50 points.\n",
    "\n",
    "### Question 1: Research Design Analysis\n",
    "\n",
    "**Topic: Research Design and Variables (Module 1)**\n",
    "\n",
    "This question presents a study on children's honesty and cheating behavior, asking students to:\n",
    "- Identify the main research question\n",
    "- Identify the subjects and sample size\n",
    "- Identify variables and their types\n",
    "\n",
    "#### Solution Guide:\n",
    "\n",
    "**Part (a): Identify the main research question**\n",
    "- The main research question is: \"Does explicitly telling children not to cheat affect their likelihood to cheat?\"\n",
    "- This question identifies the explanatory variable (explicit instruction vs. no instruction) and the response variable (cheating behavior).\n",
    "\n",
    "**Part (b): Identify subjects and sample size**\n",
    "- Subjects: Children between the ages of 5 and 15 (or 10 and 15 in version 2)\n",
    "- Sample size: 160 children (or 100 children in version 2)\n",
    "\n",
    "**Part (c): Identify variables and their types**\n",
    "Four variables were recorded:\n",
    "1. Age (numerical, continuous)\n",
    "2. Sex (categorical)\n",
    "3. Whether they were an only child or not (categorical)\n",
    "4. Whether they cheated or not (categorical)\n",
    "\n",
    "**Key Concepts Applied:**\n",
    "- Distinguishing between numerical and categorical variables\n",
    "- Identifying explanatory and response variables\n",
    "- Understanding research design elements\n",
    "\n",
    "### Question 2: Descriptive Statistics and Distribution Shape\n",
    "\n",
    "**Topic: Summarizing Data (Module 2)**\n",
    "\n",
    "This question provides summary statistics for exam grades and asks students to:\n",
    "- Determine if the distribution is left-skewed, right-skewed, or symmetric\n",
    "- Identify if there are any outliers\n",
    "\n",
    "#### Solution Guide:\n",
    "\n",
    "**Part (a): Determine the shape of the distribution**\n",
    "\n",
    "For Version 1:\n",
    "- Mean = 78, Median = 76\n",
    "- Since Mean > Median, this suggests a right-skewed distribution\n",
    "- However, Q1 (60.25) is farther from the median than Q3 (86.5), which suggests left skew\n",
    "- The distribution is slightly left-skewed\n",
    "\n",
    "For Version 2:\n",
    "- Mean = 80, Median = 84\n",
    "- Since Mean < Median, this suggests a left-skewed distribution\n",
    "- However, Q3 (95) is farther from the median than Q1 (76), which suggests right skew\n",
    "- The distribution is slightly right-skewed\n",
    "\n",
    "**Part (b): Identify outliers**\n",
    "\n",
    "For Version 1:\n",
    "- Calculate the IQR = Q3 - Q1 = 86.5 - 60.25 = 26.25\n",
    "- Lower fence = Q1 - 1.5 √ó IQR = 60.25 - 1.5 √ó 26.25 = 20.875\n",
    "- Upper fence = Q3 + 1.5 √ó IQR = 86.5 + 1.5 √ó 26.25 = 125.875\n",
    "- Min = 30, Max = 95\n",
    "- Since Min > Lower fence and Max < Upper fence, there are no outliers\n",
    "\n",
    "For Version 2:\n",
    "- Calculate the IQR = Q3 - Q1 = 95 - 76 = 19\n",
    "- Lower fence = Q1 - 1.5 √ó IQR = 76 - 1.5 √ó 19 = 47.5\n",
    "- Upper fence = Q3 + 1.5 √ó IQR = 95 + 1.5 √ó 19 = 123.5\n",
    "- Min = 45, Max = 99\n",
    "- Since Min < Lower fence, there is at least one outlier (on the lower end)\n",
    "\n",
    "**Key Concepts Applied:**\n",
    "- Relationship between mean, median, and skewness\n",
    "- Interquartile range (IQR) calculation\n",
    "- Outlier identification using the 1.5 √ó IQR rule\n",
    "\n",
    "### Question 3: Expected Value and Variance of Random Variables\n",
    "\n",
    "**Topic: Random Variables (Module 3)**\n",
    "\n",
    "This question tests understanding of expected value and variance properties for independent random variables, asking students to find the mean and standard deviation of linear combinations.\n",
    "\n",
    "#### Solution Guide:\n",
    "\n",
    "**Properties Used:**\n",
    "For independent random variables X and Y:\n",
    "- E[aX + bY + c] = aE[X] + bE[Y] + c\n",
    "- Var(aX + bY + c) = a¬≤Var(X) + b¬≤Var(Y)\n",
    "- SD(aX + bY + c) = ‚àöVar(aX + bY + c)\n",
    "\n",
    "**Version 1 Solutions:**\n",
    "\n",
    "Given:\n",
    "- E[X] = 10, SD(X) = 2, E[Y] = 20, SD(Y) = 3\n",
    "\n",
    "Part (a): Find mean and SD of X + 3Y\n",
    "- E[X + 3Y] = E[X] + 3E[Y] = 10 + 3(20) = 10 + 60 = 70\n",
    "- Var(X + 3Y) = Var(X) + 9Var(Y) = 2¬≤ + 9(3¬≤) = 4 + 9(9) = 4 + 81 = 85\n",
    "- SD(X + 3Y) = ‚àö85 ‚âà 9.22\n",
    "\n",
    "Part (b): Find mean and SD of 2X - Y - 6\n",
    "- E[2X - Y - 6] = 2E[X] - E[Y] - 6 = 2(10) - 20 - 6 = 20 - 20 - 6 = -6\n",
    "- Var(2X - Y - 6) = 4Var(X) + Var(Y) = 4(4) + 9 = 16 + 9 = 25\n",
    "- SD(2X - Y - 6) = ‚àö25 = 5\n",
    "\n",
    "**Version 2 Solutions:**\n",
    "\n",
    "Given:\n",
    "- E[X] = 40, Var(X) = 16, E[Y] = 20, Var(Y) = 9\n",
    "\n",
    "Part (a): Find mean and SD of 4X - 40\n",
    "- E[4X - 40] = 4E[X] - 40 = 4(40) - 40 = 160 - 40 = 120\n",
    "- Var(4X - 40) = 16Var(X) = 16(16) = 256\n",
    "- SD(4X - 40) = ‚àö256 = 16\n",
    "\n",
    "Part (b): Find mean and SD of X - Y\n",
    "- E[X - Y] = E[X] - E[Y] = 40 - 20 = 20\n",
    "- Var(X - Y) = Var(X) + Var(Y) = 16 + 9 = 25\n",
    "- SD(X - Y) = ‚àö25 = 5\n",
    "\n",
    "**Key Concepts Applied:**\n",
    "- Properties of expected value for linear combinations\n",
    "- Properties of variance for independent random variables\n",
    "- Relationship between variance and standard deviation\n",
    "\n",
    "### Question 4: Normal Distribution Probabilities\n",
    "\n",
    "**Topic: Normal Distribution (Module 4)**\n",
    "\n",
    "This question tests understanding of the normal distribution and standardization, asking students to find probabilities for normally distributed test scores.\n",
    "\n",
    "#### Solution Guide:\n",
    "\n",
    "**Key Formula:**\n",
    "For a normal random variable X with mean Œº and standard deviation œÉ:\n",
    "- Z = (X - Œº)/œÉ follows the standard normal distribution\n",
    "- Use Z-scores to find probabilities using standard normal tables or calculators\n",
    "\n",
    "**Version 1 Solutions:**\n",
    "\n",
    "Given:\n",
    "- Test scores are normally distributed with mean Œº = 100 and standard deviation œÉ = 15\n",
    "\n",
    "Part (a): Find P(X > 90)\n",
    "- Z = (90 - 100)/15 = -0.67\n",
    "- P(X > 90) = P(Z > -0.67) = 1 - P(Z < -0.67) = 1 - 0.251 = 0.749\n",
    "\n",
    "Part (b): Find P(112 < X < 132)\n",
    "- Z‚ÇÅ = (112 - 100)/15 = 0.8\n",
    "- Z‚ÇÇ = (132 - 100)/15 = 2.13\n",
    "- P(112 < X < 132) = P(0.8 < Z < 2.13) = P(Z < 2.13) - P(Z < 0.8) = 0.983 - 0.788 = 0.195 ‚âà 0.19\n",
    "\n",
    "**Version 2 Solutions:**\n",
    "\n",
    "Given:\n",
    "- Test scores are normally distributed with mean Œº = 1500 and standard deviation œÉ = 300\n",
    "\n",
    "Part (a): Find P(X < 1600)\n",
    "- Z = (1600 - 1500)/300 = 0.33\n",
    "- P(X < 1600) = P(Z < 0.33) = 0.629\n",
    "\n",
    "Part (b): Find P(1200 < X < 1700)\n",
    "- Z‚ÇÅ = (1200 - 1500)/300 = -1\n",
    "- Z‚ÇÇ = (1700 - 1500)/300 = 0.67\n",
    "- P(1200 < X < 1700) = P(-1 < Z < 0.67) = P(Z < 0.67) - P(Z < -1) = 0.749 - 0.159 = 0.59\n",
    "\n",
    "**Key Concepts Applied:**\n",
    "- Standardizing normal random variables\n",
    "- Using Z-scores to find probabilities\n",
    "- Finding probabilities for intervals\n",
    "\n",
    "### Question 5: Probability and Independence\n",
    "\n",
    "**Topic: Probability Concepts (Module 3)**\n",
    "\n",
    "This question tests understanding of probability, independence, and conditional probability in the context of a classroom scenario.\n",
    "\n",
    "#### Solution Guide:\n",
    "\n",
    "Given:\n",
    "- 20 students total\n",
    "- 10 students have brown eyes (event E)\n",
    "- 8 students are left-handed (event F)\n",
    "- 3 students have brown eyes and are left-handed (event E ‚à© F)\n",
    "\n",
    "**Part (a): Determine if events E and F are independent**\n",
    "\n",
    "Two events are independent if P(E ‚à© F) = P(E) √ó P(F)\n",
    "\n",
    "Calculate:\n",
    "- P(E) = 10/20 = 0.5\n",
    "- P(F) = 8/20 = 0.4\n",
    "- P(E ‚à© F) = 3/20 = 0.15\n",
    "- P(E) √ó P(F) = 0.5 √ó 0.4 = 0.2\n",
    "\n",
    "Since P(E ‚à© F) = 0.15 ‚â† 0.2 = P(E) √ó P(F), the events E and F are not independent.\n",
    "\n",
    "**Part (b): Find the probability that a left-handed student does not have brown eyes**\n",
    "\n",
    "This is asking for P(E^c | F), where E^c is the complement of E (not having brown eyes).\n",
    "\n",
    "Method 1:\n",
    "- P(E^c | F) = P(E^c ‚à© F)/P(F)\n",
    "- Number of left-handed students without brown eyes = 8 - 3 = 5\n",
    "- P(E^c ‚à© F) = 5/20 = 0.25\n",
    "- P(E^c | F) = 0.25/0.4 = 5/8 = 0.625\n",
    "\n",
    "Method 2:\n",
    "- P(E^c | F) = 1 - P(E | F)\n",
    "- P(E | F) = P(E ‚à© F)/P(F) = 3/8 = 0.375\n",
    "- P(E^c | F) = 1 - 0.375 = 0.625\n",
    "\n",
    "The probability that a left-handed student does not have brown eyes is 5/8 or 0.625.\n",
    "\n",
    "**Key Concepts Applied:**\n",
    "- Definition of independence\n",
    "- Conditional probability\n",
    "- Complement of events\n",
    "\n",
    "## Part B: Trend Analysis\n",
    "\n",
    "### Relationship Between Exam Questions and Module Content\n",
    "\n",
    "#### Module 1 Coverage\n",
    "**Question 1** directly tests concepts from Module 1:\n",
    "- Research design and methodology\n",
    "- Identifying variables and their types (categorical vs. numerical)\n",
    "- Understanding subjects and sampling\n",
    "\n",
    "This question aligns with Module 1's focus on the foundations of statistics, data collection methods, and variable classification. The question tests students' ability to analyze a research study and identify its key components.\n",
    "\n",
    "#### Module 2 Coverage\n",
    "**Question 2** tests concepts from Module 2:\n",
    "- Descriptive statistics (mean, median, quartiles)\n",
    "- Distribution shape (skewness)\n",
    "- Outlier identification using IQR\n",
    "\n",
    "This question directly applies the concepts of summarizing numerical data, understanding distribution shapes, and identifying outliers using the 1.5 √ó IQR rule, which are core topics in Module 2.\n",
    "\n",
    "#### Module 3 Coverage\n",
    "**Questions 3 and 5** test concepts from Module 3:\n",
    "- Expected value and variance of random variables (Question 3)\n",
    "- Properties of linear combinations of random variables (Question 3)\n",
    "- Probability concepts, independence, and conditional probability (Question 5)\n",
    "\n",
    "These questions assess students' understanding of probability theory, random variables, and their properties, which are the main focus of Module 3.\n",
    "\n",
    "#### Module 4 Coverage\n",
    "**Question 4** tests concepts from Module 4:\n",
    "- Normal distribution\n",
    "- Standardization (Z-scores)\n",
    "- Finding probabilities using the standard normal distribution\n",
    "\n",
    "This question directly applies the concepts of the normal distribution and using Z-scores to find probabilities, which are central topics in Module 4.\n",
    "\n",
    "### Trends and Patterns in Exam Emphasis\n",
    "\n",
    "1. **Equal Coverage Across Modules**\n",
    "   - The exam provides balanced coverage of all four modules, with approximately one question per module (with Module 3 having slightly more emphasis with two questions).\n",
    "\n",
    "2. **Focus on Computational Skills**\n",
    "   - Most questions require calculations and application of formulas rather than theoretical explanations.\n",
    "   - Students need to demonstrate their ability to apply statistical concepts to solve problems.\n",
    "\n",
    "3. **Real-World Applications**\n",
    "   - Questions are framed in real-world contexts (research studies, test scores, classroom scenarios).\n",
    "   - This emphasizes the practical application of statistical concepts.\n",
    "\n",
    "4. **Multiple Versions of Questions**\n",
    "   - The exam includes multiple versions of the same question with different numerical values.\n",
    "   - This suggests an emphasis on understanding the underlying concepts rather than memorizing specific solutions.\n",
    "\n",
    "5. **Progressive Difficulty**\n",
    "   - The exam progresses from more straightforward conceptual questions (Question 1) to more complex computational problems (Questions 3-5).\n",
    "   - This allows students to demonstrate both basic understanding and advanced application.\n",
    "\n",
    "6. **Integration of Concepts**\n",
    "   - Some questions require integrating concepts from multiple modules.\n",
    "   - For example, Question 5 combines probability concepts with conditional probability.\n",
    "\n",
    "7. **Emphasis on Core Statistical Skills**\n",
    "   - The exam focuses on fundamental statistical skills that form the foundation for more advanced topics:\n",
    "     - Understanding research design\n",
    "     - Summarizing and interpreting data\n",
    "     - Working with probability and random variables\n",
    "     - Using the normal distribution\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The midterm exam provides a comprehensive assessment of students' understanding of the first four modules of the course. It emphasizes computational skills, application of statistical concepts to real-world scenarios, and integration of concepts across modules. The balanced coverage ensures that students have a solid foundation in the fundamental principles of probability and statistics before moving on to more advanced topics in later modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 5 Summary: Introduction to Statistical Inference\n",
    "\n",
    "## 1. Introduction to Statistical Inference\n",
    "\n",
    "### What is Statistical Inference?\n",
    "- **Definition**: The process by which we estimate parameters of interest from data and quantify the uncertainty in our estimates\n",
    "- **Key Components**:\n",
    "  - Point estimates: Single values that estimate population parameters\n",
    "  - Confidence intervals: Ranges of plausible values for population parameters\n",
    "  - Hypothesis tests: Methods to evaluate claims about the population\n",
    "\n",
    "### Population Parameters vs. Sample Statistics\n",
    "- **Population Parameter**: A numerical characteristic of the entire population (usually unknown)\n",
    "  - Example: The true proportion of all voters who support a candidate (p)\n",
    "- **Sample Statistic**: A numerical characteristic calculated from a sample (known)\n",
    "  - Example: The proportion of sampled voters who support a candidate (pÃÇ)\n",
    "\n",
    "### Sources of Error in Estimation\n",
    "- **Sampling Error**: Variability that occurs due to random sampling\n",
    "  - Related to sample size (n)\n",
    "  - Unavoidable but can be quantified\n",
    "- **Bias**: Systematic error that causes estimates to consistently deviate from the true parameter\n",
    "  - Examples: Non-response bias, selection bias, poorly worded questions\n",
    "  - Can be minimized through proper study design\n",
    "\n",
    "## 2. Sampling Distribution of a Proportion\n",
    "\n",
    "### Sampling Distribution\n",
    "- **Definition**: The distribution of a sample statistic over all possible samples of the same size from the same population\n",
    "- **Properties of the Sampling Distribution of pÃÇ**:\n",
    "  - Center: The mean of pÃÇ is p (the population parameter)\n",
    "  - Spread: The standard error of pÃÇ is $\\sqrt{\\frac{p(1-p)}{n}}$\n",
    "  - Shape: Approximately normal for large enough samples\n",
    "\n",
    "### Standard Error\n",
    "- **Definition**: The standard deviation of the sampling distribution\n",
    "- **Formula for Proportion**: \n",
    "  $$SE_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}$$\n",
    "- **Interpretation**: Measures the typical deviation between the sample proportion and the population proportion\n",
    "- **Example**: For a sample of 1000 people with p = 0.75:\n",
    "  $$SE_{\\hat{p}} = \\sqrt{\\frac{0.75 \\times 0.25}{1000}} \\approx 0.0137$$\n",
    "\n",
    "### Central Limit Theorem (CLT) for Proportions\n",
    "- **Statement**: For large enough samples, the sampling distribution of pÃÇ is approximately normal with:\n",
    "  - Mean: Œº = p\n",
    "  - Standard error: $SE_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{n}}$\n",
    "\n",
    "- **Success/Failure Condition**: For the CLT to apply, we need:\n",
    "  - np ‚â• 10 (at least 10 successes)\n",
    "  - n(1-p) ‚â• 10 (at least 10 failures)\n",
    "\n",
    "### Using the CLT in Practice\n",
    "- Since p is unknown in real applications, we use pÃÇ to:\n",
    "  1. Check the success/failure condition: npÃÇ ‚â• 10 and n(1-pÃÇ) ‚â• 10\n",
    "  2. Estimate the standard error: $SE_{\\hat{p}} \\approx \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}$\n",
    "\n",
    "### Example: Applying the CLT\n",
    "If 761 out of 1000 randomly sampled people support candidate A:\n",
    "- Sample proportion: pÃÇ = 761/1000 = 0.761\n",
    "- Estimated standard error: $SE_{\\hat{p}} \\approx \\sqrt{\\frac{0.761 \\times 0.239}{1000}} \\approx 0.0135$\n",
    "- Success/failure check: \n",
    "  - npÃÇ = 1000 √ó 0.761 = 761 ‚â• 10 ‚úì\n",
    "  - n(1-pÃÇ) = 1000 √ó 0.239 = 239 ‚â• 10 ‚úì\n",
    "\n",
    "## 3. Confidence Intervals for a Proportion\n",
    "\n",
    "### Definition and Interpretation\n",
    "- **Confidence Interval**: A range of plausible values for the population parameter\n",
    "- **General Form**: Point estimate ¬± Margin of error\n",
    "- **Interpretation**: If we repeatedly collect samples and construct confidence intervals using the same method, approximately (confidence level)% of these intervals would contain the true population parameter\n",
    "\n",
    "### Formula for Confidence Interval\n",
    "- **Formula**: \n",
    "  $$\\hat{p} \\pm z^* \\times SE_{\\hat{p}}$$\n",
    "  where z* is the critical value corresponding to the desired confidence level\n",
    "\n",
    "- **Margin of Error**: \n",
    "  $$MOE = z^* \\times SE_{\\hat{p}}$$\n",
    "\n",
    "### Common Critical Values (z*)\n",
    "- 90% confidence: z* = 1.64\n",
    "- 95% confidence: z* = 1.96\n",
    "- 99% confidence: z* = 2.58\n",
    "\n",
    "### Example: Constructing a 95% Confidence Interval\n",
    "For a sample where 761 out of 1000 people support candidate A:\n",
    "- pÃÇ = 0.761\n",
    "- SE = 0.0135\n",
    "- 95% confidence interval:\n",
    "  $$0.761 \\pm 1.96 \\times 0.0135 = 0.761 \\pm 0.0265 = (0.7345, 0.7875)$$\n",
    "\n",
    "- Interpretation: We are 95% confident that the true proportion of people who support candidate A is between 73.45% and 78.75%.\n",
    "\n",
    "### Effect of Confidence Level on Interval Width\n",
    "- Higher confidence level ‚Üí Wider interval\n",
    "- Lower confidence level ‚Üí Narrower interval\n",
    "- Trade-off between precision (narrow interval) and confidence (high percentage)\n",
    "\n",
    "## 4. Sample Size Determination\n",
    "\n",
    "### Determining Sample Size for a Desired Margin of Error\n",
    "- **Formula**: \n",
    "  $$n = \\frac{z^{*2} \\times p(1-p)}{MOE^2}$$\n",
    "\n",
    "- Since p is unknown before sampling, we can:\n",
    "  1. Use a previous estimate of p\n",
    "  2. Use p = 0.5 (which maximizes p(1-p) and gives the largest, most conservative sample size)\n",
    "\n",
    "### Example: Sample Size Calculation\n",
    "To achieve a margin of error of 5% with 95% confidence:\n",
    "- z* = 1.96\n",
    "- Using p = 0.5 (conservative approach):\n",
    "  $$n = \\frac{1.96^2 \\times 0.5 \\times 0.5}{0.05^2} = \\frac{0.9604}{0.0025} = 384.16$$\n",
    "- We would need at least 385 participants.\n",
    "\n",
    "## 5. Common Misconceptions about Confidence Intervals\n",
    "\n",
    "### What a Confidence Interval IS:\n",
    "- A range of plausible values for the population parameter\n",
    "- A procedure that, when repeated, captures the true parameter at the specified rate\n",
    "- A measure of the reliability of our estimation method\n",
    "\n",
    "### What a Confidence Interval IS NOT:\n",
    "- A probability statement about the parameter (once calculated, the interval either contains the parameter or it doesn't)\n",
    "- A statement that X% of the population falls within the interval\n",
    "- A guarantee that the true parameter is in any specific interval\n",
    "\n",
    "## 6. Practical Examples\n",
    "\n",
    "### Example 1: Teenage Phone Usage\n",
    "A random sample of 1000 teenagers were interviewed about their average daily phone use. About 60% said they spent around 5-7 hours on their phones per day.\n",
    "\n",
    "- Sample proportion: pÃÇ = 0.6\n",
    "- Standard error: $SE_{\\hat{p}} = \\sqrt{\\frac{0.6 \\times 0.4}{1000}} = 0.015$\n",
    "- 95% confidence interval: \n",
    "  $$0.6 \\pm 1.96 \\times 0.015 = 0.6 \\pm 0.029 = (0.571, 0.629)$$\n",
    "\n",
    "- Interpretation: We are 95% confident that the true proportion of teenagers who spend 5-7 hours on their phones daily is between 57.1% and 62.9%.\n",
    "\n",
    "### Example 2: Roller Coaster Survey\n",
    "According to a survey, around 38% of American teenagers are scared to ride roller coasters. This survey was conducted based on a random sample of 800 teenagers.\n",
    "\n",
    "- Checking CLT conditions:\n",
    "  - npÃÇ = 800 √ó 0.38 = 304 ‚â• 10 ‚úì\n",
    "  - n(1-pÃÇ) = 800 √ó 0.62 = 496 ‚â• 10 ‚úì\n",
    "- Standard error: $SE_{\\hat{p}} = \\sqrt{\\frac{0.38 \\times 0.62}{800}} \\approx 0.017$\n",
    "- 95% confidence interval:\n",
    "  $$0.38 \\pm 1.96 \\times 0.017 = 0.38 \\pm 0.033 = (0.347, 0.413)$$\n",
    "\n",
    "### Example 3: E-book Preferences\n",
    "A research firm wants to find the proportion of adults who prefer traditional books to E-books. They took a random sample of 200 adults and found that 65% like E-books better.\n",
    "\n",
    "- Standard error: $SE_{\\hat{p}} = \\sqrt{\\frac{0.65 \\times 0.35}{200}} \\approx 0.034$\n",
    "- For an 80% confidence interval, z* = 1.28\n",
    "- 80% confidence interval:\n",
    "  $$0.65 \\pm 1.28 \\times 0.034 = 0.65 \\pm 0.044 = (0.606, 0.694)$$\n",
    "\n",
    "- Interpretation: We are 80% confident that the true proportion of adults who prefer E-books is between 60.6% and 69.4%.\n",
    "\n",
    "### Example 4: Sample Size for Delivery Company\n",
    "A delivery company wants to estimate the proportion of on-time deliveries with a margin of error of 3% at 95% confidence. How many customers should they survey?\n",
    "\n",
    "- Using pÃÇ = 0.5 (conservative approach):\n",
    "  $$n = \\frac{1.96^2 \\times 0.5 \\times 0.5}{0.03^2} = \\frac{0.9604}{0.0009} = 1067.11$$\n",
    "- They should survey at least 1068 customers.\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "1. Statistical inference allows us to make educated guesses about population parameters based on sample data.\n",
    "\n",
    "2. The Central Limit Theorem tells us that for large enough samples, the sampling distribution of pÃÇ is approximately normal, regardless of the shape of the population distribution.\n",
    "\n",
    "3. Confidence intervals provide a range of plausible values for the population parameter, along with a measure of confidence in that range.\n",
    "\n",
    "4. The width of a confidence interval is affected by:\n",
    "   - Sample size (n): Larger samples lead to narrower intervals\n",
    "   - Confidence level: Higher confidence requires wider intervals\n",
    "   - Population variability: More variable populations require wider intervals\n",
    "\n",
    "5. When interpreting confidence intervals, be careful to avoid common misconceptions:\n",
    "   - A 95% confidence interval does not mean there is a 95% probability that the parameter is in that specific interval\n",
    "   - Instead, it means that if we repeated the sampling process many times, about 95% of the resulting intervals would contain the true parameter\n",
    "\n",
    "6. Sample size calculations help us determine how many observations we need to achieve a desired level of precision in our estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 6 Summary: Hypothesis Testing for a Proportion\n",
    "\n",
    "## 1. Introduction to Hypothesis Testing\n",
    "\n",
    "### The Hypothesis Testing Framework\n",
    "- **Definition**: A formal procedure for evaluating claims about a population parameter\n",
    "- **Purpose**: To determine whether sample data provides sufficient evidence against a specified claim\n",
    "- **Key Components**:\n",
    "  - Null Hypothesis (H‚ÇÄ): The \"status quo\" or \"skeptical\" perspective\n",
    "  - Alternative Hypothesis (H‚ÇÅ or H<sub>A</sub>): The claim being investigated\n",
    "\n",
    "### Types of Hypotheses\n",
    "- **Null Hypothesis (H‚ÇÄ)**:\n",
    "  - Makes a specific claim about the parameter (often that there is \"no effect\" or \"no difference\")\n",
    "  - Always contains an equality (=, ‚â§, or ‚â•)\n",
    "  - Example: H‚ÇÄ: p = 0.5 (the proportion equals 0.5)\n",
    "\n",
    "- **Alternative Hypothesis (H<sub>A</sub>)**:\n",
    "  - The claim we are looking for evidence to support\n",
    "  - Contains only inequalities (<, >, or ‚â†)\n",
    "  - Example: H<sub>A</sub>: p > 0.5 (the proportion is greater than 0.5)\n",
    "\n",
    "### Types of Hypothesis Tests\n",
    "- **Two-sided test**: H<sub>A</sub> claims the parameter is different from the null value (‚â†)\n",
    "  - Example: H‚ÇÄ: p = 0.5 vs. H<sub>A</sub>: p ‚â† 0.5\n",
    "- **One-sided test**: H<sub>A</sub> claims the parameter is either greater than (>) or less than (<) the null value\n",
    "  - Example: H‚ÇÄ: p ‚â§ 0.5 vs. H<sub>A</sub>: p > 0.5\n",
    "  - Example: H‚ÇÄ: p ‚â• 0.5 vs. H<sub>A</sub>: p < 0.5\n",
    "\n",
    "## 2. The Logic of Hypothesis Testing\n",
    "\n",
    "### Decision Process\n",
    "- We start by assuming the null hypothesis is true\n",
    "- We collect sample data and calculate a test statistic\n",
    "- We determine how likely our observed result (or more extreme) would be if H‚ÇÄ were true\n",
    "- Based on this probability (p-value), we either:\n",
    "  - Reject H‚ÇÄ if the evidence against it is strong enough\n",
    "  - Fail to reject H‚ÇÄ if the evidence is not strong enough\n",
    "\n",
    "### Important Note\n",
    "- Failing to reject H‚ÇÄ does not mean we accept or prove H‚ÇÄ\n",
    "- It simply means we don't have enough evidence to reject it\n",
    "- This is similar to \"innocent until proven guilty\" in a legal system\n",
    "\n",
    "## 3. Errors in Hypothesis Testing\n",
    "\n",
    "### Type I Error\n",
    "- **Definition**: Rejecting a true null hypothesis\n",
    "- **Probability**: Œ± (significance level)\n",
    "- **Example**: Concluding a medical treatment is effective when it actually isn't\n",
    "\n",
    "### Type II Error\n",
    "- **Definition**: Failing to reject a false null hypothesis\n",
    "- **Probability**: Œ≤\n",
    "- **Example**: Failing to detect that a medical treatment is effective when it actually is\n",
    "\n",
    "### Relationship Between Errors\n",
    "- There is a trade-off between Type I and Type II errors\n",
    "- Decreasing Œ± (making it harder to reject H‚ÇÄ) increases Œ≤\n",
    "- Increasing Œ± (making it easier to reject H‚ÇÄ) decreases Œ≤\n",
    "\n",
    "### Power of a Test\n",
    "- **Definition**: The probability of correctly rejecting a false null hypothesis (1 - Œ≤)\n",
    "- **Factors affecting power**:\n",
    "  - Sample size (larger samples increase power)\n",
    "  - Effect size (larger differences from H‚ÇÄ are easier to detect)\n",
    "  - Significance level (higher Œ± increases power but also increases Type I error risk)\n",
    "\n",
    "## 4. Testing Hypotheses Using Confidence Intervals\n",
    "\n",
    "### Procedure\n",
    "1. Calculate a confidence interval for the parameter\n",
    "2. Check if the null value (from H‚ÇÄ) falls within the interval\n",
    "3. If the null value is outside the interval, reject H‚ÇÄ\n",
    "4. If the null value is inside the interval, fail to reject H‚ÇÄ\n",
    "\n",
    "### Example: Multiple Choice Question\n",
    "A multiple choice question has 4 options. We want to test if adults perform better than random guessing.\n",
    "- H‚ÇÄ: p = 0.25 (adults are as accurate as random guessing)\n",
    "- H<sub>A</sub>: p ‚â† 0.25 (adults perform differently than random guessing)\n",
    "\n",
    "**Scenario 1**: 21 out of 100 adults answer correctly\n",
    "- Sample proportion: pÃÇ = 0.21\n",
    "- Standard error: SE = ‚àö[0.21(1-0.21)/100] = 0.0407\n",
    "- 95% confidence interval: pÃÇ ¬± 1.96 √ó SE = 0.21 ¬± 1.96 √ó 0.0407 = (0.1302, 0.2898)\n",
    "- Since p‚ÇÄ = 0.25 falls within the interval, we fail to reject H‚ÇÄ\n",
    "\n",
    "**Scenario 2**: 37 out of 100 adults answer correctly\n",
    "- Sample proportion: pÃÇ = 0.37\n",
    "- Standard error: SE = ‚àö[0.37(1-0.37)/100] = 0.0483\n",
    "- 95% confidence interval: pÃÇ ¬± 1.96 √ó SE = 0.37 ¬± 1.96 √ó 0.0483 = (0.2754, 0.4646)\n",
    "- Since p‚ÇÄ = 0.25 falls outside the interval, we reject H‚ÇÄ\n",
    "\n",
    "## 5. Testing Hypotheses Using P-values\n",
    "\n",
    "### P-value\n",
    "- **Definition**: The probability of observing a test statistic as extreme as, or more extreme than, the one observed, assuming H‚ÇÄ is true\n",
    "- **Interpretation**: A measure of the strength of evidence against H‚ÇÄ\n",
    "- **Decision rule**: Reject H‚ÇÄ if p-value < Œ±\n",
    "\n",
    "### Calculating P-values for a Proportion Test\n",
    "1. Calculate the test statistic:\n",
    "   $$z = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}$$\n",
    "   where p‚ÇÄ is the null value, pÃÇ is the sample proportion, and n is the sample size\n",
    "\n",
    "2. Find the p-value:\n",
    "   - For H<sub>A</sub>: p ‚â† p‚ÇÄ (two-sided): P(Z ‚â§ -|z|) + P(Z ‚â• |z|) = 2 √ó P(Z ‚â• |z|)\n",
    "   - For H<sub>A</sub>: p > p‚ÇÄ (right-tailed): P(Z ‚â• z)\n",
    "   - For H<sub>A</sub>: p < p‚ÇÄ (left-tailed): P(Z ‚â§ z)\n",
    "\n",
    "### Example: Policy Support\n",
    "We want to test if a majority of adults support Policy A.\n",
    "- H‚ÇÄ: p = 0.5 (50% support)\n",
    "- H<sub>A</sub>: p ‚â† 0.5 (support differs from 50%)\n",
    "\n",
    "A random sample of 1000 adults shows 42% support the policy.\n",
    "- Test statistic: z = (0.42 - 0.5)/‚àö[0.5(1-0.5)/1000] = -0.08/0.016 = -5\n",
    "- P-value (two-sided): 2 √ó P(Z ‚â• 5) ‚âà 5.73 √ó 10‚Åª‚Å∑\n",
    "- Since p-value < 0.05, we reject H‚ÇÄ\n",
    "- Conclusion: There is strong evidence that the proportion of adults who support Policy A differs from 50% (specifically, it appears to be less than 50%)\n",
    "\n",
    "## 6. Steps for Conducting a Hypothesis Test for a Proportion\n",
    "\n",
    "### Step 1: Prepare\n",
    "- Identify the parameter of interest\n",
    "- State the null and alternative hypotheses\n",
    "- Choose the significance level Œ±\n",
    "- Identify the sample proportion pÃÇ and sample size n\n",
    "\n",
    "### Step 2: Check\n",
    "- Verify that the sample is random or representative\n",
    "- Check the success/failure condition using the null value p‚ÇÄ:\n",
    "  - np‚ÇÄ ‚â• 10\n",
    "  - n(1-p‚ÇÄ) ‚â• 10\n",
    "\n",
    "### Step 3: Calculate\n",
    "- Compute the standard error using the null value: SE = ‚àö[p‚ÇÄ(1-p‚ÇÄ)/n]\n",
    "- Calculate the test statistic: z = (pÃÇ - p‚ÇÄ)/SE\n",
    "- Find the p-value based on the form of H<sub>A</sub>\n",
    "\n",
    "### Step 4: Conclude\n",
    "- Compare the p-value to Œ±\n",
    "- Make a decision: reject H‚ÇÄ or fail to reject H‚ÇÄ\n",
    "- State the conclusion in the context of the problem\n",
    "\n",
    "## 7. Choosing the Significance Level (Œ±)\n",
    "\n",
    "### Common Choices\n",
    "- Œ± = 0.05 (5%): Standard in many fields\n",
    "- Œ± = 0.01 (1%): More stringent, used when Type I errors are costly\n",
    "- Œ± = 0.10 (10%): More lenient, used when Type II errors are costly\n",
    "\n",
    "### Considerations\n",
    "- **Type I Error Concerns**: Use smaller Œ± when falsely rejecting H‚ÇÄ is serious\n",
    "  - Example: Approving an ineffective drug (Œ± = 0.01 or lower)\n",
    "- **Type II Error Concerns**: Use larger Œ± when failing to detect an effect is serious\n",
    "  - Example: Missing a potential environmental hazard (Œ± = 0.10)\n",
    "\n",
    "## 8. Practical Examples\n",
    "\n",
    "### Example 1: College Dropout Rate\n",
    "A study aims to determine if the dropout rate for undergraduate college students has changed from the 2018 rate of 40%.\n",
    "\n",
    "- Parameter of interest: p = proportion of undergraduate students who drop out\n",
    "- H‚ÇÄ: p = 0.4\n",
    "- H<sub>A</sub>: p ‚â† 0.4\n",
    "- Significance level: Œ± = 0.05\n",
    "\n",
    "Suppose a random sample of 500 students shows 175 dropped out (pÃÇ = 0.35).\n",
    "- Check conditions: np‚ÇÄ = 500 √ó 0.4 = 200 ‚â• 10; n(1-p‚ÇÄ) = 500 √ó 0.6 = 300 ‚â• 10 ‚úì\n",
    "- SE = ‚àö[0.4(1-0.4)/500] = 0.022\n",
    "- Test statistic: z = (0.35 - 0.4)/0.022 = -2.27\n",
    "- P-value (two-sided): 2 √ó P(Z ‚â• 2.27) ‚âà 0.023\n",
    "- Decision: Since p-value < 0.05, reject H‚ÇÄ\n",
    "- Conclusion: There is sufficient evidence to conclude that the dropout rate has changed from 40% (appears to have decreased).\n",
    "\n",
    "### Example 2: New Drink Flavor\n",
    "A company will introduce a new drink to the market if more than 65% of people like the flavor.\n",
    "\n",
    "- Parameter of interest: p = proportion of people who like the flavor\n",
    "- H‚ÇÄ: p = 0.65\n",
    "- H<sub>A</sub>: p > 0.65\n",
    "- Significance level: Œ± = 0.05\n",
    "\n",
    "Suppose a random sample of 200 people shows 140 like the flavor (pÃÇ = 0.7).\n",
    "- Check conditions: np‚ÇÄ = 200 √ó 0.65 = 130 ‚â• 10; n(1-p‚ÇÄ) = 200 √ó 0.35 = 70 ‚â• 10 ‚úì\n",
    "- SE = ‚àö[0.65(1-0.65)/200] = 0.034\n",
    "- Test statistic: z = (0.7 - 0.65)/0.034 = 1.47\n",
    "- P-value (right-tailed): P(Z ‚â• 1.47) ‚âà 0.071\n",
    "- Decision: Since p-value > 0.05, fail to reject H‚ÇÄ\n",
    "- Conclusion: There is insufficient evidence to conclude that more than 65% of people like the flavor.\n",
    "\n",
    "### Example 3: Law Support in a Small Town\n",
    "A study investigates whether the proportion of residents in a small town who support a certain law is 68%.\n",
    "\n",
    "- Parameter of interest: p = proportion of residents who support the law\n",
    "- H‚ÇÄ: p = 0.68\n",
    "- H<sub>A</sub>: p ‚â† 0.68\n",
    "- Significance level: Œ± = 0.05\n",
    "\n",
    "A random sample of 200 residents shows 140 support the law (pÃÇ = 0.7).\n",
    "- Check conditions: np‚ÇÄ = 200 √ó 0.68 = 136 ‚â• 10; n(1-p‚ÇÄ) = 200 √ó 0.32 = 64 ‚â• 10 ‚úì\n",
    "- 95% confidence interval: pÃÇ ¬± 1.96 √ó ‚àö[pÃÇ(1-pÃÇ)/n] = 0.7 ¬± 1.96 √ó ‚àö[0.7(1-0.7)/200] = 0.7 ¬± 0.064 = (0.636, 0.764)\n",
    "- Since p‚ÇÄ = 0.68 falls within the interval, fail to reject H‚ÇÄ\n",
    "- Conclusion: There is insufficient evidence to conclude that the proportion of residents who support the law differs from 68%.\n",
    "\n",
    "## 9. Common Misconceptions and Pitfalls\n",
    "\n",
    "### Misconception 1: Failing to reject H‚ÇÄ means H‚ÇÄ is true\n",
    "- Correct interpretation: We don't have enough evidence to reject H‚ÇÄ, not that H‚ÇÄ is proven true\n",
    "\n",
    "### Misconception 2: P-value is the probability that H‚ÇÄ is true\n",
    "- Correct interpretation: P-value is the probability of observing data as extreme as ours if H‚ÇÄ were true\n",
    "\n",
    "### Misconception 3: Statistical significance implies practical significance\n",
    "- A result can be statistically significant but too small to matter in practice\n",
    "- Always consider the context and magnitude of the effect\n",
    "\n",
    "### Pitfall 1: Using pÃÇ instead of p‚ÇÄ to calculate SE in hypothesis testing\n",
    "- For confidence intervals: Use pÃÇ to calculate SE\n",
    "- For hypothesis tests: Use p‚ÇÄ to calculate SE\n",
    "\n",
    "### Pitfall 2: Incorrect formulation of hypotheses\n",
    "- H‚ÇÄ must contain an equality (=, ‚â§, or ‚â•)\n",
    "- H<sub>A</sub> must contain only inequalities (<, >, or ‚â†)\n",
    "- Parameters (p, Œº) should be used, not statistics (pÃÇ, xÃÑ)\n",
    "\n",
    "## 10. Key Takeaways\n",
    "\n",
    "1. Hypothesis testing provides a formal framework for making decisions based on data\n",
    "\n",
    "2. The null hypothesis (H‚ÇÄ) represents the status quo or skeptical perspective, while the alternative hypothesis (H<sub>A</sub>) represents the claim we're looking for evidence to support\n",
    "\n",
    "3. Two approaches to hypothesis testing:\n",
    "   - Using confidence intervals: Reject H‚ÇÄ if the null value falls outside the interval\n",
    "   - Using p-values: Reject H‚ÇÄ if the p-value is less than the significance level Œ±\n",
    "\n",
    "4. Type I error occurs when we reject a true H‚ÇÄ; Type II error occurs when we fail to reject a false H‚ÇÄ\n",
    "\n",
    "5. The significance level Œ± represents the probability of making a Type I error\n",
    "\n",
    "6. When conducting a hypothesis test for a proportion:\n",
    "   - Use p‚ÇÄ (not pÃÇ) to calculate the standard error\n",
    "   - Check the success/failure condition using p‚ÇÄ\n",
    "   - State conclusions in the context of the problem\n",
    "\n",
    "7. The choice of significance level should balance the risks of Type I and Type II errors based on the specific context of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 7 Summary: Inference for Comparing Two Proportions\n",
    "\n",
    "## 1. Introduction to Comparing Two Proportions\n",
    "\n",
    "### Why Compare Two Proportions?\n",
    "- **Purpose**: To determine if there is a significant difference between two population proportions\n",
    "- **Applications**:\n",
    "  - Medical studies (treatment vs. control groups)\n",
    "  - Marketing (comparing effectiveness of two campaigns)\n",
    "  - Social research (comparing behaviors across different demographics)\n",
    "  - Quality control (comparing defect rates between manufacturing processes)\n",
    "\n",
    "### Key Parameters and Statistics\n",
    "- **Population Parameters**: p‚ÇÅ and p‚ÇÇ (true proportions in each population)\n",
    "- **Sample Statistics**: pÃÇ‚ÇÅ and pÃÇ‚ÇÇ (observed proportions in each sample)\n",
    "- **Parameter of Interest**: p‚ÇÅ - p‚ÇÇ (difference between population proportions)\n",
    "- **Point Estimate**: pÃÇ‚ÇÅ - pÃÇ‚ÇÇ (difference between sample proportions)\n",
    "\n",
    "## 2. Confidence Intervals for the Difference of Two Proportions\n",
    "\n",
    "### Conditions for Valid Inference\n",
    "1. **Independence**:\n",
    "   - Data are independent within each group\n",
    "   - Data are independent between groups\n",
    "   - Satisfied by random sampling or random assignment\n",
    "\n",
    "2. **Success/Failure Condition**:\n",
    "   - For each group i (i = 1, 2):\n",
    "     - n·µ¢pÃÇ·µ¢ ‚â• 10 (at least 10 successes)\n",
    "     - n·µ¢(1-pÃÇ·µ¢) ‚â• 10 (at least 10 failures)\n",
    "\n",
    "### Formula for the Standard Error\n",
    "- **Standard Error for Difference in Proportions**:\n",
    "  $$SE = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}$$\n",
    "\n",
    "- **Estimated Standard Error** (using sample proportions):\n",
    "  $$SE = \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}$$\n",
    "\n",
    "### Confidence Interval Formula\n",
    "- **General Formula**:\n",
    "  $$(\\hat{p}_1 - \\hat{p}_2) \\pm z^* \\times SE$$\n",
    "\n",
    "- **Expanded Form**:\n",
    "  $$(\\hat{p}_1 - \\hat{p}_2) \\pm z^* \\times \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}$$\n",
    "\n",
    "### Example: Blood Thinner Study\n",
    "A study examined the effect of blood thinners on survival after heart attacks:\n",
    "\n",
    "| Group | Survived | Died | Total |\n",
    "|-------|----------|------|-------|\n",
    "| Control | 11 | 39 | 50 |\n",
    "| Treatment | 14 | 26 | 40 |\n",
    "\n",
    "**Step 1**: Calculate sample proportions\n",
    "- pÃÇ‚Çú (treatment) = 14/40 = 0.35\n",
    "- pÃÇ‚Çñ (control) = 11/50 = 0.22\n",
    "- Difference: pÃÇ‚Çú - pÃÇ‚Çñ = 0.35 - 0.22 = 0.13\n",
    "\n",
    "**Step 2**: Check conditions\n",
    "- Independence: Satisfied (randomized experiment)\n",
    "- Success/Failure: All groups have at least 10 successes and failures\n",
    "\n",
    "**Step 3**: Calculate standard error\n",
    "$$SE = \\sqrt{\\frac{0.35(1-0.35)}{40} + \\frac{0.22(1-0.22)}{50}} = 0.095$$\n",
    "\n",
    "**Step 4**: Construct 90% confidence interval (z* = 1.65)\n",
    "$$0.13 \\pm 1.65 \\times 0.095 = 0.13 \\pm 0.157 = (-0.027, 0.287)$$\n",
    "\n",
    "**Interpretation**: We are 90% confident that blood thinners have an impact on survival rate ranging from -2.7% (slightly harmful) to +28.7% (beneficial). Since the interval contains 0, we cannot conclude at this confidence level whether blood thinners help or harm in this context.\n",
    "\n",
    "## 3. Hypothesis Testing for the Difference of Two Proportions\n",
    "\n",
    "### Hypothesis Formulation\n",
    "- **Null Hypothesis (H‚ÇÄ)**: p‚ÇÅ - p‚ÇÇ = 0 (no difference between proportions)\n",
    "- **Alternative Hypothesis (H<sub>A</sub>)**:\n",
    "  - Two-sided: p‚ÇÅ - p‚ÇÇ ‚â† 0 (proportions are different)\n",
    "  - One-sided: p‚ÇÅ - p‚ÇÇ > 0 or p‚ÇÅ - p‚ÇÇ < 0 (one proportion is larger)\n",
    "\n",
    "### Pooled Proportion\n",
    "- **When to Use**: When testing H‚ÇÄ: p‚ÇÅ = p‚ÇÇ (null hypothesis assumes equal proportions)\n",
    "- **Formula**:\n",
    "  $$\\hat{p}_{pooled} = \\frac{n_1\\hat{p}_1 + n_2\\hat{p}_2}{n_1 + n_2} = \\frac{\\text{total successes}}{\\text{total sample size}}$$\n",
    "\n",
    "### Standard Error Under the Null Hypothesis\n",
    "- **Formula**:\n",
    "  $$SE = \\sqrt{\\hat{p}_{pooled}(1-\\hat{p}_{pooled})\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}$$\n",
    "\n",
    "### Test Statistic\n",
    "- **Z-statistic**:\n",
    "  $$Z = \\frac{(\\hat{p}_1 - \\hat{p}_2) - 0}{SE} = \\frac{\\hat{p}_1 - \\hat{p}_2}{SE}$$\n",
    "\n",
    "### P-value Calculation\n",
    "- **Two-sided test**: P(|Z| ‚â• |observed z|)\n",
    "- **Right-tailed test**: P(Z ‚â• observed z)\n",
    "- **Left-tailed test**: P(Z ‚â§ observed z)\n",
    "\n",
    "### Example: Survival Rate Study\n",
    "A large-scale study examined survival rates in treatment and control groups:\n",
    "\n",
    "| Group | Survived | Died | Total |\n",
    "|-------|----------|------|-------|\n",
    "| Control | 505 | 44,405 | 44,910 |\n",
    "| Treatment | 500 | 44,425 | 44,925 |\n",
    "\n",
    "**Step 1**: State hypotheses\n",
    "- H‚ÇÄ: p‚Çú - p‚Çñ = 0 (no difference in death rates)\n",
    "- H<sub>A</sub>: p‚Çú - p‚Çñ ‚â† 0 (death rates are different)\n",
    "\n",
    "**Step 2**: Calculate sample proportions\n",
    "- pÃÇ‚Çú = 500/44,925 = 0.01113\n",
    "- pÃÇ‚Çñ = 505/44,910 = 0.01125\n",
    "- Difference: pÃÇ‚Çú - pÃÇ‚Çñ = -0.00012\n",
    "\n",
    "**Step 3**: Calculate pooled proportion\n",
    "$$\\hat{p}_{pooled} = \\frac{500 + 505}{44,925 + 44,910} = \\frac{1,005}{89,835} = 0.0112$$\n",
    "\n",
    "**Step 4**: Check conditions\n",
    "- Independence: Satisfied (randomized experiment)\n",
    "- Success/Failure: All values (n‚Çú √ó pÃÇ‚Çö‚Çí‚Çí‚Çó‚Çë‚Çê, n‚Çú √ó (1-pÃÇ‚Çö‚Çí‚Çí‚Çó‚Çë‚Çê), etc.) are greater than 10\n",
    "\n",
    "**Step 5**: Calculate standard error\n",
    "$$SE = \\sqrt{0.0112 \\times 0.9888 \\times \\left(\\frac{1}{44,925} + \\frac{1}{44,910}\\right)} = 0.00070$$\n",
    "\n",
    "**Step 6**: Calculate test statistic\n",
    "$$Z = \\frac{-0.00012 - 0}{0.00070} = -0.17$$\n",
    "\n",
    "**Step 7**: Find p-value\n",
    "For a two-sided test with Z = -0.17, p-value = 0.865\n",
    "\n",
    "**Step 8**: Make decision\n",
    "Since p-value = 0.865 > 0.05, we fail to reject H‚ÇÄ.\n",
    "\n",
    "**Interpretation**: The difference in deaths between the control and treatment groups can be reasonably explained by chance. There is insufficient evidence to conclude that the treatment affects survival rates.\n",
    "\n",
    "## 4. Sample Size Determination for Confidence Intervals\n",
    "\n",
    "### Determining Sample Size for a Desired Margin of Error\n",
    "- **Formula for a Single Proportion**:\n",
    "  $$n = \\frac{z^{*2} \\times p(1-p)}{MOE^2}$$\n",
    "\n",
    "- **Conservative Approach** (when p is unknown):\n",
    "  - Use p = 0.5 (maximizes p(1-p))\n",
    "  - Results in the largest, most conservative sample size\n",
    "  - Formula simplifies to:\n",
    "    $$n = \\frac{z^{*2} \\times 0.25}{MOE^2}$$\n",
    "\n",
    "### Example: Sample Size Calculation\n",
    "To achieve a margin of error of 5% with 95% confidence:\n",
    "- z* = 1.96\n",
    "- Using p = 0.5 (conservative approach):\n",
    "  $$n = \\frac{1.96^2 \\times 0.5 \\times 0.5}{0.05^2} = \\frac{0.9604}{0.0025} = 384.16$$\n",
    "- We would need at least 385 participants.\n",
    "\n",
    "### Reducing the Margin of Error\n",
    "To cut the margin of error in half while maintaining the same confidence level:\n",
    "- Original MOE = z* √ó ‚àö[p(1-p)/n]\n",
    "- To halve the MOE: n<sub>new</sub> = 4 √ó n<sub>original</sub>\n",
    "\n",
    "**Example**: If a 95% CI based on 100 students has a certain margin of error, to cut that margin of error in half (while maintaining 95% confidence), we would need 4 √ó 100 = 400 students.\n",
    "\n",
    "## 5. Practical Examples\n",
    "\n",
    "### Example 1: Non-profit vs. For-profit Employee Happiness\n",
    "A study compared happiness levels between employees in non-profit and for-profit organizations:\n",
    "- Non-profit: 423 out of 467 employees reported being happy (pÃÇ‚ÇÅ = 0.906)\n",
    "- For-profit: 446 out of 531 employees reported being happy (pÃÇ‚ÇÇ = 0.840)\n",
    "\n",
    "**Confidence Interval Calculation**:\n",
    "- Difference in proportions: pÃÇ‚ÇÅ - pÃÇ‚ÇÇ = 0.906 - 0.840 = 0.066\n",
    "- Standard error:\n",
    "  $$SE = \\sqrt{\\frac{0.906 \\times 0.094}{467} + \\frac{0.840 \\times 0.160}{531}} = 0.0214$$\n",
    "- 99% confidence interval (z* = 2.576):\n",
    "  $$0.066 \\pm 2.576 \\times 0.0214 = 0.066 \\pm 0.055 = (0.0167, 0.1233)$$\n",
    "\n",
    "**Interpretation**: We are 99% confident that the proportion of employees who are happy working in non-profit organizations is between 1.67% and 12.33% higher than the proportion of employees who are happy working in for-profit organizations.\n",
    "\n",
    "### Example 2: Foreign-born Residents Comparison\n",
    "A study compared the proportion of foreign-born residents in the U.S. and China:\n",
    "- U.S.: 196 out of 980 residents were foreign-born (pÃÇ‚ÇÅ = 0.2)\n",
    "- China: 212 out of 1560 residents were foreign-born (pÃÇ‚ÇÇ = 0.136)\n",
    "\n",
    "**Hypothesis Test**:\n",
    "- H‚ÇÄ: p‚ÇÅ - p‚ÇÇ = 0 (same proportion of foreign-born residents)\n",
    "- H<sub>A</sub>: p‚ÇÅ - p‚ÇÇ ‚â† 0 (different proportions)\n",
    "\n",
    "- Pooled proportion:\n",
    "  $$\\hat{p}_{pooled} = \\frac{196 + 212}{980 + 1560} = \\frac{408}{2540} = 0.1606$$\n",
    "\n",
    "- Standard error:\n",
    "  $$SE = \\sqrt{0.1606 \\times 0.8394 \\times \\left(\\frac{1}{980} + \\frac{1}{1560}\\right)} = 0.0150$$\n",
    "\n",
    "- Test statistic:\n",
    "  $$Z = \\frac{0.2 - 0.136}{0.0150} = 4.267$$\n",
    "\n",
    "- P-value (two-sided): 2 √ó P(Z > 4.267) < 0.0001\n",
    "\n",
    "**Conclusion**: Since p-value < 0.05, we reject H‚ÇÄ. There is strong evidence that the proportion of foreign-born residents differs between the U.S. and China.\n",
    "\n",
    "## 6. Key Takeaways\n",
    "\n",
    "1. **Comparing Two Proportions**:\n",
    "   - Allows us to determine if there's a significant difference between two groups\n",
    "   - Uses the difference in sample proportions (pÃÇ‚ÇÅ - pÃÇ‚ÇÇ) as the point estimate\n",
    "\n",
    "2. **Confidence Intervals**:\n",
    "   - Provide a range of plausible values for the true difference in proportions\n",
    "   - Use individual sample proportions to calculate the standard error\n",
    "   - Interpretation should address both magnitude and direction of the difference\n",
    "\n",
    "3. **Hypothesis Testing**:\n",
    "   - Typically tests whether the difference in proportions equals zero\n",
    "   - Uses the pooled proportion to calculate standard error under the null hypothesis\n",
    "   - Follows the same general framework as single-proportion hypothesis tests\n",
    "\n",
    "4. **Sample Size Considerations**:\n",
    "   - Larger samples provide more precise estimates (narrower confidence intervals)\n",
    "   - To halve the margin of error, quadruple the sample size\n",
    "   - When planning studies, use conservative estimates (p = 0.5) if no prior information is available\n",
    "\n",
    "5. **Practical Significance**:\n",
    "   - Statistical significance doesn't always imply practical importance\n",
    "   - Consider the context and magnitude of the difference when interpreting results\n",
    "   - Confidence intervals provide more information about effect size than p-values alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 8 Summary: Goodness of Fit Tests\n",
    "\n",
    "## 1. Introduction to Goodness of Fit Tests\n",
    "\n",
    "### Purpose and Applications\n",
    "- **Definition**: Statistical procedures that determine whether observed data conform to a theoretical distribution or expected frequencies\n",
    "- **Applications**:\n",
    "  - Testing if a sample is representative of a population\n",
    "  - Determining if data follow a particular distribution (e.g., normal, binomial)\n",
    "  - Evaluating whether observed frequencies match expected frequencies\n",
    "  - Assessing whether categorical variables are distributed as expected\n",
    "\n",
    "### Types of Goodness of Fit Tests\n",
    "- **One-Way Chi-Square Test**: Tests if observed frequencies in a single categorical variable match expected frequencies\n",
    "- **Chi-Square Test for Independence**: Tests if two categorical variables are related (covered in later modules)\n",
    "- **Kolmogorov-Smirnov Test**: Tests if a sample comes from a specific continuous distribution (not covered in this module)\n",
    "\n",
    "## 2. The Chi-Square Distribution\n",
    "\n",
    "### Definition and Properties\n",
    "- **Definition**: The distribution of a sum of squares of independent standard normal random variables\n",
    "- **Formula**: If Z‚ÇÅ, Z‚ÇÇ, ..., Z‚Çñ are independent standard normal random variables, then:\n",
    "  $$\\chi^2_k = Z_1^2 + Z_2^2 + ... + Z_k^2$$\n",
    "  follows a chi-square distribution with k degrees of freedom\n",
    "\n",
    "- **Properties**:\n",
    "  - Always non-negative (sum of squared values)\n",
    "  - Right-skewed, especially with low degrees of freedom\n",
    "  - Becomes more symmetric and approximately normal as degrees of freedom increase\n",
    "  - Mean equals the degrees of freedom\n",
    "  - Variance equals twice the degrees of freedom\n",
    "\n",
    "### Degrees of Freedom\n",
    "- **Definition**: The number of values that are free to vary in the calculation of a statistic\n",
    "- **Interpretation**: In a chi-square test, degrees of freedom = (number of categories - 1)\n",
    "- **Reason**: When we know the total sample size and all but one category count, the final count is determined\n",
    "\n",
    "### Example: Reading Chi-Square Tables\n",
    "For a chi-square distribution with 3 degrees of freedom, the probability that the chi-square value exceeds 6.25 is:\n",
    "$$P(\\chi^2_3 \\geq 6.25) \\approx 0.1001$$\n",
    "\n",
    "This means that if we have a chi-square test statistic of 6.25 with 3 degrees of freedom, the p-value would be approximately 0.1001.\n",
    "\n",
    "## 3. Chi-Square Goodness of Fit Test\n",
    "\n",
    "### Test Statistic\n",
    "- **Formula**:\n",
    "  $$\\chi^2 = \\sum_{i=1}^{k} \\frac{(O_i - E_i)^2}{E_i}$$\n",
    "  where:\n",
    "  - O·µ¢ = Observed count in category i\n",
    "  - E·µ¢ = Expected count in category i under the null hypothesis\n",
    "  - k = Number of categories\n",
    "\n",
    "- **Interpretation**: Measures the discrepancy between observed and expected frequencies\n",
    "  - Larger values indicate greater discrepancy\n",
    "  - Values close to zero suggest good fit\n",
    "\n",
    "### Hypothesis Testing Framework\n",
    "- **Null Hypothesis (H‚ÇÄ)**: The observed frequencies match the expected frequencies (or the data follow the specified distribution)\n",
    "- **Alternative Hypothesis (H‚ÇÅ)**: The observed frequencies do not match the expected frequencies (or the data do not follow the specified distribution)\n",
    "- **Decision Rule**: Reject H‚ÇÄ if p-value < Œ± (significance level)\n",
    "\n",
    "### Conditions for Valid Chi-Square Test\n",
    "1. **Independence**:\n",
    "   - Observations must be independent of each other\n",
    "   - Satisfied by random sampling or random assignment\n",
    "\n",
    "2. **Sample Size**:\n",
    "   - Each expected count must be at least 5\n",
    "   - Ensures the chi-square approximation is valid\n",
    "\n",
    "## 4. Testing for Specific Distributions\n",
    "\n",
    "### Testing Categorical Distributions\n",
    "- **Example**: Testing if jurors represent the racial demographics of registered voters\n",
    "- **Procedure**:\n",
    "  1. Calculate expected counts based on population proportions\n",
    "  2. Compare observed counts to expected counts using chi-square statistic\n",
    "  3. Determine p-value using chi-square distribution with (k-1) degrees of freedom\n",
    "\n",
    "### Testing Probability Distributions\n",
    "- **Example**: Testing if dice are fair\n",
    "- **Procedure**:\n",
    "  1. Determine the theoretical probabilities under the null hypothesis\n",
    "  2. Calculate expected counts by multiplying probabilities by total sample size\n",
    "  3. Compare observed counts to expected counts using chi-square statistic\n",
    "  4. Determine p-value using chi-square distribution with appropriate degrees of freedom\n",
    "\n",
    "### Binning Continuous Data\n",
    "- For continuous distributions, data must be binned (grouped into categories)\n",
    "- Bins should be chosen to ensure expected counts ‚â• 5 in each bin\n",
    "- Test is sensitive to choice of bins, but reasonable choices should yield similar results\n",
    "\n",
    "## 5. Detailed Examples\n",
    "\n",
    "### Example 1: Jury Representation Test\n",
    "\n",
    "A random sample of 275 jurors from a small county had jurors identify their racial group. We want to test if the sample is representative of the population of registered voters.\n",
    "\n",
    "| Race | White | Black | Hispanic | Other | Total |\n",
    "|------|-------|-------|----------|-------|-------|\n",
    "| On Juries (observed) | 205 | 26 | 25 | 19 | 275 |\n",
    "| Registered Voters (proportion) | 0.72 | 0.07 | 0.12 | 0.09 | 1.00 |\n",
    "| Expected Counts | 198 | 19.25 | 33 | 24.75 | 275 |\n",
    "\n",
    "**Step 1**: State hypotheses\n",
    "- H‚ÇÄ: The jurors are a random sample (no racial bias)\n",
    "- H‚ÇÅ: The jurors are not a random sample (racial bias exists)\n",
    "\n",
    "**Step 2**: Calculate expected counts\n",
    "- Expected White jurors: 275 √ó 0.72 = 198\n",
    "- Expected Black jurors: 275 √ó 0.07 = 19.25\n",
    "- Expected Hispanic jurors: 275 √ó 0.12 = 33\n",
    "- Expected Other jurors: 275 √ó 0.09 = 24.75\n",
    "\n",
    "**Step 3**: Calculate Z-scores for each category\n",
    "- Z‚ÇÅ (White): (205 - 198)/‚àö198 = 0.5\n",
    "- Z‚ÇÇ (Black): (26 - 19.25)/‚àö19.25 = 1.54\n",
    "- Z‚ÇÉ (Hispanic): (25 - 33)/‚àö33 = -1.39\n",
    "- Z‚ÇÑ (Other): (19 - 24.75)/‚àö24.75 = -1.16\n",
    "\n",
    "**Step 4**: Calculate chi-square statistic\n",
    "$$\\chi^2 = (0.5)^2 + (1.54)^2 + (-1.39)^2 + (-1.16)^2 = 5.8993$$\n",
    "\n",
    "**Step 5**: Determine degrees of freedom and p-value\n",
    "- Degrees of freedom = 4 - 1 = 3\n",
    "- P-value = P(œá¬≤‚ÇÉ ‚â• 5.8993) ‚âà 0.1116\n",
    "\n",
    "**Step 6**: Make decision\n",
    "- Since p-value = 0.1116 > 0.05, we fail to reject H‚ÇÄ\n",
    "- Conclusion: There is insufficient evidence of racial bias in juror selection\n",
    "\n",
    "### Example 2: Testing if Dice are Fair\n",
    "\n",
    "A player rolls two dice 200 times and records the number of sixes that appear on each roll:\n",
    "\n",
    "| Number of Sixes | 0 | 1 | 2 | Total |\n",
    "|-----------------|---|---|---|-------|\n",
    "| Observed Count | 130 | 58 | 12 | 200 |\n",
    "\n",
    "**Step 1**: State hypotheses\n",
    "- H‚ÇÄ: The dice are fair\n",
    "- H‚ÇÅ: The dice are not fair\n",
    "\n",
    "**Step 2**: Calculate expected probabilities under H‚ÇÄ\n",
    "- P(0 sixes) = (5/6)¬≤ = 25/36 ‚âà 0.6944\n",
    "- P(1 six) = 2 √ó (1/6) √ó (5/6) = 10/36 ‚âà 0.2778\n",
    "- P(2 sixes) = (1/6)¬≤ = 1/36 ‚âà 0.0278\n",
    "\n",
    "**Step 3**: Calculate expected counts\n",
    "- Expected count for 0 sixes: 200 √ó 25/36 = 138.889\n",
    "- Expected count for 1 six: 200 √ó 10/36 = 55.556\n",
    "- Expected count for 2 sixes: 200 √ó 1/36 = 5.556\n",
    "\n",
    "**Step 4**: Calculate chi-square statistic\n",
    "$$\\chi^2 = \\frac{(130-138.889)^2}{138.889} + \\frac{(58-55.556)^2}{55.556} + \\frac{(12-5.556)^2}{5.556} \\approx 8.15$$\n",
    "\n",
    "**Step 5**: Determine degrees of freedom and p-value\n",
    "- Degrees of freedom = 3 - 1 = 2\n",
    "- P-value = P(œá¬≤‚ÇÇ ‚â• 8.15) ‚âà 0.017\n",
    "\n",
    "**Step 6**: Make decision\n",
    "- Since p-value = 0.017 < 0.05, we reject H‚ÇÄ\n",
    "- Conclusion: There is evidence that the dice are not fair\n",
    "- Note: Looking at the data, we can see more 2's (double sixes) than expected, suggesting the dice might be biased toward sixes\n",
    "\n",
    "### Example 3: Hospital Check-ins by Weekday\n",
    "\n",
    "A hospital administrator wants to find out if patient check-ins are evenly distributed across weekdays. They randomly sample 210 records:\n",
    "\n",
    "| Day | Monday | Tuesday | Wednesday | Thursday | Friday | Total |\n",
    "|-----|--------|---------|-----------|----------|--------|-------|\n",
    "| Observed Count | 32 | 40 | 36 | 45 | 57 | 210 |\n",
    "\n",
    "**Step 1**: State hypotheses\n",
    "- H‚ÇÄ: Check-ins are evenly distributed across weekdays\n",
    "- H‚ÇÅ: Check-ins are not evenly distributed across weekdays\n",
    "\n",
    "**Step 2**: Calculate expected counts under H‚ÇÄ\n",
    "- Expected count for each day: 210 √∑ 5 = 42\n",
    "\n",
    "**Step 3**: Calculate chi-square statistic\n",
    "$$\\chi^2 = \\frac{(32-42)^2}{42} + \\frac{(40-42)^2}{42} + \\frac{(36-42)^2}{42} + \\frac{(45-42)^2}{42} + \\frac{(57-42)^2}{42} \\approx 9.29$$\n",
    "\n",
    "**Step 4**: Determine degrees of freedom and p-value\n",
    "- Degrees of freedom = 5 - 1 = 4\n",
    "- P-value = P(œá¬≤‚ÇÑ ‚â• 9.29) ‚âà 0.054\n",
    "\n",
    "**Step 5**: Make decision\n",
    "- Since p-value = 0.054 > 0.05, we fail to reject H‚ÇÄ\n",
    "- Conclusion: There is insufficient evidence that patient check-ins are unevenly distributed across weekdays\n",
    "\n",
    "## 6. Common Misconceptions and Pitfalls\n",
    "\n",
    "### Misconception 1: Chi-Square Tests Prove the Null Hypothesis\n",
    "- Correct understanding: Failing to reject H‚ÇÄ does not prove that the observed data follow the expected distribution; it only means we lack evidence to conclude otherwise\n",
    "\n",
    "### Misconception 2: Chi-Square Tests Work with Small Samples\n",
    "- Correct understanding: Chi-square tests require expected counts of at least 5 in each category for valid results\n",
    "\n",
    "### Pitfall 1: Inappropriate Binning\n",
    "- Problem: Results can vary based on how continuous data is binned\n",
    "- Solution: Use consistent, reasonable binning strategies and ensure expected counts ‚â• 5 in each bin\n",
    "\n",
    "### Pitfall 2: Ignoring Independence Assumption\n",
    "- Problem: Non-independent observations can lead to invalid results\n",
    "- Solution: Ensure random sampling or appropriate experimental design\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "1. **Chi-Square Goodness of Fit Test**:\n",
    "   - Tests whether observed frequencies match expected frequencies\n",
    "   - Uses the chi-square statistic: œá¬≤ = Œ£[(O·µ¢ - E·µ¢)¬≤/E·µ¢]\n",
    "   - Larger values indicate greater discrepancy between observed and expected\n",
    "\n",
    "2. **Chi-Square Distribution**:\n",
    "   - Right-skewed distribution that approaches normal as degrees of freedom increase\n",
    "   - Used to determine p-values for chi-square test statistics\n",
    "   - Degrees of freedom = number of categories - 1\n",
    "\n",
    "3. **Conditions for Valid Chi-Square Test**:\n",
    "   - Independence of observations\n",
    "   - Expected count ‚â• 5 in each category\n",
    "\n",
    "4. **Applications**:\n",
    "   - Testing if a sample represents a population\n",
    "   - Testing if data follow a specific distribution\n",
    "   - Testing if categorical data are distributed as expected\n",
    "\n",
    "5. **Interpretation**:\n",
    "   - Small p-values (< Œ±) suggest the observed data do not match the expected distribution\n",
    "   - Large p-values (‚â• Œ±) suggest insufficient evidence to conclude the data don't match the expected distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 9 Summary: Inference for Numerical Data\n",
    "\n",
    "## 1. Introduction to Inference for Numerical Data\n",
    "\n",
    "### From Categorical to Numerical Data\n",
    "- **Previous Modules**: Focused on inference for categorical data\n",
    "  - Single proportion\n",
    "  - Difference of two proportions\n",
    "  - Multiple groups (goodness of fit)\n",
    "- **This Module**: Focuses on inference for numerical data\n",
    "  - Single mean\n",
    "  - Paired data\n",
    "  - Difference of two means\n",
    "  - Many means\n",
    "\n",
    "### Key Differences in Approach\n",
    "\n",
    "| Categorical Data | Numerical Data |\n",
    "|------------------|----------------|\n",
    "| Sample proportion: pÃÇ | Sample mean: xÃÑ |\n",
    "| Population proportion: p | Population mean: Œº |\n",
    "| Normal distribution with SE = ‚àö[p(1-p)/n] | t-distribution with SE = s/‚àön |\n",
    "| z-statistic | t-statistic |\n",
    "\n",
    "## 2. The t-Distribution\n",
    "\n",
    "### Why Use the t-Distribution?\n",
    "- When working with numerical data, we typically don't know the population standard deviation (œÉ)\n",
    "- We must estimate œÉ using the sample standard deviation (s)\n",
    "- This additional uncertainty is accounted for by using the t-distribution instead of the normal distribution\n",
    "\n",
    "### Properties of the t-Distribution\n",
    "- Always centered at 0 (like the standard normal)\n",
    "- Parametrized by a single parameter: degrees of freedom (df)\n",
    "- More spread out than the normal distribution (heavier tails)\n",
    "- As df increases, the t-distribution approaches the standard normal distribution\n",
    "- For df > 30, the t-distribution is very similar to the normal distribution\n",
    "\n",
    "### Degrees of Freedom\n",
    "- For a single sample: df = n - 1\n",
    "- For two independent samples: df = min(n‚ÇÅ - 1, n‚ÇÇ - 1) (conservative approach)\n",
    "- Represents the number of independent pieces of information available\n",
    "\n",
    "## 3. One-Sample t-Confidence Intervals\n",
    "\n",
    "### Conditions for Valid Inference\n",
    "1. **Independence**: The sample observations must be independent\n",
    "   - Satisfied by random sampling from a large population\n",
    "2. **Normality**: \n",
    "   - If n < 30: Data should come from a normally distributed population (or have no outliers)\n",
    "   - If n ‚â• 30: The Central Limit Theorem applies (no extreme outliers)\n",
    "\n",
    "### Formula for t-Confidence Interval\n",
    "$$\\bar{x} \\pm t^*_{df} \\times \\frac{s}{\\sqrt{n}}$$\n",
    "\n",
    "Where:\n",
    "- xÃÑ is the sample mean\n",
    "- s is the sample standard deviation\n",
    "- n is the sample size\n",
    "- t*·µà·∂† is the critical value from the t-distribution with df degrees of freedom\n",
    "- df = n - 1\n",
    "\n",
    "### Steps for Constructing a t-Confidence Interval\n",
    "1. **Prepare**: Identify or calculate xÃÑ, s, n, and determine the confidence level\n",
    "2. **Check**: Verify the conditions for using the t-distribution\n",
    "3. **Calculate**: Compute SE = s/‚àön and find the critical value t*·µà·∂†\n",
    "4. **Conclude**: Construct and interpret the confidence interval\n",
    "\n",
    "### Example: Height of 18-Year-Olds\n",
    "A random sample of 25 eighteen-year-olds has a mean height of 67.73 inches with a standard deviation of 2.00 inches.\n",
    "\n",
    "**Step 1**: We have xÃÑ = 67.73, s = 2.00, n = 25, and we want a 95% confidence interval.\n",
    "\n",
    "**Step 2**: The sample is random, and there are no clear outliers, so conditions are satisfied.\n",
    "\n",
    "**Step 3**: \n",
    "- SE = s/‚àön = 2.00/‚àö25 = 0.4\n",
    "- df = n - 1 = 25 - 1 = 24\n",
    "- t*‚ÇÇ‚ÇÑ = 2.10 (for 95% confidence)\n",
    "\n",
    "**Step 4**: \n",
    "- 95% CI = 67.73 ¬± 2.10 √ó 0.4 = 67.73 ¬± 0.84 = (66.89, 68.57)\n",
    "- Interpretation: We are 95% confident that the average height of 18-year-olds in the population is between 66.89 and 68.57 inches.\n",
    "\n",
    "## 4. One-Sample t-Tests\n",
    "\n",
    "### Hypothesis Testing Framework\n",
    "- **Null Hypothesis (H‚ÇÄ)**: Œº = Œº‚ÇÄ (population mean equals a specific value)\n",
    "- **Alternative Hypothesis (H‚ÇÅ)**:\n",
    "  - Two-sided: Œº ‚â† Œº‚ÇÄ\n",
    "  - Right-tailed: Œº > Œº‚ÇÄ\n",
    "  - Left-tailed: Œº < Œº‚ÇÄ\n",
    "\n",
    "### Test Statistic\n",
    "$$T = \\frac{\\bar{x} - \\mu_0}{\\frac{s}{\\sqrt{n}}}$$\n",
    "\n",
    "### Steps for Conducting a One-Sample t-Test\n",
    "1. **Prepare**: Identify or calculate xÃÑ, s, n, and determine the significance level Œ±\n",
    "2. **Check**: Verify the conditions for using the t-distribution\n",
    "3. **Calculate**: Compute the test statistic T and find the p-value\n",
    "4. **Conclude**: Compare the p-value to Œ± and make a decision\n",
    "\n",
    "### Example: Sleep Duration of UCSD Students\n",
    "We want to determine if UCSD students sleep less than 7 hours per night on average. A random sample of 50 students has a mean sleep duration of 6.74 hours with a standard deviation of 0.71 hours.\n",
    "\n",
    "**Step 1**: We have xÃÑ = 6.74, s = 0.71, n = 50, and we'll use Œ± = 0.05.\n",
    "\n",
    "**Step 2**: The sample is random and n ‚â• 30, so conditions are satisfied.\n",
    "\n",
    "**Step 3**: \n",
    "- SE = s/‚àön = 0.71/‚àö50 = 0.1004\n",
    "- df = n - 1 = 50 - 1 = 49\n",
    "- T = (6.74 - 7)/0.1004 = -2.59\n",
    "- p-value = P(T < -2.59) = 0.0063\n",
    "\n",
    "**Step 4**: \n",
    "- Since p-value = 0.0063 < 0.05, we reject H‚ÇÄ\n",
    "- Conclusion: There is strong evidence that UCSD students sleep less than 7 hours per night on average.\n",
    "\n",
    "## 5. Paired Data Analysis\n",
    "\n",
    "### What is Paired Data?\n",
    "- Two sets of observations are paired if each observation in one set has a special correspondence with exactly one observation in the other set\n",
    "- Examples:\n",
    "  - Before and after measurements on the same subjects\n",
    "  - Measurements on matched pairs (e.g., twins)\n",
    "  - Prices of the same items at two different stores\n",
    "\n",
    "### Analyzing Paired Data\n",
    "- Calculate the differences between paired observations\n",
    "- Analyze these differences using one-sample t-methods\n",
    "- The parameter of interest is Œºd (the mean difference)\n",
    "\n",
    "### Example: Grocery Store Prices\n",
    "Comparing prices of the same items at two different grocery stores:\n",
    "\n",
    "| Item | Whole Foods | Vons | Difference (WF - V) |\n",
    "|------|-------------|------|---------------------|\n",
    "| Fuji Apples | $1.89 | $1.49 | $0.40 |\n",
    "| Whole Milk | $2.49 | $3.99 | -$1.50 |\n",
    "| Yogurt | $5.89 | $5.99 | -$0.10 |\n",
    "\n",
    "We would analyze the differences using a one-sample t-test or confidence interval.\n",
    "\n",
    "## 6. Confidence Intervals for Difference of Means\n",
    "\n",
    "### Conditions for Valid Inference\n",
    "1. **Independence (extended)**: \n",
    "   - Data are independent within each group\n",
    "   - Data are independent between groups\n",
    "   - Satisfied by random sampling or random assignment\n",
    "2. **Normality**: \n",
    "   - If n < 30: Data should come from normally distributed populations\n",
    "   - If n ‚â• 30: The Central Limit Theorem applies\n",
    "\n",
    "### Formula for Standard Error\n",
    "$$SE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$$\n",
    "\n",
    "### Formula for Confidence Interval\n",
    "$$({\\bar{x}_1 - \\bar{x}_2}) \\pm t^*_{df} \\times SE$$\n",
    "\n",
    "Where:\n",
    "- xÃÑ‚ÇÅ and xÃÑ‚ÇÇ are the sample means\n",
    "- s‚ÇÅ and s‚ÇÇ are the sample standard deviations\n",
    "- n‚ÇÅ and n‚ÇÇ are the sample sizes\n",
    "- df = min(n‚ÇÅ - 1, n‚ÇÇ - 1) (conservative approach)\n",
    "\n",
    "### Example: Treatment Effect Study\n",
    "A small randomized control trial gives the following results for treating a particular condition:\n",
    "\n",
    "| Group | n | Sample Mean | Sample SD |\n",
    "|-------|---|-------------|-----------|\n",
    "| Treatment | 9 | 3.5 | 5.17 |\n",
    "| Control | 9 | -4.33 | 2.76 |\n",
    "\n",
    "**Step 1**: We want a 95% confidence interval for the treatment effect.\n",
    "\n",
    "**Step 2**: The data are from a randomized trial, so independence is satisfied.\n",
    "\n",
    "**Step 3**: \n",
    "- SE = ‚àö[(5.17¬≤/9) + (2.76¬≤/9)] = 1.95\n",
    "- df = min(9-1, 9-1) = 8\n",
    "- t*‚Çà = 2.31 (for 95% confidence)\n",
    "- Point estimate = xÃÑ‚ÇÅ - xÃÑ‚ÇÇ = 3.5 - (-4.33) = 7.83\n",
    "\n",
    "**Step 4**: \n",
    "- 95% CI = 7.83 ¬± 2.31 √ó 1.95 = 7.83 ¬± 4.51 = (3.32, 12.34)\n",
    "- Interpretation: We are 95% confident that the true difference in mean outcomes between the treatment and control groups is between 3.32 and 12.34 units.\n",
    "\n",
    "## 7. Hypothesis Testing for Difference of Means\n",
    "\n",
    "### Hypothesis Testing Framework\n",
    "- **Null Hypothesis (H‚ÇÄ)**: Œº‚ÇÅ - Œº‚ÇÇ = 0 (no difference between population means)\n",
    "- **Alternative Hypothesis (H‚ÇÅ)**:\n",
    "  - Two-sided: Œº‚ÇÅ - Œº‚ÇÇ ‚â† 0\n",
    "  - Right-tailed: Œº‚ÇÅ - Œº‚ÇÇ > 0\n",
    "  - Left-tailed: Œº‚ÇÅ - Œº‚ÇÇ < 0\n",
    "\n",
    "### Test Statistic\n",
    "$$T = \\frac{(\\bar{x}_1 - \\bar{x}_2) - 0}{SE}$$\n",
    "\n",
    "### Example: Birth Weight and Smoking\n",
    "A study investigates whether newborns from mothers who smoke have different average birth weights than newborns from mothers who don't smoke.\n",
    "\n",
    "| Group | n | Sample Mean | Sample SD |\n",
    "|-------|---|-------------|-----------|\n",
    "| Non-smoker | 100 | 7.18 | 1.6 |\n",
    "| Smoker | 50 | 6.78 | 1.43 |\n",
    "\n",
    "**Step 1**: We have Œ± = 0.05.\n",
    "\n",
    "**Step 2**: The data come from a random sample, and n ‚â• 30 for both groups, so conditions are satisfied.\n",
    "\n",
    "**Step 3**: \n",
    "- Point estimate = xÃÑ‚Çô - xÃÑ‚Çõ = 7.18 - 6.78 = 0.4\n",
    "- SE = ‚àö[(1.6¬≤/100) + (1.43¬≤/50)] = 0.26\n",
    "- T = 0.4/0.26 = 1.54\n",
    "- df = min(100-1, 50-1) = 49\n",
    "- p-value = P(|T| ‚â• 1.54) = 0.1304\n",
    "\n",
    "**Step 4**: \n",
    "- Since p-value = 0.1304 > 0.05, we fail to reject H‚ÇÄ\n",
    "- Conclusion: There is not enough evidence to conclude that there is a difference in average birth weight between newborns from mothers who smoke and those who don't.\n",
    "\n",
    "## 8. Statistical Power for Difference of Means\n",
    "\n",
    "### Definition of Statistical Power\n",
    "- The probability of correctly rejecting the null hypothesis when a specific alternative hypothesis is true\n",
    "- Mathematically: power = P(reject H‚ÇÄ | H‚ÇÅ is true)\n",
    "- Equivalently: power = 1 - P(Type II error)\n",
    "\n",
    "### Factors Affecting Power\n",
    "1. **Sample size**: Larger samples increase power\n",
    "2. **Effect size**: Larger differences are easier to detect\n",
    "3. **Variability**: Less variability increases power\n",
    "4. **Significance level**: Higher Œ± increases power (but also increases Type I error risk)\n",
    "\n",
    "### Calculating Power\n",
    "1. Determine the rejection region under H‚ÇÄ\n",
    "2. Calculate the probability of falling in the rejection region under H‚ÇÅ\n",
    "\n",
    "### Example: Blood Pressure Medication\n",
    "A study is designed to test if a new blood pressure medication reduces blood pressure compared to a standard medication. We want to detect a difference of 3 mmHg.\n",
    "\n",
    "- Sample size: n‚ÇÅ = n‚ÇÇ = 100\n",
    "- Estimated standard deviation: s‚ÇÅ = s‚ÇÇ = 12\n",
    "- Significance level: Œ± = 0.05\n",
    "\n",
    "**Step 1**: Calculate the standard error\n",
    "- SE = ‚àö[(12¬≤/100) + (12¬≤/100)] = 1.7\n",
    "\n",
    "**Step 2**: Determine the rejection region\n",
    "- For Œ± = 0.05 (two-sided), reject H‚ÇÄ if |T| > 1.96\n",
    "- This corresponds to xÃÑ‚ÇÅ - xÃÑ‚ÇÇ < -3.332 or xÃÑ‚ÇÅ - xÃÑ‚ÇÇ > 3.332\n",
    "\n",
    "**Step 3**: Calculate power for detecting a 3 mmHg reduction\n",
    "- Under H‚ÇÅ, xÃÑ‚ÇÅ - xÃÑ‚ÇÇ follows approximately N(-3, 1.7¬≤)\n",
    "- Power = P(xÃÑ‚ÇÅ - xÃÑ‚ÇÇ < -3.332 | Œº‚ÇÅ - Œº‚ÇÇ = -3)\n",
    "- Z = (-3.332 - (-3))/1.7 = -0.2\n",
    "- Power = P(Z < -0.2) = 0.42 or 42%\n",
    "\n",
    "**Step 4**: Determine sample size for 80% power\n",
    "- For 80% power, we need Z = 0.84\n",
    "- Distance between means = 2.8 √ó SE = 3\n",
    "- 2.8 √ó ‚àö[(12¬≤/n) + (12¬≤/n)] = 3\n",
    "- Solving for n: n = 251 per group\n",
    "\n",
    "## 9. Common Misconceptions and Pitfalls\n",
    "\n",
    "### Misconception 1: t-Distribution vs. Normal Distribution\n",
    "- **Misconception**: The t-distribution is always very different from the normal distribution\n",
    "- **Reality**: For large degrees of freedom (df > 30), the t-distribution is very similar to the normal distribution\n",
    "\n",
    "### Misconception 2: Paired vs. Independent Samples\n",
    "- **Misconception**: Any comparison of two groups should use the two-sample t-test\n",
    "- **Reality**: Paired data should be analyzed using paired methods (one-sample t-test on differences)\n",
    "\n",
    "### Pitfall 1: Ignoring Conditions\n",
    "- **Problem**: Using t-methods when conditions are not satisfied\n",
    "- **Solution**: Always check independence and normality conditions\n",
    "\n",
    "### Pitfall 2: Misinterpreting p-values\n",
    "- **Problem**: Interpreting a non-significant result as \"proving\" the null hypothesis\n",
    "- **Solution**: A non-significant result only means there is insufficient evidence to reject H‚ÇÄ\n",
    "\n",
    "## 10. Key Takeaways\n",
    "\n",
    "1. **t-Distribution**:\n",
    "   - Used when the population standard deviation is unknown\n",
    "   - Accounts for the additional uncertainty from estimating œÉ with s\n",
    "   - Approaches the normal distribution as sample size increases\n",
    "\n",
    "2. **One-Sample Inference**:\n",
    "   - Confidence interval: xÃÑ ¬± t*·µà·∂† √ó (s/‚àön)\n",
    "   - Hypothesis test: T = (xÃÑ - Œº‚ÇÄ)/(s/‚àön)\n",
    "   - Degrees of freedom: df = n - 1\n",
    "\n",
    "3. **Paired Data**:\n",
    "   - Analyze the differences between paired observations\n",
    "   - Use one-sample methods on these differences\n",
    "\n",
    "4. **Two-Sample Inference**:\n",
    "   - Confidence interval: (xÃÑ‚ÇÅ - xÃÑ‚ÇÇ) ¬± t*·µà·∂† √ó SE\n",
    "   - Hypothesis test: T = (xÃÑ‚ÇÅ - xÃÑ‚ÇÇ)/SE\n",
    "   - Standard error: SE = ‚àö[(s‚ÇÅ¬≤/n‚ÇÅ) + (s‚ÇÇ¬≤/n‚ÇÇ)]\n",
    "   - Degrees of freedom: df = min(n‚ÇÅ - 1, n‚ÇÇ - 1) (conservative approach)\n",
    "\n",
    "5. **Statistical Power**:\n",
    "   - The probability of correctly rejecting H‚ÇÄ when H‚ÇÅ is true\n",
    "   - Increases with larger sample sizes, larger effect sizes, and higher significance levels\n",
    "   - Important for study design and sample size determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 215: Probability and Statistics for Data Science\n",
    "## Module 10 Summary: Comparing Many Means with ANOVA\n",
    "\n",
    "## 1. Introduction to ANOVA\n",
    "\n",
    "### What is ANOVA?\n",
    "- **Definition**: Analysis of Variance (ANOVA) is a statistical method used to compare means across multiple groups\n",
    "- **Purpose**: Tests whether there are significant differences between the means of three or more independent groups\n",
    "- **Advantage over Multiple t-tests**: \n",
    "  - Reduces the risk of Type I errors that would accumulate when conducting multiple pairwise comparisons\n",
    "  - If you have k groups, you would need k(k-1)/2 pairwise comparisons, increasing the chance of finding differences by random chance\n",
    "\n",
    "### Hypothesis Framework\n",
    "- **Null Hypothesis (H‚ÇÄ)**: All population means are equal\n",
    "  $$H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_k$$\n",
    "- **Alternative Hypothesis (H‚ÇÅ)**: At least one population mean is different from the others\n",
    "  $$H_1: \\mu_i \\neq \\mu_j \\text{ for some } i \\neq j$$\n",
    "\n",
    "## 2. Conditions for Valid ANOVA\n",
    "\n",
    "### Three Key Conditions\n",
    "1. **Independence**: \n",
    "   - Observations are independent within each group\n",
    "   - Observations are independent across groups\n",
    "   - Typically satisfied by random sampling or random assignment\n",
    "\n",
    "2. **Normality**: \n",
    "   - The data within each group is approximately normally distributed\n",
    "   - Less critical for larger sample sizes due to the Central Limit Theorem\n",
    "   - Can be assessed using histograms, Q-Q plots, or formal tests\n",
    "\n",
    "3. **Variability** (Homogeneity of Variance): \n",
    "   - The variability across groups is approximately equal\n",
    "   - Can be assessed by comparing standard deviations or using formal tests like Levene's test\n",
    "   - ANOVA is somewhat robust to violations of this assumption when sample sizes are equal\n",
    "\n",
    "## 3. The F-Statistic and F-Distribution\n",
    "\n",
    "### The Basic Idea\n",
    "- ANOVA compares two sources of variation:\n",
    "  - **Between-group variation**: Variation of group means around the overall mean\n",
    "  - **Within-group variation**: Variation of individual observations around their group means\n",
    "- If the between-group variation is large relative to the within-group variation, we have evidence that the group means differ\n",
    "\n",
    "### The F-Statistic\n",
    "- **Formula**:\n",
    "  $$F = \\frac{\\text{Mean Square Between Groups (MSG)}}{\\text{Mean Square Error (MSE)}}$$\n",
    "\n",
    "- **Mean Square Between Groups (MSG)**:\n",
    "  $$\\text{MSG} = \\frac{1}{k-1} \\sum_{i=1}^{k} n_i(\\bar{x}_i - \\bar{x})^2$$\n",
    "  where:\n",
    "  - k = number of groups\n",
    "  - n_i = sample size of group i\n",
    "  - $\\bar{x}_i$ = sample mean of group i\n",
    "  - $\\bar{x}$ = overall sample mean\n",
    "\n",
    "- **Mean Square Error (MSE)**:\n",
    "  $$\\text{MSE} = \\frac{1}{n-k} \\left( \\sum_{i=1}^{n} (x_i - \\bar{x})^2 - \\sum_{i=1}^{k} n_i(\\bar{x}_i - \\bar{x})^2 \\right)$$\n",
    "  where n = total sample size\n",
    "\n",
    "### The F-Distribution\n",
    "- The F-distribution is parametrized by two degrees of freedom:\n",
    "  - **df‚ÇÅ** = k - 1 (degrees of freedom for MSG)\n",
    "  - **df‚ÇÇ** = n - k (degrees of freedom for MSE)\n",
    "- Properties:\n",
    "  - Always non-negative (ratio of variances)\n",
    "  - Right-skewed, especially with low degrees of freedom\n",
    "  - Becomes less skewed as degrees of freedom increase\n",
    "  - Under H‚ÇÄ, the F-statistic follows an F-distribution with df‚ÇÅ and df‚ÇÇ degrees of freedom\n",
    "\n",
    "## 4. Conducting an ANOVA Test\n",
    "\n",
    "### Step-by-Step Procedure\n",
    "1. **Prepare**: Identify the groups and collect data\n",
    "2. **Check**: Verify the conditions for ANOVA\n",
    "3. **Calculate**: Compute the F-statistic\n",
    "4. **Conclude**: Determine the p-value and make a decision\n",
    "\n",
    "### Example: Course Delivery Methods\n",
    "A professor taught the same class in three different formats (remote, hybrid, and in-person) and wants to know if the mean final scores differ. Here are the scores:\n",
    "\n",
    "| Remote | Hybrid | In-person |\n",
    "|--------|--------|-----------|\n",
    "| 80     | 94     | 78        |\n",
    "| 84     | 85     | 83        |\n",
    "| 90     | 87     | 93        |\n",
    "| 84     | 90     | 81        |\n",
    "| 89     | 76     | -         |\n",
    "| -      | 89     | -         |\n",
    "| **Mean** | **85.4** | **89.0** | **83.3** |\n",
    "\n",
    "**Step 1**: Check conditions\n",
    "- Independence: Assume students were randomly assigned to different formats\n",
    "- Normality: Sample sizes are small, but assume scores within each group are approximately normal\n",
    "- Variability: The standard deviations appear comparable across groups\n",
    "\n",
    "**Step 2**: Calculate the overall mean\n",
    "- $\\bar{x} = \\frac{5(85.4) + 6(89.0) + 4(83.3)}{5+6+4} = \\frac{1278.8}{15} = 85.25$\n",
    "\n",
    "**Step 3**: Calculate MSG\n",
    "- MSG = $\\frac{1}{3-1} [5(85.4-85.25)^2 + 6(89.0-85.25)^2 + 4(83.3-85.25)^2]$\n",
    "- MSG = $\\frac{1}{2} [5(0.15)^2 + 6(3.75)^2 + 4(-1.95)^2]$\n",
    "- MSG = $\\frac{1}{2} [0.1125 + 84.375 + 15.21]$\n",
    "- MSG = $\\frac{99.6975}{2} = 49.85$\n",
    "\n",
    "**Step 4**: Calculate MSE\n",
    "- First, calculate the sum of squares within each group:\n",
    "  - Remote: $(80-85.4)^2 + (84-85.4)^2 + (90-85.4)^2 + (84-85.4)^2 + (89-85.4)^2 = 102.8$\n",
    "  - Hybrid: $(94-89.0)^2 + (85-89.0)^2 + (87-89.0)^2 + (90-89.0)^2 + (76-89.0)^2 + (89-89.0)^2 = 234.0$\n",
    "  - In-person: $(78-83.3)^2 + (83-83.3)^2 + (93-83.3)^2 + (81-83.3)^2 = 146.75$\n",
    "- Total within-group sum of squares = 102.8 + 234.0 + 146.75 = 483.55\n",
    "- MSE = $\\frac{483.55}{15-3} = \\frac{483.55}{12} = 40.30$\n",
    "\n",
    "**Step 5**: Calculate the F-statistic\n",
    "- F = $\\frac{MSG}{MSE} = \\frac{49.85}{40.30} = 1.24$\n",
    "\n",
    "**Step 6**: Determine the p-value\n",
    "- df‚ÇÅ = 3 - 1 = 2\n",
    "- df‚ÇÇ = 15 - 3 = 12\n",
    "- Using an F-distribution table or software: p-value = P(F ‚â• 1.24) ‚âà 0.32\n",
    "\n",
    "**Step 7**: Make a decision\n",
    "- Since p-value = 0.32 > 0.05, we fail to reject H‚ÇÄ\n",
    "- Conclusion: There is insufficient evidence to conclude that the mean final scores differ across the three teaching formats\n",
    "\n",
    "## 5. Multiple Comparisons\n",
    "\n",
    "### The Multiple Comparisons Problem\n",
    "- If ANOVA rejects H‚ÇÄ, we know that at least one mean differs, but we don't know which ones\n",
    "- To identify specific differences, we need to conduct pairwise comparisons\n",
    "- However, conducting multiple tests increases the risk of Type I errors (false positives)\n",
    "\n",
    "### Bonferroni Correction\n",
    "- **Purpose**: Controls the overall Type I error rate when conducting multiple comparisons\n",
    "- **Adjusted Significance Level**:\n",
    "  $$\\alpha^* = \\frac{\\alpha}{K}$$\n",
    "  where K = k(k-1)/2 is the number of pairwise comparisons\n",
    "\n",
    "- **Justification**: By the union bound in probability theory:\n",
    "  $$\\mathbb{P}\\left( \\bigcup_{i=1}^{K} (p_i \\leq \\alpha/K) \\right) \\leq \\sum_{i=1}^{K} \\mathbb{P}(p_i \\leq \\alpha/K) \\leq \\alpha$$\n",
    "\n",
    "### Example: Pairwise Comparisons with Bonferroni Correction\n",
    "Suppose ANOVA indicates significant differences among four groups (A, B, C, D) with Œ± = 0.05.\n",
    "\n",
    "**Step 1**: Calculate the number of pairwise comparisons\n",
    "- K = 4(4-1)/2 = 6 comparisons (A-B, A-C, A-D, B-C, B-D, C-D)\n",
    "\n",
    "**Step 2**: Calculate the adjusted significance level\n",
    "- Œ±* = 0.05/6 = 0.0083\n",
    "\n",
    "**Step 3**: Conduct pairwise t-tests\n",
    "- For each pair, calculate the t-statistic and p-value\n",
    "- Compare each p-value to Œ±* = 0.0083\n",
    "- Only pairs with p-values < 0.0083 are considered significantly different\n",
    "\n",
    "### Important Note\n",
    "- It is possible to reject the null hypothesis in ANOVA but not find significant differences in any pairwise comparisons\n",
    "- This does not invalidate the ANOVA result\n",
    "- It simply means we cannot identify which specific groups differ with the given sample sizes\n",
    "\n",
    "## 6. Practical Example: Travel Routes\n",
    "\n",
    "A student is interested in the times it takes (in minutes) to get to campus using three different routes. She wants to test whether the mean times are equal.\n",
    "\n",
    "| Route 1 | Route 2 | Route 3 |\n",
    "|---------|---------|---------|\n",
    "| 30      | 27      | 16      |\n",
    "| 32      | 29      | 41      |\n",
    "| 27      | 28      | 22      |\n",
    "| 35      | 36      | 31      |\n",
    "| **Mean** | **31.0** | **30.0** | **27.5** |\n",
    "\n",
    "**Step 1**: State hypotheses\n",
    "- H‚ÇÄ: Œº‚ÇÅ = Œº‚ÇÇ = Œº‚ÇÉ (mean travel times are equal across routes)\n",
    "- H‚ÇÅ: At least one mean is different\n",
    "\n",
    "**Step 2**: Check conditions (assume all conditions are met)\n",
    "\n",
    "**Step 3**: Calculate the overall mean\n",
    "- $\\bar{x} = \\frac{4(31.0) + 4(30.0) + 4(27.5)}{12} = 29.5$\n",
    "\n",
    "**Step 4**: Calculate MSG\n",
    "- MSG = $\\frac{1}{3-1} [4(31.0-29.5)^2 + 4(30.0-29.5)^2 + 4(27.5-29.5)^2]$\n",
    "- MSG = $\\frac{1}{2} [4(1.5)^2 + 4(0.5)^2 + 4(-2.0)^2]$\n",
    "- MSG = $\\frac{1}{2} [9 + 1 + 16]$\n",
    "- MSG = $\\frac{26}{2} = 13$\n",
    "\n",
    "**Step 5**: Calculate MSE\n",
    "- Total sum of squares = $(30-29.5)^2 + (32-29.5)^2 + ... + (31-29.5)^2 = 467$\n",
    "- Between-group sum of squares = 26\n",
    "- Within-group sum of squares = 467 - 26 = 441\n",
    "- MSE = $\\frac{441}{12-3} = \\frac{441}{9} = 49$\n",
    "\n",
    "**Step 6**: Calculate the F-statistic\n",
    "- F = $\\frac{MSG}{MSE} = \\frac{13}{49} = 0.265$\n",
    "\n",
    "**Step 7**: Determine the p-value\n",
    "- df‚ÇÅ = 3 - 1 = 2\n",
    "- df‚ÇÇ = 12 - 3 = 9\n",
    "- Using software: p-value = P(F ‚â• 0.265) ‚âà 0.773\n",
    "\n",
    "**Step 8**: Make a decision\n",
    "- Since p-value = 0.773 > 0.05, we fail to reject H‚ÇÄ\n",
    "- Conclusion: There is insufficient evidence to conclude that the mean travel times differ across the three routes\n",
    "\n",
    "## 7. Common Misconceptions and Pitfalls\n",
    "\n",
    "### Misconception 1: ANOVA Tests for Any Difference\n",
    "- **Misconception**: ANOVA tests for any type of difference between groups\n",
    "- **Reality**: ANOVA specifically tests for differences in means, not medians, variances, or other parameters\n",
    "\n",
    "### Misconception 2: Significant ANOVA Means All Groups Differ\n",
    "- **Misconception**: A significant ANOVA result means all group means are different\n",
    "- **Reality**: A significant result only indicates that at least one mean differs from the others\n",
    "\n",
    "### Pitfall 1: Ignoring Conditions\n",
    "- **Problem**: Using ANOVA when conditions are not satisfied\n",
    "- **Solution**: Always check independence, normality, and equal variance assumptions\n",
    "\n",
    "### Pitfall 2: Conducting Multiple t-tests Without Correction\n",
    "- **Problem**: Performing multiple pairwise comparisons without adjusting the significance level\n",
    "- **Solution**: Use the Bonferroni correction or other multiple comparison procedures\n",
    "\n",
    "## 8. Key Takeaways\n",
    "\n",
    "1. **ANOVA Purpose**:\n",
    "   - Compares means across multiple groups with a single hypothesis test\n",
    "   - Controls the overall Type I error rate better than multiple pairwise comparisons\n",
    "\n",
    "2. **F-Statistic**:\n",
    "   - Ratio of between-group variance to within-group variance\n",
    "   - Large values suggest significant differences between group means\n",
    "   - Under H‚ÇÄ, follows an F-distribution with df‚ÇÅ = k-1 and df‚ÇÇ = n-k degrees of freedom\n",
    "\n",
    "3. **Conditions**:\n",
    "   - Independence: Observations are independent within and across groups\n",
    "   - Normality: Data within each group is approximately normally distributed\n",
    "   - Equal Variance: Variability across groups is comparable\n",
    "\n",
    "4. **Multiple Comparisons**:\n",
    "   - After a significant ANOVA, use pairwise comparisons to identify specific differences\n",
    "   - Apply the Bonferroni correction to control the overall Type I error rate\n",
    "   - Adjusted significance level: Œ±* = Œ±/K, where K = k(k-1)/2\n",
    "\n",
    "5. **Interpretation**:\n",
    "   - Failing to reject H‚ÇÄ: Insufficient evidence of differences between means\n",
    "   - Rejecting H‚ÇÄ: At least one group mean differs from the others\n",
    "   - Follow-up with multiple comparisons to identify specific differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 215: Probability and Statistics for Data Science\n",
    "## Comprehensive Final Exam Practice Questions\n",
    "\n",
    "This document contains 50 practice questions representative of what might appear on the DSC 215 final exam. Questions cover all modules, with emphasis on Modules 5-10 as specified for the final exam. Each question includes the relevant module(s), necessary equations, and a complete solution.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1\n",
    "**Module(s): 1**\n",
    "\n",
    "A researcher wants to study the effect of background music on student performance on math tests. She randomly assigns 100 students to one of two groups: one group takes the test with classical music playing, and the other takes the test in silence. Identify:\n",
    "\n",
    "a) The explanatory and response variables\n",
    "b) Whether this is an observational study or an experiment\n",
    "c) The population of interest\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "a) Explanatory variable: Presence of background music (categorical: classical music or silence)\n",
    "   Response variable: Math test performance (numerical)\n",
    "\n",
    "b) This is an experiment because the researcher is actively manipulating the explanatory variable (presence of music) by randomly assigning students to conditions.\n",
    "\n",
    "c) The population of interest is likely all students (or potentially all students at a particular school/level depending on the context).\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2\n",
    "**Module(s): 2**\n",
    "\n",
    "The following data represents the number of hours 10 students spent studying for an exam:\n",
    "3, 5, 2, 8, 4, 6, 3, 7, 5, 12\n",
    "\n",
    "a) Calculate the mean, median, and mode.\n",
    "b) Calculate the range, variance, and standard deviation.\n",
    "c) Are there any outliers? Use the 1.5 √ó IQR rule.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "a) Mean = (3 + 5 + 2 + 8 + 4 + 6 + 3 + 7 + 5 + 12) / 10 = 55 / 10 = 5.5 hours\n",
    "   \n",
    "   To find the median, arrange the data in ascending order:\n",
    "   2, 3, 3, 4, 5, 5, 6, 7, 8, 12\n",
    "   \n",
    "   Since n = 10 (even), median = (5 + 5) / 2 = 5 hours\n",
    "   \n",
    "   Mode = 3 and 5 (both appear twice)\n",
    "\n",
    "b) Range = Maximum - Minimum = 12 - 2 = 10 hours\n",
    "\n",
    "   Variance = Œ£(x - Œº)¬≤ / n\n",
    "   = [(3-5.5)¬≤ + (5-5.5)¬≤ + (2-5.5)¬≤ + (8-5.5)¬≤ + (4-5.5)¬≤ + (6-5.5)¬≤ + (3-5.5)¬≤ + (7-5.5)¬≤ + (5-5.5)¬≤ + (12-5.5)¬≤] / 10\n",
    "   = [6.25 + 0.25 + 12.25 + 6.25 + 2.25 + 0.25 + 6.25 + 2.25 + 0.25 + 42.25] / 10\n",
    "   = 78.5 / 10 = 7.85\n",
    "\n",
    "   Standard deviation = ‚àöVariance = ‚àö7.85 ‚âà 2.80 hours\n",
    "\n",
    "c) To find outliers, we need Q1, Q3, and IQR:\n",
    "   Q1 = median of lower half = (3 + 3) / 2 = 3\n",
    "   Q3 = median of upper half = (7 + 8) / 2 = 7.5\n",
    "   IQR = Q3 - Q1 = 7.5 - 3 = 4.5\n",
    "   \n",
    "   Lower fence = Q1 - 1.5 √ó IQR = 3 - 1.5 √ó 4.5 = 3 - 6.75 = -3.75\n",
    "   Upper fence = Q3 + 1.5 √ó IQR = 7.5 + 1.5 √ó 4.5 = 7.5 + 6.75 = 14.25\n",
    "   \n",
    "   Since all values are between -3.75 and 14.25, there are no outliers according to the 1.5 √ó IQR rule.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3\n",
    "**Module(s): 3**\n",
    "\n",
    "A fair six-sided die is rolled twice. Let X be the sum of the two rolls.\n",
    "\n",
    "a) Find the probability mass function (PMF) of X.\n",
    "b) Calculate P(X = 7).\n",
    "c) Calculate P(X ‚â§ 5).\n",
    "d) Calculate the expected value and variance of X.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "a) The PMF of X (the sum of two dice) is:\n",
    "\n",
    "   P(X = 2) = 1/36 (only when both dice show 1)\n",
    "   P(X = 3) = 2/36 = 1/18 (when the dice show 1,2 or 2,1)\n",
    "   P(X = 4) = 3/36 = 1/12 (when the dice show 1,3 or 3,1 or 2,2)\n",
    "   P(X = 5) = 4/36 = 1/9 (when the dice show 1,4 or 4,1 or 2,3 or 3,2)\n",
    "   P(X = 6) = 5/36 (when the dice show 1,5 or 5,1 or 2,4 or 4,2 or 3,3)\n",
    "   P(X = 7) = 6/36 = 1/6 (when the dice show 1,6 or 6,1 or 2,5 or 5,2 or 3,4 or 4,3)\n",
    "   P(X = 8) = 5/36 (when the dice show 2,6 or 6,2 or 3,5 or 5,3 or 4,4)\n",
    "   P(X = 9) = 4/36 = 1/9 (when the dice show 3,6 or 6,3 or 4,5 or 5,4)\n",
    "   P(X = 10) = 3/36 = 1/12 (when the dice show 4,6 or 6,4 or 5,5)\n",
    "   P(X = 11) = 2/36 = 1/18 (when the dice show 5,6 or 6,5)\n",
    "   P(X = 12) = 1/36 (only when both dice show 6)\n",
    "\n",
    "b) P(X = 7) = 6/36 = 1/6 ‚âà 0.167\n",
    "\n",
    "c) P(X ‚â§ 5) = P(X = 2) + P(X = 3) + P(X = 4) + P(X = 5)\n",
    "   = 1/36 + 2/36 + 3/36 + 4/36 = 10/36 = 5/18 ‚âà 0.278\n",
    "\n",
    "d) Expected value:\n",
    "   E[X] = Œ£ x¬∑P(X = x)\n",
    "   = 2¬∑(1/36) + 3¬∑(2/36) + 4¬∑(3/36) + 5¬∑(4/36) + 6¬∑(5/36) + 7¬∑(6/36) + 8¬∑(5/36) + 9¬∑(4/36) + 10¬∑(3/36) + 11¬∑(2/36) + 12¬∑(1/36)\n",
    "   = (2 + 6 + 12 + 20 + 30 + 42 + 40 + 36 + 30 + 22 + 12)/36\n",
    "   = 252/36 = 7\n",
    "\n",
    "   Variance:\n",
    "   Var(X) = E[X¬≤] - (E[X])¬≤\n",
    "   \n",
    "   E[X¬≤] = Œ£ x¬≤¬∑P(X = x)\n",
    "   = 2¬≤¬∑(1/36) + 3¬≤¬∑(2/36) + 4¬≤¬∑(3/36) + 5¬≤¬∑(4/36) + 6¬≤¬∑(5/36) + 7¬≤¬∑(6/36) + 8¬≤¬∑(5/36) + 9¬≤¬∑(4/36) + 10¬≤¬∑(3/36) + 11¬≤¬∑(2/36) + 12¬≤¬∑(1/36)\n",
    "   = (4 + 18 + 48 + 100 + 180 + 294 + 320 + 324 + 300 + 242 + 144)/36\n",
    "   = 1974/36 = 54.83\n",
    "   \n",
    "   Var(X) = 54.83 - 7¬≤ = 54.83 - 49 = 5.83\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4\n",
    "**Module(s): 4**\n",
    "\n",
    "The weights of adult male gorillas are normally distributed with a mean of 180 kg and a standard deviation of 21 kg.\n",
    "\n",
    "a) What is the probability that a randomly selected male gorilla weighs more than 200 kg?\n",
    "b) What is the probability that a randomly selected male gorilla weighs between 160 kg and 190 kg?\n",
    "c) What weight separates the heaviest 10% of male gorillas from the rest?\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "a) We need to find P(X > 200) where X ~ N(180, 21¬≤)\n",
    "   \n",
    "   First, standardize to a Z-score:\n",
    "   Z = (X - Œº) / œÉ = (200 - 180) / 21 = 20 / 21 ‚âà 0.95\n",
    "   \n",
    "   We need P(Z > 0.95) = 1 - P(Z < 0.95) = 1 - 0.8289 = 0.1711\n",
    "   \n",
    "   The probability that a randomly selected male gorilla weighs more than 200 kg is approximately 0.1711 or 17.11%.\n",
    "\n",
    "b) We need to find P(160 < X < 190)\n",
    "   \n",
    "   Standardize both bounds:\n",
    "   Z‚ÇÅ = (160 - 180) / 21 = -20 / 21 ‚âà -0.95\n",
    "   Z‚ÇÇ = (190 - 180) / 21 = 10 / 21 ‚âà 0.48\n",
    "   \n",
    "   P(160 < X < 190) = P(-0.95 < Z < 0.48) = P(Z < 0.48) - P(Z < -0.95)\n",
    "   = 0.6844 - 0.1711 = 0.5133\n",
    "   \n",
    "   The probability that a randomly selected male gorilla weighs between 160 kg and 190 kg is approximately 0.5133 or 51.33%.\n",
    "\n",
    "c) We need to find the value x such that P(X > x) = 0.10, or equivalently, P(X < x) = 0.90\n",
    "   \n",
    "   For P(Z < z) = 0.90, we have z = 1.28 (from standard normal table)\n",
    "   \n",
    "   Therefore:\n",
    "   (x - 180) / 21 = 1.28\n",
    "   x - 180 = 1.28 √ó 21 = 26.88\n",
    "   x = 180 + 26.88 = 206.88\n",
    "   \n",
    "   The weight that separates the heaviest 10% of male gorillas from the rest is approximately 206.88 kg.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5\n",
    "**Module(s): 5**\n",
    "\n",
    "A polling organization wants to estimate the proportion of voters who support a certain candidate in an upcoming election. They want the margin of error to be no more than 3% with 95% confidence.\n",
    "\n",
    "a) What is the minimum sample size needed?\n",
    "b) If a previous poll suggested that about 40% of voters support the candidate, what sample size would be needed?\n",
    "c) If they end up surveying 1200 voters and find that 45% support the candidate, construct a 95% confidence interval for the true proportion.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "a) When we don't have a prior estimate of the proportion, we use p = 0.5 to get the most conservative (largest) sample size.\n",
    "\n",
    "   The formula for sample size is:\n",
    "   n = (z*)¬≤ √ó p(1-p) / (MOE)¬≤\n",
    "   \n",
    "   For 95% confidence, z* = 1.96\n",
    "   MOE = 0.03\n",
    "   \n",
    "   n = (1.96)¬≤ √ó 0.5 √ó 0.5 / (0.03)¬≤\n",
    "   = 3.8416 √ó 0.25 / 0.0009\n",
    "   = 0.9604 / 0.0009\n",
    "   = 1067.11\n",
    "   \n",
    "   Therefore, the minimum sample size needed is 1068 voters.\n",
    "\n",
    "b) With a prior estimate of p = 0.4:\n",
    "   \n",
    "   n = (1.96)¬≤ √ó 0.4 √ó 0.6 / (0.03)¬≤\n",
    "   = 3.8416 √ó 0.24 / 0.0009\n",
    "   = 0.922 / 0.0009\n",
    "   = 1024.44\n",
    "   \n",
    "   Therefore, the sample size needed is 1025 voters.\n",
    "\n",
    "c) With a sample of 1200 voters where 45% support the candidate:\n",
    "   \n",
    "   pÃÇ = 0.45\n",
    "   \n",
    "   The formula for the confidence interval is:\n",
    "   pÃÇ ¬± z* √ó ‚àö[pÃÇ(1-pÃÇ)/n]\n",
    "   \n",
    "   95% CI = 0.45 ¬± 1.96 √ó ‚àö[0.45 √ó 0.55 / 1200]\n",
    "   = 0.45 ¬± 1.96 √ó ‚àö[0.2475 / 1200]\n",
    "   = 0.45 ¬± 1.96 √ó ‚àö0.000206\n",
    "   = 0.45 ¬± 1.96 √ó 0.0144\n",
    "   = 0.45 ¬± 0.028\n",
    "   = (0.422, 0.478)\n",
    "   \n",
    "   We are 95% confident that the true proportion of voters who support the candidate is between 42.2% and 47.8%.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 6\n",
    "**Module(s): 6**\n",
    "\n",
    "A company claims that more than 80% of its products meet the highest quality standards. A random sample of 200 products shows that 172 meet these standards.\n",
    "\n",
    "a) Set up the appropriate hypotheses to test the company's claim.\n",
    "b) Calculate the test statistic and p-value.\n",
    "c) At Œ± = 0.05, what is your conclusion?\n",
    "d) Calculate a 95% confidence interval for the true proportion. Does this interval support your conclusion in part c?\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "a) The company claims that more than 80% of products meet the highest standards.\n",
    "   \n",
    "   H‚ÇÄ: p ‚â§ 0.80 (null hypothesis)\n",
    "   H‚ÇÅ: p > 0.80 (alternative hypothesis)\n",
    "\n",
    "b) From the sample, pÃÇ = 172/200 = 0.86\n",
    "   \n",
    "   Test statistic:\n",
    "   z = (pÃÇ - p‚ÇÄ) / ‚àö[p‚ÇÄ(1-p‚ÇÄ)/n]\n",
    "   = (0.86 - 0.80) / ‚àö[0.80 √ó 0.20 / 200]\n",
    "   = 0.06 / ‚àö0.0008\n",
    "   = 0.06 / 0.0283\n",
    "   = 2.12\n",
    "   \n",
    "   For a right-tailed test, p-value = P(Z > 2.12) = 1 - P(Z < 2.12) = 1 - 0.983 = 0.017\n",
    "\n",
    "c) Since p-value = 0.017 < Œ± = 0.05, we reject the null hypothesis.\n",
    "   \n",
    "   Conclusion: There is sufficient evidence to support the company's claim that more than 80% of its products meet the highest quality standards.\n",
    "\n",
    "d) 95% confidence interval:\n",
    "   pÃÇ ¬± z* √ó ‚àö[pÃÇ(1-pÃÇ)/n]\n",
    "   = 0.86 ¬± 1.96 √ó ‚àö[0.86 √ó 0.14 / 200]\n",
    "   = 0.86 ¬± 1.96 √ó ‚àö0.0006\n",
    "   = 0.86 ¬± 1.96 √ó 0.0245\n",
    "   = 0.86 ¬± 0.048\n",
    "   = (0.812, 0.908)\n",
    "   \n",
    "   Since the entire confidence interval is above 0.80, this supports our conclusion to reject H‚ÇÄ in part c.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 7\n",
    "**Module(s): 7**\n",
    "\n",
    "A researcher wants to compare the effectiveness of two different teaching methods. Method A is used with 40 students, and their average test score is 78 with a standard deviation of 8. Method B is used with 35 students, and their average test score is 82 with a standard deviation of 10.\n",
    "\n",
    "a) Construct a 95% confidence interval for the difference in mean test scores (Method B - Method A).\n",
    "b) Based on your confidence interval, is there a significant difference between the two teaching methods at the 5% level?\n",
    "c) Conduct a hypothesis test to determine if Method B results in higher test scores than Method A.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "a) The formula for the confidence interval for the difference in means is:\n",
    "   (xÃÑ‚ÇÇ - xÃÑ‚ÇÅ) ¬± t* √ó ‚àö[(s‚ÇÅ¬≤/n‚ÇÅ) + (s‚ÇÇ¬≤/n‚ÇÇ)]\n",
    "   \n",
    "   Where:\n",
    "   xÃÑ‚ÇÅ = 78 (Method A)\n",
    "   xÃÑ‚ÇÇ = 82 (Method B)\n",
    "   s‚ÇÅ = 8\n",
    "   s‚ÇÇ = 10\n",
    "   n‚ÇÅ = 40\n",
    "   n‚ÇÇ = 35\n",
    "   \n",
    "   For a conservative approach, we use df = min(n‚ÇÅ-1, n‚ÇÇ-1) = min(39, 34) = 34\n",
    "   For 95% confidence with df = 34, t* ‚âà 2.03\n",
    "   \n",
    "   Standard error = ‚àö[(8¬≤/40) + (10¬≤/35)] = ‚àö[1.6 + 2.86] = ‚àö4.46 = 2.11\n",
    "   \n",
    "   95% CI = (82 - 78) ¬± 2.03 √ó 2.11\n",
    "   = 4 ¬± 4.28\n",
    "   = (-0.28, 8.28)\n",
    "\n",
    "b) Since the confidence interval includes zero, we cannot conclude that there is a significant difference between the two teaching methods at the 5% level.\n",
    "\n",
    "c) Hypotheses:\n",
    "   H‚ÇÄ: Œº‚ÇÇ - Œº‚ÇÅ ‚â§ 0 (Method B does not result in higher scores than Method A)\n",
    "   H‚ÇÅ: Œº‚ÇÇ - Œº‚ÇÅ > 0 (Method B results in higher scores than Method A)\n",
    "   \n",
    "   Test statistic:\n",
    "   t = (xÃÑ‚ÇÇ - xÃÑ‚ÇÅ) / ‚àö[(s‚ÇÅ¬≤/n‚ÇÅ) + (s‚ÇÇ¬≤/n‚ÇÇ)]\n",
    "   = (82 - 78) / 2.11\n",
    "   = 4 / 2.11\n",
    "   = 1.90\n",
    "   \n",
    "   For a right-tailed test with df = 34, the p-value = P(t > 1.90) ‚âà 0.033\n",
    "   \n",
    "   Since p-value = 0.033 < Œ± = 0.05, we reject the null hypothesis.\n",
    "   \n",
    "   Conclusion: There is sufficient evidence to conclude that Method B results in higher test scores than Method A.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 8\n",
    "**Module(s): 8**\n",
    "\n",
    "A six-sided die is rolled 120 times with the following results:\n",
    "\n",
    "| Outcome | 1 | 2 | 3 | 4 | 5 | 6 |\n",
    "|---------|---|---|---|---|---|---|\n",
    "| Frequency | 15 | 17 | 22 | 25 | 18 | 23 |\n",
    "\n",
    "Test whether the die is fair using a chi-square goodness of fit test with Œ± = 0.05.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "Step 1: State the hypotheses\n",
    "H‚ÇÄ: The die is fair (all outcomes have equal probability)\n",
    "H‚ÇÅ: The die is not fair (at least one outcome has a different probability)\n",
    "\n",
    "Step 2: Calculate expected frequencies\n",
    "If the die is fair, each outcome has probability 1/6.\n",
    "Expected frequency for each outcome = 120 √ó (1/6) = 20\n",
    "\n",
    "Step 3: Calculate the chi-square statistic\n",
    "œá¬≤ = Œ£[(O - E)¬≤/E]\n",
    "= (15 - 20)¬≤/20 + (17 - 20)¬≤/20 + (22 - 20)¬≤/20 + (25 - 20)¬≤/20 + (18 - 20)¬≤/20 + (23 - 20)¬≤/20\n",
    "= 25/20 + 9/20 + 4/20 + 25/20 + 4/20 + 9/20\n",
    "= 76/20 = 3.8\n",
    "\n",
    "Step 4: Determine the critical value\n",
    "Degrees of freedom = k - 1 = 6 - 1 = 5\n",
    "For Œ± = 0.05 and df = 5, the critical value is 11.07\n",
    "\n",
    "Step 5: Make a decision\n",
    "Since œá¬≤ = 3.8 < 11.07, we fail to reject H‚ÇÄ.\n",
    "\n",
    "Conclusion: There is insufficient evidence to conclude that the die is not fair.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 9\n",
    "**Module(s): 9**\n",
    "\n",
    "A researcher wants to test if a new medication reduces blood pressure. They measure the blood pressure (in mmHg) of 12 patients before and after taking the medication, with the following results:\n",
    "\n",
    "| Patient | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 |\n",
    "|---------|---|---|---|---|---|---|---|---|---|----|----|----|\n",
    "| Before | 145 | 152 | 138 | 160 | 155 | 142 | 148 | 135 | 162 | 150 | 143 | 157 |\n",
    "| After | 140 | 145 | 135 | 153 | 150 | 137 | 141 | 132 | 158 | 145 | 140 | 152 |\n",
    "| Difference | 5 | 7 | 3 | 7 | 5 | 5 | 7 | 3 | 4 | 5 | 3 | 5 |\n",
    "\n",
    "a) Is this a paired or independent samples test? Explain.\n",
    "b) Conduct a hypothesis test to determine if the medication significantly reduces blood pressure at Œ± = 0.01.\n",
    "c) Calculate a 99% confidence interval for the mean reduction in blood pressure.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "a) This is a paired samples test because we have before and after measurements on the same 12 patients. The data are not independent; each \"after\" measurement is related to its corresponding \"before\" measurement.\n",
    "\n",
    "b) Step 1: State the hypotheses\n",
    "   H‚ÇÄ: Œºd ‚â§ 0 (medication does not reduce blood pressure)\n",
    "   H‚ÇÅ: Œºd > 0 (medication reduces blood pressure)\n",
    "   \n",
    "   Where Œºd is the mean difference (before - after)\n",
    "   \n",
    "   Step 2: Calculate the sample statistics\n",
    "   Mean difference: dÃÑ = (5 + 7 + 3 + 7 + 5 + 5 + 7 + 3 + 4 + 5 + 3 + 5) / 12 = 59 / 12 = 4.92\n",
    "   \n",
    "   Standard deviation of differences:\n",
    "   sd = ‚àö[Œ£(d - dÃÑ)¬≤ / (n-1)]\n",
    "   = ‚àö[((5-4.92)¬≤ + (7-4.92)¬≤ + ... + (5-4.92)¬≤) / 11]\n",
    "   = ‚àö[(0.08¬≤ + 2.08¬≤ + (-1.92)¬≤ + 2.08¬≤ + 0.08¬≤ + 0.08¬≤ + 2.08¬≤ + (-1.92)¬≤ + (-0.92)¬≤ + 0.08¬≤ + (-1.92)¬≤ + 0.08¬≤) / 11]\n",
    "   = ‚àö[26.92 / 11] = ‚àö2.45 = 1.57\n",
    "   \n",
    "   Step 3: Calculate the test statistic\n",
    "   t = dÃÑ / (sd / ‚àön) = 4.92 / (1.57 / ‚àö12) = 4.92 / 0.45 = 10.93\n",
    "   \n",
    "   Step 4: Determine the p-value\n",
    "   For a right-tailed test with df = 12 - 1 = 11, the p-value = P(t > 10.93) < 0.0001\n",
    "   \n",
    "   Step 5: Make a decision\n",
    "   Since p-value < 0.0001 < Œ± = 0.01, we reject H‚ÇÄ.\n",
    "   \n",
    "   Conclusion: There is strong evidence that the medication significantly reduces blood pressure.\n",
    "\n",
    "c) 99% confidence interval:\n",
    "   dÃÑ ¬± t* √ó (sd / ‚àön)\n",
    "   \n",
    "   For 99% confidence with df = 11, t* ‚âà 3.11\n",
    "   \n",
    "   99% CI = 4.92 ¬± 3.11 √ó (1.57 / ‚àö12)\n",
    "   = 4.92 ¬± 3.11 √ó 0.45\n",
    "   = 4.92 ¬± 1.40\n",
    "   = (3.52, 6.32)\n",
    "   \n",
    "   We are 99% confident that the true mean reduction in blood pressure due to the medication is between 3.52 and 6.32 mmHg.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 10\n",
    "**Module(s): 10**\n",
    "\n",
    "A researcher wants to compare the effectiveness of three different fertilizers (A, B, and C) on plant growth. They randomly assign 18 plants to the three fertilizer groups (6 plants per group) and measure the height increase (in cm) after one month:\n",
    "\n",
    "Fertilizer A: 5.2, 4.8, 6.1, 5.5, 4.9, 5.3\n",
    "Fertilizer B: 6.5, 7.2, 6.8, 7.0, 6.3, 6.9\n",
    "Fertilizer C: 5.8, 6.2, 5.5, 6.0, 5.7, 5.9\n",
    "\n",
    "a) Conduct a one-way ANOVA to determine if there are significant differences in mean height increase among the three fertilizers at Œ± = 0.05.\n",
    "b) If the ANOVA indicates significant differences, which pairs of fertilizers differ significantly? Use the Bonferroni correction with an overall Œ± = 0.05.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "a) Step 1: Calculate the group means and overall mean\n",
    "   \n",
    "   Mean for Fertilizer A: xÃÑ‚ÇÅ = (5.2 + 4.8 + 6.1 + 5.5 + 4.9 + 5.3) / 6 = 31.8 / 6 = 5.3\n",
    "   Mean for Fertilizer B: xÃÑ‚ÇÇ = (6.5 + 7.2 + 6.8 + 7.0 + 6.3 + 6.9) / 6 = 40.7 / 6 = 6.78\n",
    "   Mean for Fertilizer C: xÃÑ‚ÇÉ = (5.8 + 6.2 + 5.5 + 6.0 + 5.7 + 5.9) / 6 = 35.1 / 6 = 5.85\n",
    "   \n",
    "   Overall mean: xÃÑ = (31.8 + 40.7 + 35.1) / 18 = 107.6 / 18 = 5.98\n",
    "   \n",
    "   Step 2: Calculate the sum of squares\n",
    "   \n",
    "   Between-group sum of squares (SSB):\n",
    "   SSB = Œ£ n·µ¢(xÃÑ·µ¢ - xÃÑ)¬≤\n",
    "   = 6(5.3 - 5.98)¬≤ + 6(6.78 - 5.98)¬≤ + 6(5.85 - 5.98)¬≤\n",
    "   = 6(0.68)¬≤ + 6(0.8)¬≤ + 6(0.13)¬≤\n",
    "   = 6(0.46 + 0.64 + 0.017)\n",
    "   = 6 √ó 1.117 = 6.7\n",
    "   \n",
    "   Within-group sum of squares (SSW):\n",
    "   SSW = Œ£ Œ£ (x·µ¢‚±º - xÃÑ·µ¢)¬≤\n",
    "   \n",
    "   For Fertilizer A:\n",
    "   (5.2 - 5.3)¬≤ + (4.8 - 5.3)¬≤ + (6.1 - 5.3)¬≤ + (5.5 - 5.3)¬≤ + (4.9 - 5.3)¬≤ + (5.3 - 5.3)¬≤\n",
    "   = (-0.1)¬≤ + (-0.5)¬≤ + (0.8)¬≤ + (0.2)¬≤ + (-0.4)¬≤ + (0)¬≤\n",
    "   = 0.01 + 0.25 + 0.64 + 0.04 + 0.16 + 0 = 1.1\n",
    "   \n",
    "   For Fertilizer B:\n",
    "   (6.5 - 6.78)¬≤ + (7.2 - 6.78)¬≤ + (6.8 - 6.78)¬≤ + (7.0 - 6.78)¬≤ + (6.3 - 6.78)¬≤ + (6.9 - 6.78)¬≤\n",
    "   = (-0.28)¬≤ + (0.42)¬≤ + (0.02)¬≤ + (0.22)¬≤ + (-0.48)¬≤ + (0.12)¬≤\n",
    "   = 0.078 + 0.176 + 0.0004 + 0.048 + 0.23 + 0.014 = 0.55\n",
    "   \n",
    "   For Fertilizer C:\n",
    "   (5.8 - 5.85)¬≤ + (6.2 - 5.85)¬≤ + (5.5 - 5.85)¬≤ + (6.0 - 5.85)¬≤ + (5.7 - 5.85)¬≤ + (5.9 - 5.85)¬≤\n",
    "   = (-0.05)¬≤ + (0.35)¬≤ + (-0.35)¬≤ + (0.15)¬≤ + (-0.15)¬≤ + (0.05)¬≤\n",
    "   = 0.0025 + 0.1225 + 0.1225 + 0.0225 + 0.0225 + 0.0025 = 0.295\n",
    "   \n",
    "   SSW = 1.1 + 0.55 + 0.295 = 1.945\n",
    "   \n",
    "   Step 3: Calculate the mean squares\n",
    "   \n",
    "   Mean Square Between (MSB) = SSB / (k - 1) = 6.7 / (3 - 1) = 6.7 / 2 = 3.35\n",
    "   Mean Square Within (MSW) = SSW / (n - k) = 1.945 / (18 - 3) = 1.945 / 15 = 0.13\n",
    "   \n",
    "   Step 4: Calculate the F-statistic\n",
    "   \n",
    "   F = MSB / MSW = 3.35 / 0.13 = 25.77\n",
    "   \n",
    "   Step 5: Determine the critical value\n",
    "   \n",
    "   For Œ± = 0.05, df‚ÇÅ = 2, df‚ÇÇ = 15, the critical value is approximately 3.68\n",
    "   \n",
    "   Step 6: Make a decision\n",
    "   \n",
    "   Since F = 25.77 > 3.68, we reject H‚ÇÄ.\n",
    "   \n",
    "   Conclusion: There are significant differences in mean height increase among the three fertilizers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "### Summarize Module Content\n",
    "----------------\n",
    "\n",
    "Goal: Generate a comprehensive Markdown file summarizing key concepts from Module 1 of a statistics course (DSC 215).\n",
    "\n",
    "Role: You are an expert statistics educator with a Ph.D. and 30 years of experience creating effective learning materials.\n",
    "\n",
    "Context: The provided text includes content from Module 1 lectures, pre-checks, reviews, examples, and homework.\n",
    "\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Create a Markdown summary of Module 1.\n",
    "2. Include relevant statistical equations.\n",
    "3. Provide examples demonstrating the application of those equations\n",
    "\n",
    "----------------\n",
    "### Summarize Midterm and id topics cover to what was asked on midterm\n",
    "----------------\n",
    "Goal: Generate a comprehensive Markdown file summarizing key concepts from Module 10 of a statistics course (DSC 215).\n",
    "\n",
    "Role: You are an expert statistics educator with a Ph.D. and 30 years of experience creating effective learning materials.\n",
    "\n",
    "Context: The provided text includes content from the midterm exam. Students were told that the midterm exam would cover Module 1, Module 2, Module 3, and Module 4.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Part A\n",
    "\n",
    "1. Create a Markdown summary of the midterm exam.\n",
    "\n",
    "2. Include relevant statistical equations.\n",
    "\n",
    "3. Provide step by step instructions on how to solve each question\n",
    "\n",
    "\n",
    "\n",
    "Part B\n",
    "\n",
    "1. Are there any trends between the material covered and the questions asked\n",
    "\n",
    "Context: material covered refers to the text used to generate module-1-summary.md,module-2-summary.md, module-3-summary.md and module-4-summary.md\n",
    "----------------------\n",
    "Goal: Generate a comprehensive Markdown file containing potential final exam questions for DSC 215.\n",
    "\n",
    "Role: You are an expert statistics educator with a Ph.D. and 30 years of experience in creating effective learning materials, including detailed exam analyses and practice problems.\n",
    "\n",
    "Context:\n",
    "* The provided text includes the midterm exam and module summaries.\n",
    "* The final exam for DSC 215 is comprehensive, covering Module 1, Module 2, Module 3, Module 4, Module 5, Module 6, Module 7, Module 8, Module 9, and Module 10.\n",
    "* Students were informed that the final exam will emphasize Module 5, Module 6, Module 7, Module 8, Module 9, and Module 10.\n",
    "* You have access to the following Markdown files:\n",
    "    * module-1-summary.md\n",
    "    * module-2-summary.md\n",
    "    * module-3-summary.md\n",
    "    * module-4-summary.md\n",
    "    * midterm-exam-analysis.md (This file contains an analysis of the midterm exam, including trends and patterns)\n",
    "    * module-5-summary.md\n",
    "    * module-6-summary.md\n",
    "    * module-7-summary.md\n",
    "    * module-8-summary.md\n",
    "    * module-9-summary.md\n",
    "    * module-10-summary.md\n",
    "* Students will have 3 hours to complete the final exam.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Part A: Generate Potential Final Exam Questions\n",
    "\n",
    "1.  Create a Markdown file containing 50 potential final exam questions for DSC 215.\n",
    "    * These questions should be representative of the material covered in all modules, with heavier emphasis on Modules 5-10.\n",
    "    * Use information from the module summaries and the trends/patterns identified in `midterm-exam-analysis.md` to guide the creation of these questions.\n",
    "    * Vary the question types (e.g., problem-solving, conceptual explanations, interpretations).\n",
    "    * Ensure the difficulty level is appropriate for a final exam in an introductory statistics course.\n",
    "    * Consider the 3-hour exam time limit when designing the questions (i.e., avoid overly complex problems that would take too long).\n",
    "2.  For each question, include:\n",
    "    * All relevant statistical equations needed to solve the problem (if applicable).\n",
    "    * A step-by-step solution guide, clearly demonstrating how to apply the equations or arrive at the answer.\n",
    "    * Indicate the module(s) the question primarily covers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
