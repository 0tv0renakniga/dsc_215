# DSC 215: Probability and Statistics for Data Science
## Module 3 Summary: Introduction to Probability

## 1. Foundations of Probability

### Why Study Probability?
- Probability is the foundation upon which statistics is built
- Essential for machine learning, artificial intelligence, game theory, and information theory
- Provides a formal framework for understanding uncertainty and randomness
- Enables deeper understanding of statistical tools and techniques

### Sample Space
- **Definition**: The set of all possible outcomes of an experiment, denoted by Œ©
- **Examples**:
  - Rolling a die: Œ© = {1, 2, 3, 4, 5, 6}
  - Flipping a coin: Œ© = {Heads, Tails}
  - Tossing a coin three times: Œ© = {HHH, HHT, HTH, HTT, THH, THT, TTH, TTT}

### Event Space
- **Definition**: A set whose elements are themselves sets (subsets of Œ©)
- Every event A in the event space ‚Ñ± is a subset of Œ© (A ‚äÇ Œ©)
- The event space must satisfy certain properties:
  - The empty set ‚àÖ must be in ‚Ñ±
  - If A is in ‚Ñ±, then its complement A<sup>c</sup> must also be in ‚Ñ±
  - If A‚ÇÅ, A‚ÇÇ, ... are all in ‚Ñ±, then their union must also be in ‚Ñ±

### Probability Measure
- **Definition**: A function ‚Ñô: ‚Ñ± ‚Üí [0,1] that assigns probabilities to events
- Must satisfy the axioms of probability:
  - ‚Ñô(A) ‚â• 0 for all A ‚àà ‚Ñ± (non-negativity)
  - ‚Ñô(Œ©) = 1 (normalization)
  - If A‚ÇÅ, A‚ÇÇ, ... are disjoint events (A<sub>i</sub> ‚à© A<sub>j</sub> = ‚àÖ for i ‚â† j), then ‚Ñô(A‚ÇÅ ‚à™ A‚ÇÇ ‚à™ ...) = Œ£·µ¢ ‚Ñô(A<sub>i</sub>)

### Properties of Probability Measures
- If A ‚äÇ B, then ‚Ñô(A) ‚â§ ‚Ñô(B)
- ‚Ñô(A ‚à© B) ‚â§ min(‚Ñô(A), ‚Ñô(B))
- ‚Ñô(A ‚à™ B) ‚â§ ‚Ñô(A) + ‚Ñô(B) (union bound)
- ‚Ñô(A<sup>c</sup>) = 1 - ‚Ñô(A)

### Example: Die Rolling
For a fair six-sided die:
- Sample space: Œ© = {1, 2, 3, 4, 5, 6}
- Event space: ‚Ñ± = P(Œ©) (the power set of Œ©)
- Probability measure: If a set A ‚àà ‚Ñ± has i elements, then ‚Ñô(A) = i/6
- Example: ‚Ñô({1, 4, 6}) = 3/6 = 0.5

## 2. Conditional Probability and Independence

### Conditional Probability
- **Definition**: The probability of event A occurring given that event B has occurred
- Formula: 
  $$\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}$$
  where ‚Ñô(B) ‚â† 0

### Independence
- **Definition**: Two events A and B are independent if and only if:
  $$\mathbb{P}(A \cap B) = \mathbb{P}(A) \times \mathbb{P}(B)$$
- Equivalently, A and B are independent if:
  $$\mathbb{P}(A|B) = \mathbb{P}(A)$$

### Example: Left-Handed People
If 9% of people are left-handed and 2 people are selected at random from a large population:
- Probability both are left-handed (assuming independence):
  $$\mathbb{P}(\text{both left-handed}) = \mathbb{P}(\text{first left-handed}) \times \mathbb{P}(\text{second left-handed}) = 0.09^2 = 0.0081$$

### Example: Smallpox in Boston, 1721
A dataset of 6,224 individuals exposed to smallpox in Boston:

| | Inoculated | Not Inoculated | Total |
|---|---|---|---|
| Lived | 238 | 5,136 | 5,374 |
| Died | 6 | 844 | 850 |
| Total | 244 | 5,980 | 6,224 |

- Probability a non-inoculated person died from smallpox:
  $$\mathbb{P}(\text{died}|\text{not inoculated}) = \frac{\mathbb{P}(\text{died} \cap \text{not inoculated})}{\mathbb{P}(\text{not inoculated})} = \frac{0.1356}{0.9608} \approx 0.1411$$

- Probability an inoculated person died from smallpox:
  $$\mathbb{P}(\text{died}|\text{inoculated}) = \frac{\mathbb{P}(\text{died} \cap \text{inoculated})}{\mathbb{P}(\text{inoculated})} = \frac{0.0010}{0.0392} \approx 0.0246$$

### Bayes' Theorem
- **Formula**:
  $$\mathbb{P}(A|B) = \frac{\mathbb{P}(B|A)\mathbb{P}(A)}{\mathbb{P}(B)}$$

- When ‚Ñô(B) is not directly accessible:
  $$\mathbb{P}(B) = \sum_i \mathbb{P}(B|A_i)\mathbb{P}(A_i)$$
  where A<sub>i</sub> ‚à© A<sub>j</sub> = ‚àÖ for i ‚â† j and ‚à™<sub>i</sub> A<sub>i</sub> = Œ©

### Example: Bayes' Theorem with Smallpox Data
Using the smallpox data to find the probability that a person who died was not inoculated:
$$\mathbb{P}(\text{not inoculated}|\text{died}) = \frac{\mathbb{P}(\text{died}|\text{not inoculated})\mathbb{P}(\text{not inoculated})}{\mathbb{P}(\text{died})} = \frac{0.1411 \times 0.9608}{0.1366} \approx 0.993$$

## 3. Random Variables

### Definition and Types
- **Random Variable**: A function from the sample space Œ© to another set (typically ‚Ñù or ‚Ñï)
- **Discrete Random Variable**: Takes values from a countable set
- **Continuous Random Variable**: Takes values from an uncountable set (typically an interval of real numbers)

### Example: Coin Tossing
For a sequence of 3 coin tosses:
- Sample space: Œ© = {(H,H,H), (H,H,T), (H,T,H), (H,T,T), (T,H,H), (T,H,T), (T,T,H), (T,T,T)}
- Random variable X = number of heads in the sequence
- For œâ = (H,H,T), X(œâ) = 2

## 4. Probability Distributions for Discrete Random Variables

### Probability Mass Function (PMF)
- **Definition**: A function p<sub>X</sub>: ‚Ñù ‚Üí [0,1] that gives the probability that a discrete random variable equals a specific value
- **Formula**: p<sub>X</sub>(a) = ‚Ñô(X = a)
- **Properties**:
  - p<sub>X</sub>(x) ‚â• 0 for all x
  - Œ£<sub>x</sub> p<sub>X</sub>(x) = 1

### Example: Fair Coin Toss
For a random variable X associated with a fair coin toss (X = 1 for heads, X = 0 for tails):
$$p_X(x) = \begin{cases}
1/2 & \text{if } x = 0 \\
1/2 & \text{if } x = 1 \\
0 & \text{otherwise}
\end{cases}$$

## 5. Probability Distributions for Continuous Random Variables

### Cumulative Distribution Function (CDF)
- **Definition**: A function F<sub>X</sub>: ‚Ñù ‚Üí [0,1] that gives the probability that a random variable is less than or equal to a specific value
- **Formula**: F<sub>X</sub>(x) = ‚Ñô(X ‚â§ x)
- **Properties**:
  - 0 ‚â§ F<sub>X</sub>(x) ‚â§ 1
  - lim<sub>x‚Üí-‚àû</sub> F<sub>X</sub>(x) = 0
  - lim<sub>x‚Üí+‚àû</sub> F<sub>X</sub>(x) = 1
  - x ‚â§ y ‚üπ F<sub>X</sub>(x) ‚â§ F<sub>X</sub>(y) (non-decreasing)
  - ‚Ñô(a ‚â§ X ‚â§ b) = F<sub>X</sub>(b) - F<sub>X</sub>(a)

### Probability Density Function (PDF)
- **Definition**: For a continuous random variable with a differentiable CDF, the PDF is the derivative of the CDF
- **Formula**: f<sub>X</sub>(x) = d/dx F<sub>X</sub>(x)
- **Properties**:
  - f<sub>X</sub>(x) ‚â• 0
  - ‚à´<sub>-‚àû</sub><sup>‚àû</sup> f<sub>X</sub>(x)dx = 1
  - For a set A, ‚à´<sub>A</sub> f<sub>X</sub>(x)dx = ‚Ñô(X ‚àà A)
  - ‚Ñô(a ‚â§ X ‚â§ b) = ‚à´<sub>a</sub><sup>b</sup> f<sub>X</sub>(x)dx
  - F<sub>X</sub>(x) = ‚à´<sub>-‚àû</sub><sup>x</sup> f<sub>X</sub>(z)dz
  - Important: In general, f<sub>X</sub>(x) ‚â† ‚Ñô(X = x)

### Example: Uniform Random Variable
For a uniform random variable on [0,1]:
$$f_X(x) = \begin{cases}
1 & \text{if } x \in [0,1] \\
0 & \text{otherwise}
\end{cases}$$

The CDF is:
$$F_X(x) = \begin{cases}
0 & \text{if } x < 0 \\
x & \text{if } 0 \leq x \leq 1 \\
1 & \text{if } x > 1
\end{cases}$$

## 6. Expectation, Variance, and Standard Deviation

### Expectation (Expected Value)
- **For Discrete Random Variables**:
  $$\mathbb{E}(X) = \sum_{x \in S} x \cdot p_X(x)$$

- **For Continuous Random Variables**:
  $$\mathbb{E}(X) = \int_{-\infty}^{\infty} x \cdot f_X(x) dx$$

- **For Functions of Random Variables**:
  - Discrete: ùîº(g(X)) = Œ£<sub>x‚ààS</sub> g(x) ¬∑ p<sub>X</sub>(x)
  - Continuous: ùîº(g(X)) = ‚à´<sub>-‚àû</sub><sup>‚àû</sup> g(x) ¬∑ f<sub>X</sub>(x) dx

### Example: Expected Value of a Die Roll
For a fair six-sided die:
$$\mathbb{E}(X) = \sum_{i=1}^{6} i \times \frac{1}{6} = \frac{1}{6} + \frac{2}{6} + \frac{3}{6} + \frac{4}{6} + \frac{5}{6} + \frac{6}{6} = \frac{21}{6} = 3.5$$

### Example: Expected Value of a Uniform Random Variable
For a uniform random variable on [0,1]:
$$\mathbb{E}(X) = \int_{-\infty}^{\infty} x \cdot f_X(x) dx = \int_{0}^{1} x \cdot 1 \, dx = \left[ \frac{x^2}{2} \right]_{0}^{1} = \frac{1}{2}$$

### Linearity of Expectation
- If X and Y are random variables, and a and b are constants:
  $$\mathbb{E}(a \cdot g(X) + b \cdot h(Y)) = a \cdot \mathbb{E}(g(X)) + b \cdot \mathbb{E}(h(Y))$$

### Example: Linearity of Expectation
If the expected sales price of an apple is $1 and an orange is $2, then the expected sales price of 2 apples and 3 oranges is:
$$\mathbb{E}(2 \cdot \text{apple price} + 3 \cdot \text{orange price}) = 2 \cdot \mathbb{E}(\text{apple price}) + 3 \cdot \mathbb{E}(\text{orange price}) = 2 \times \$1 + 3 \times \$2 = \$8$$

### Variance and Standard Deviation
- **Variance**: Measures the spread or dispersion of a random variable around its mean
  $$\text{Var}(X) = \mathbb{E}((X - \mu)^2) = \mathbb{E}(X^2) - \mu^2$$
  where Œº = ùîº(X)

- **Standard Deviation**: Square root of the variance
  $$\sigma = \sqrt{\text{Var}(X)}$$

### Example: Variance of a Die Roll
For a fair six-sided die with Œº = 3.5:
$$\text{Var}(X) = \mathbb{E}((X - \mu)^2) = \sum_{i=1}^{6} (i - 3.5)^2 \times \frac{1}{6} = \frac{105}{36} \approx 2.92$$
$$\sigma = \sqrt{\frac{105}{36}} \approx 1.71$$

### Example: Variance of a Uniform Random Variable
For a uniform random variable on [0,1] with Œº = 1/2:
$$\text{Var}(X) = \mathbb{E}((X - \mu)^2) = \int_{0}^{1} (x - \frac{1}{2})^2 dx = \frac{1}{12}$$
$$\sigma = \sqrt{\frac{1}{12}} \approx 0.289$$

### Properties of Variance
- For independent random variables X and Y, and constants a and b:
  $$\text{Var}(a \cdot g(X) + b \cdot h(Y)) = a^2 \cdot \text{Var}(g(X)) + b^2 \cdot \text{Var}(h(Y))$$

## 7. Practical Examples

### Example 1: Card Drawing
For a standard deck of 52 cards:
- The sample space for drawing two cards and recording their sum (Ace = 1, Jack/Queen/King = 10) is:
  {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}
- The sample space for the number of spades when drawing five cards is:
  {0, 1, 2, 3, 4, 5}

### Example 2: Die Rolling and Event Independence
For a fair 6-sided die with events:
- A: The number rolled is odd = {1, 3, 5}
- B: The number rolled is greater than or equal to 4 = {4, 5, 6}
- C: The number rolled doesn't start with the letters "f" or "t" = {1, 6}

To determine if A and C are independent:
- ‚Ñô(A) = 3/6 = 1/2
- ‚Ñô(C) = 2/6 = 1/3
- ‚Ñô(A ‚à© C) = |{1}|/6 = 1/6
- Since ‚Ñô(A) √ó ‚Ñô(C) = (1/2) √ó (1/3) = 1/6 = ‚Ñô(A ‚à© C), events A and C are independent

### Example 3: Blood Type Probabilities
Given that 44% of Americans have type O blood, 42% have type A, 10% have type B, and the rest have type AB:
- Probability of not having type A blood: 1 - 0.42 = 0.58
- Probability of having type A or AB blood: 0.42 + (1 - 0.44 - 0.42 - 0.10) = 0.42 + 0.04 = 0.46

### Example 4: Betting Game Expected Value
In a betting game where you roll a die and:
- Win $200 if you get a 5 on the first roll
- Win $100 if you don't get a 5 on the first roll but get a 5 on the second roll
- Win $0 otherwise

The expected winnings are:
- ‚Ñô(X = 200) = 1/6
- ‚Ñô(X = 100) = (5/6) √ó (1/6) = 5/36
- ‚Ñô(X = 0) = 1 - 1/6 - 5/36 = 25/36
- ùîº(X) = 200 √ó (1/6) + 100 √ó (5/36) + 0 √ó (25/36) = 33.33 + 13.89 = $47.22

## 8. Key Takeaways

1. Probability provides a formal framework for quantifying uncertainty and randomness
2. The three components of a probability space are:
   - Sample space (Œ©): All possible outcomes
   - Event space (‚Ñ±): Collection of subsets of Œ©
   - Probability measure (‚Ñô): Function assigning probabilities to events

3. Conditional probability allows us to update probabilities based on new information
4. Independence of events means the occurrence of one event doesn't affect the probability of another
5. Random variables map outcomes to numbers, allowing mathematical analysis
6. Probability distributions describe the likelihood of different values of a random variable:
   - PMF for discrete random variables
   - PDF and CDF for continuous random variables

7. Expected value represents the long-run average of a random variable
8. Variance and standard deviation measure the spread or dispersion around the expected value
9. Linearity of expectation and properties of variance simplify calculations for combinations of random variables